[
  {
    "objectID": "supplementary/useful-resources.html",
    "href": "supplementary/useful-resources.html",
    "title": "Useful Resources",
    "section": "",
    "text": "In this document we collect some useful resources for ordinal regression models.\n\nBooks\n\nAnalysis of Ordinal Categorical Data: a very complete book about modeling ordinal data.\nCategorical Data Analysis\nAn Introduction to Categorical Data Analysis: Introduction about categorical data in R with some examples with ordinal data (Chapter 6)\nDoing Bayesian Data Analysis: Overview of Bayesian data analysis. Chapter 23 is about ordinal regression models.\nRegression and Other Stories: Chapter 15 contains some examples and parametrizations of ordinal regression models\n\n\n\nPosts\n\nViolation of Proportional Odds is Not Fatal\nAssessing the Proportional Odds Assumption and Its Impact\nResources for Ordinal Regression Models\nBayesian Estimation of Signal Detection Models\nCausal inference with ordinal regression\n\n\n\nPackages\n\nordinal: https://cran.r-project.org/web/packages/ordinal/index.html\nVGAM: https://cran.r-project.org/web/packages/VGAM/index.html\nbrms: https://paul-buerkner.github.io/brms/\nMASS::polr(): https://rdrr.io/cran/MASS/man/polr.html\nordDisp: https://cran.r-project.org/web/packages/ordDisp/index.html"
  },
  {
    "objectID": "supplementary/ordinal-notes.html",
    "href": "supplementary/ordinal-notes.html",
    "title": "Ordinal Notes",
    "section": "",
    "text": "In the tutorial we used the \\(\\alpha_k -\\mathbf{X}\\boldsymbol{\\beta}\\) parametrization (thus with the minus sign) because this force the \\(\\beta\\) to have the interpretation as in standard regression models. Usually, \\(\\beta\\) is the increase in \\(y\\) for a unit increase in \\(x\\). A negative \\(\\beta\\) means that the expected value of \\(y\\) decrease for an increase in \\(x\\). Let’s see an example using the positive sign for \\(\\beta\\).\n\nk &lt;- 4 # number of ordinal outcomes\nx &lt;- c(0, 1) # a binary predictor\nb1 &lt;- log(3) # log odds ratio comparing x1 and x0\nprobs0 &lt;- rep(1/k, k)\nalpha &lt;- prob_to_alpha(probs0, link = \"logit\")\nX &lt;- matrix(x, nrow = 2)\n\n# positive sign\n(lp &lt;- lapply(alpha, function(a) c(a + X %*% b1)))\n\n$`1|2`\n[1] -1.098612  0.000000\n\n$`2|3`\n[1] 0.000000 1.098612\n\n$`3|4`\n[1] 1.098612 2.197225\n\ncump &lt;- lapply(lp, plogis)\ncump &lt;- cbind(0, data.frame(cump), 1)\np &lt;- data.frame(t(apply(cump, 1, diff)))\nnames(p) &lt;- paste0(\"y\", 1:k)\np$x &lt;- x\np\n\n    y1   y2   y3   y4 x\n1 0.25 0.25 0.25 0.25 0\n2 0.50 0.25 0.15 0.10 1\n\np |&gt; \n  pivot_longer(starts_with(\"y\")) |&gt; \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k + X\\\\beta$\"))\n\n\n\n\n\n\n\n\nClearly a positive \\(\\beta\\) create higher probability for lower \\(Y\\) categories. This can be somehow not intuitive thus we can use the negative sign.\n\n# negative sign\n\n(lp &lt;- lapply(alpha, function(a) c(a - X %*% b1)))\n\n$`1|2`\n[1] -1.098612 -2.197225\n\n$`2|3`\n[1]  0.000000 -1.098612\n\n$`3|4`\n[1] 1.098612 0.000000\n\ncump &lt;- lapply(lp, plogis)\ncump &lt;- cbind(0, data.frame(cump), 1)\np &lt;- data.frame(t(apply(cump, 1, diff)))\nnames(p) &lt;- paste0(\"y\", 1:k)\np$x &lt;- x\n\np |&gt; \n  pivot_longer(starts_with(\"y\")) |&gt; \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k - X\\\\beta$\"))\n\n\n\n\n\n\n\n\nUsing the second parametrization, with positive \\(\\beta\\) we have higher probability for higher \\(Y\\) categories and the opposite.\nThe negative-sign parametrization is implicit when simulating from the latent distribution. Let’s see an example. We use a continuous \\(x\\) predictor because it is easier to see the results.\n\nn &lt;- 1e3\nx &lt;- runif(1e3)\nB &lt;- 3\nys_p &lt;- x * B + rnorm(n)\nys_n &lt;- x * -B + rnorm(n)\ny_p &lt;- findInterval(ys_p, alpha) + 1\ny_n &lt;- findInterval(ys_n, alpha) + 1\n\npar(mfrow = c(1,2))\nplot(x, ys_p, col = y_p, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = 3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\nlegend(\"bottomleft\", fill = 1:k, legend = paste0(\"Y\", 1:k))\n\nplot(x, ys_n, col = y_n, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = -3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\n\n\n\n\nSimulated data using the latent variable approach. The dotted lines are the thresholds \\(\\alpha\\)"
  },
  {
    "objectID": "supplementary/ordinal-notes.html#mathbfxboldsymbolbeta-vs-mathbfxboldsymbolbeta-parametrization",
    "href": "supplementary/ordinal-notes.html#mathbfxboldsymbolbeta-vs-mathbfxboldsymbolbeta-parametrization",
    "title": "Ordinal Notes",
    "section": "",
    "text": "In the tutorial we used the \\(\\alpha_k -\\mathbf{X}\\boldsymbol{\\beta}\\) parametrization (thus with the minus sign) because this force the \\(\\beta\\) to have the interpretation as in standard regression models. Usually, \\(\\beta\\) is the increase in \\(y\\) for a unit increase in \\(x\\). A negative \\(\\beta\\) means that the expected value of \\(y\\) decrease for an increase in \\(x\\). Let’s see an example using the positive sign for \\(\\beta\\).\n\nk &lt;- 4 # number of ordinal outcomes\nx &lt;- c(0, 1) # a binary predictor\nb1 &lt;- log(3) # log odds ratio comparing x1 and x0\nprobs0 &lt;- rep(1/k, k)\nalpha &lt;- prob_to_alpha(probs0, link = \"logit\")\nX &lt;- matrix(x, nrow = 2)\n\n# positive sign\n(lp &lt;- lapply(alpha, function(a) c(a + X %*% b1)))\n\n$`1|2`\n[1] -1.098612  0.000000\n\n$`2|3`\n[1] 0.000000 1.098612\n\n$`3|4`\n[1] 1.098612 2.197225\n\ncump &lt;- lapply(lp, plogis)\ncump &lt;- cbind(0, data.frame(cump), 1)\np &lt;- data.frame(t(apply(cump, 1, diff)))\nnames(p) &lt;- paste0(\"y\", 1:k)\np$x &lt;- x\np\n\n    y1   y2   y3   y4 x\n1 0.25 0.25 0.25 0.25 0\n2 0.50 0.25 0.15 0.10 1\n\np |&gt; \n  pivot_longer(starts_with(\"y\")) |&gt; \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k + X\\\\beta$\"))\n\n\n\n\n\n\n\n\nClearly a positive \\(\\beta\\) create higher probability for lower \\(Y\\) categories. This can be somehow not intuitive thus we can use the negative sign.\n\n# negative sign\n\n(lp &lt;- lapply(alpha, function(a) c(a - X %*% b1)))\n\n$`1|2`\n[1] -1.098612 -2.197225\n\n$`2|3`\n[1]  0.000000 -1.098612\n\n$`3|4`\n[1] 1.098612 0.000000\n\ncump &lt;- lapply(lp, plogis)\ncump &lt;- cbind(0, data.frame(cump), 1)\np &lt;- data.frame(t(apply(cump, 1, diff)))\nnames(p) &lt;- paste0(\"y\", 1:k)\np$x &lt;- x\n\np |&gt; \n  pivot_longer(starts_with(\"y\")) |&gt; \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k - X\\\\beta$\"))\n\n\n\n\n\n\n\n\nUsing the second parametrization, with positive \\(\\beta\\) we have higher probability for higher \\(Y\\) categories and the opposite.\nThe negative-sign parametrization is implicit when simulating from the latent distribution. Let’s see an example. We use a continuous \\(x\\) predictor because it is easier to see the results.\n\nn &lt;- 1e3\nx &lt;- runif(1e3)\nB &lt;- 3\nys_p &lt;- x * B + rnorm(n)\nys_n &lt;- x * -B + rnorm(n)\ny_p &lt;- findInterval(ys_p, alpha) + 1\ny_n &lt;- findInterval(ys_n, alpha) + 1\n\npar(mfrow = c(1,2))\nplot(x, ys_p, col = y_p, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = 3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\nlegend(\"bottomleft\", fill = 1:k, legend = paste0(\"Y\", 1:k))\n\nplot(x, ys_n, col = y_n, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = -3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\n\n\n\n\nSimulated data using the latent variable approach. The dotted lines are the thresholds \\(\\alpha\\)"
  },
  {
    "objectID": "supplementary/ordinal-notes.html#checking-the-impact-of-mathbfbeta",
    "href": "supplementary/ordinal-notes.html#checking-the-impact-of-mathbfbeta",
    "title": "Ordinal Notes",
    "section": "Checking the impact of \\(\\mathbf{\\beta}\\)",
    "text": "Checking the impact of \\(\\mathbf{\\beta}\\)\nChoosing one or more plausible \\(\\beta_j\\) values can be challenging. For a single \\(\\beta\\) we can easily think about the odds ratio (for a logit model) or the Cohen’s \\(d\\) (for a probit) model. With multiple predictors and their interactions is not easy to fix plausible values. A good strategy is to try different values and check the impact on the predicted probabilities. In practice, we need to compute the predicted probabilities using the model equation for the \\(k\\) ordinal outcomes. This can be easily done with the sim_ord_latent() function, fixing the simulate = FALSE parameter. In this way, only the predicted probabilities are computed. Let’s see an example for a single \\(x\\) sampled for a standard normal distribution.\n\nk &lt;- 4\ndat &lt;- data.frame(x = seq(-4, 4, 0.1))\nb1 &lt;- 0.5\nprobs0 &lt;- rep(1/k, k)\ndat &lt;- sim_ord_latent(~x, beta = b1, prob0 = probs0, data = dat, simulate = FALSE)\nhead(dat)\n\n     x       yp1      yp12     yp123        y1        y2         y3         y4\n1 -4.0 0.7112346 0.8807971 0.9568355 0.7112346 0.1695625 0.07603839 0.04316453\n2 -3.9 0.7008582 0.8754466 0.9547226 0.7008582 0.1745885 0.07927594 0.04527742\n3 -3.8 0.6902712 0.8698915 0.9525114 0.6902712 0.1796203 0.08261987 0.04748860\n4 -3.7 0.6794810 0.8641271 0.9501979 0.6794810 0.1846461 0.08607076 0.04980214\n5 -3.6 0.6684954 0.8581489 0.9477778 0.6684954 0.1896536 0.08962886 0.05222221\n6 -3.5 0.6573231 0.8519528 0.9452469 0.6573231 0.1946297 0.09329410 0.05475309\n\n\nThen we can plot the results:\n\ndat |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = name)) +\n  geom_line()\n\n\n\n\n\n\n\n\nIn this case the \\(\\beta_1 = 0.5\\) can be considered a plausible value. Let’s see what happens increasing it:\n\ndata.frame(x = seq(-4, 4, 0.1)) |&gt; \n  sim_ord_latent(~x, beta = 4, prob0 = probs0, data = _, simulate = FALSE) |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = name)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe can clearly see the difference and probably \\(\\beta = 4\\) can be considered too large. To note, the same result can be achieved using the num_latent_plot() (that under the hood uses the sim_ord_latent() function).\nLet’s make now an example, with an interaction between a continous and categorical predictor.\n\ndat &lt;- expand_grid(x = seq(-4, 4, 0.1), g = c(\"a\", \"b\"))\ndat$g &lt;- factor(dat$g)\ncontrasts(dat$g) &lt;- c(-0.5, 0.5)\nbeta &lt;- c(b1 = 0.5, b2 = 1, b3 = 0.1)\ndat &lt;- sim_ord_latent(~ x * g, beta = beta, prob0 = probs0, data = dat, simulate = FALSE)\n\ndat |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = name, lty = g)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe can see the impact of \\(\\beta_3 = 0.1\\) that is the difference in slopes between the two groups. We can also use another plot to better see the group effect on each \\(Y\\).\n\ndat |&gt; \n  pivot_longer(matches(\"^y[1-9]\")) |&gt; \n  ggplot(aes(x = x, y = value, color = g)) +\n  geom_line() +\n  facet_wrap(~name)"
  },
  {
    "objectID": "supplementary/location-scale-models.html",
    "href": "supplementary/location-scale-models.html",
    "title": "Location-Scale Ordinal Models",
    "section": "",
    "text": "The default ordinal regression model assume that the variance of the underlying latent distribution is the same across conditions. This is similar to a standard linear regression assuming the homogeneity of variances.\nFor example, when comparing two groups or conditions we can run a standard linear model (i.e., a t-test) assuming homogeneity of variances or using the Welch t-test (see Delacre et al., 2017) that relaxes the assumption. In addition, there are the so-called location-scale models that allows to include predictors also for the scale (e.g., the variance) of the distribution.\nThis can be done also in ordinal regression where instead of assuming the same variance between conditions, the linear predictors can be included. The Equations 1 and 2 expand the standard model including the linear predictor on the scale of the latent distribution.\n\\[\nP(Y \\leq k) = g^{-1}\\left(\\frac{\\alpha_k - \\mathbf{X}\\boldsymbol{\\beta}}{e^{\\boldsymbol{Z}\\boldsymbol{\\zeta}}}\\right)\n\\tag{1}\\]\n\\[\nY^\\star = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\;\\;\\; \\epsilon_i \\sim \\mathcal{N}(0, e^{Z\\zeta})\n\\tag{2}\\]\nWhere \\(X\\zeta\\) is the linear predictor for the scale of the distribution. By default for both the logit and probit model the scale is fixed to 1. On scale-location models we put predictors on both parameters. Given that the scale cannot be negative we use a log link function \\(\\eta = \\text{log}(X\\zeta)\\). As suggested by Tutz & Berger (2017) location-scale models can be considered as a more parsimonious approach compared to partially or completely relaxing the proportional odds assumption. Allowing the scale to be different as a function of the predictor create more modelling flexibility. Furthermore, two groups could be theoretically different only in the scale of the latent distribution with a similar location. In this example, the only way to capture group differences is by including a scale effect. The Figure 1 depict the impact of having different scales between two groups on the ordinal probabilities but the same location. Clearly, there is no location effect and the two groups are predicted to be the same considering only the location effect.\n\ncat_latent_plot(location = c(0, 0), scale = c(1, 2), prob0 = rep(1/4, 4), link = \"logit\", plot = FALSE) |&gt; \n  cowplot::plot_grid(plotlist = _, nrow = 1)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nTutz (2022) provide a very clear and intuitive explanation of what happen when including a scale effect and how to interpret the result. As suggested before, the scale-location model allow to independently predict changes in the location and the scale. While location shifts are simply interpreted as increasing/decreasing the latent \\(\\mu\\) or the odds of responding a certain category scale effects are not straightforward. As the scale increase (e.g., the variance increase) there is an higher probability mass on extreme categories. On the other side as the scale decrease, responses are more concentrated on specific categories.\nThe location parameter determine the category and the scale determine the concentration around the category. For example, if one group have a certain latent mean \\(\\mu_1\\) and a small scale \\(\\sigma^2 = 1/3\\) (thus one third compared to the standard version of the distribution), all responses will be focused on categories around the latent mean. On the other side, increasing the scale will increase the cumulative probabilities for all categories and for values that tends to infinity extreme categories are preferred. The scale parameter can somehow be interpreted as the response style.\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\nThe location-scale model can be simulated using the sim_ord_latent() function and providing the predictors for the scale parameters. Given the log link function, predictors are provided on the log scale. For example, we simulate the effect of a binary variable \\(x\\) representing two independent groups predicting the \\(k = 5\\) response. We simulate a location effect of \\(\\beta_1 = 0.5\\) (in probit scale) and \\(\\zeta_1 = \\text{log}(2) = 0.70\\). The first group has a \\(\\sigma = 1\\) and the second group has \\(\\sigma = 2\\). Again we simulate that the baseline probabilities are uniform for the first group. The \\(\\log\\) link function (and \\(e\\) as inverse link function) is used to make sure that the variance is always positive. If the first group has a scale of 1 and the second group a scale of 2, the \\(\\zeta_1 = \\log(\\frac{s_2}{s_1}) = \\log(\\frac{2}{1}) \\approx 0.7\\). Thus the two groups have a log difference in terms of scale of 0.7 (or a ratio of 2).\n\nk &lt;- 5  # number of options (1)\nn &lt;- 1e5 # number of observations (1)\nb1 &lt;- 0.5 # beta1, the shift in the latent distribution (2)\nz1 &lt;- log(2) # zeta1, the change in the scale\nprobs0 &lt;- rep(1/k, k) # probabilities when x = 0 (3)\nalphas &lt;- prob_to_alpha(probs0, link = \"probit\") # get true thresholds from probabilities (3)\ndat &lt;- data.frame(x = rep(c(0, 1), each = n/2))\ndat &lt;- sim_ord_latent(~x, scale = ~x, beta = b1, zeta = z1, prob0 = probs0, data = dat, link = \"probit\")\nfit &lt;- clm(y ~ x, scale = ~ x, data = dat, link = \"probit\")\nsummary(fit)\n\nformula: y ~ x\nscale:   ~x\ndata:    dat\n\n link   threshold nobs  logLik     AIC       niter max.grad cond.H \n probit flexible  1e+05 -151151.38 302314.76 11(0) 7.09e-07 3.2e+01\n\nCoefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx  0.50451    0.01176   42.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlog-scale coefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx 0.701164   0.008418   83.29   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n     Estimate Std. Error z value\n1|2 -0.835177   0.006287 -132.84\n2|3 -0.254419   0.005425  -46.90\n3|4  0.248568   0.005411   45.93\n4|5  0.829602   0.006277  132.17\n\n\nTo better understand the impact of assuming (or simulating) a different latent scale we fit \\(k - 1\\) binomial regressions and check the estimated coefficients. We are not simulating a specific beta for each outcome but simulating a scale effect is actually impacting the regression coefficients. When generating data for a binary outcome the linear predictor is composed by \\(\\eta = \\beta_0 + \\beta_1x\\). The threshold \\(\\alpha\\) and slope of the function can be estimated using \\(\\alpha = -\\frac{\\beta_0}{\\beta_1}\\) and the slope is \\(\\frac{1}{\\beta_1}\\) (Faraggi et al., 2003; Knoblauch & Maloney, 2012). Under the proportional odds assumption, there is only a change in thresholds \\(\\alpha\\) this a shift in the sigmoid along the \\(x\\) axis. When including a scale effect a change in the sigmoid is combined with a change in the slope.\n\nset.seed(2023)\nk &lt;- 4\nn &lt;- 1e5\nb1 &lt;- 3\nd1 &lt;- log(2)\ndat &lt;- data.frame(x = runif(n))\ndat &lt;- sim_ord_latent(~x, ~x, beta = b1, zeta = d1, prob0 = rep(1/k, k), data = dat, link = \"probit\")\n\ndat$y1vs234 &lt;- ifelse(dat$y &lt;= 1, 1, 0)\ndat$y12vs34 &lt;- ifelse(dat$y &lt;= 2, 1, 0)\ndat$y123vs4 &lt;- ifelse(dat$y &lt;= 3, 1, 0)\n\ndat$y &lt;- ordered(dat$y)\nfit &lt;- clm(y ~ x, scale = ~x, data = dat, link = \"probit\")\n\nfit1vs234 &lt;- glm(y1vs234 ~ x, data = dat, family = binomial(link = \"probit\"))\nfit12vs34 &lt;- glm(y12vs34 ~ x, data = dat, family = binomial(link = \"probit\"))\nfit123vs4 &lt;- glm(y123vs4 ~ x, data = dat, family = binomial(link = \"probit\"))\n\nfits &lt;- list(y1vs234 = fit1vs234, fit12vs34 = fit12vs34, fit123vs4 = fit123vs4)\n\nlapply(fits, function(x) coef(x)) |&gt; \n  bind_rows(.id = \"model\") |&gt; \n  mutate(xp = list(seq(0, 5, 0.01))) |&gt; \n  unnest(xp) |&gt; \n  ggplot(aes(x = xp, y = `(Intercept)` + x * xp)) +\n  geom_line(aes(color = model)) +\n  theme_minimal(15)\n\n\n\n\n\n\n\n\n\n\n\nWhen fitting a model with clm() the way of relaxing the proportional odds or parallel slopes assumption is including what is called a nominal effect. Basically when a predictor is included as a nominal effect, there will be \\(k - 1\\) coefficients (where \\(k\\) is the number of ordered levels). In this way, the odds ratio or the difference in \\(z\\) scores will not be the same across \\(k\\) levels.\nIf a scale effect is included in clm() is not immediately clear what happens to the proportional odds assumption. In fact, we are still estimating a single \\(\\beta\\) but we have extra parameters on the scale of the distribution. However, changing the scale of the distribution is the same as changing the slope. This is clear from Figure 3 where changing the scale of the latent distribution is the same as changing the slope of the cumulative probability function. The slope (\\(\\beta_1\\)) is related to the scale of the distribution (logistic or normal) as \\(\\beta_1 = \\frac{1}{s}\\). In fact, if we include a scale effect we are changing the scale of the underlying latent distribution and thus the slope of of the functions.\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nIn fact, if we simulate a model with a scale effect, odds ratios or \\(z\\) differences are no longer constant.\nWe will simulate the following situation where a binary predictor \\(x\\) has an effect both on location and scale of the logistic distribution.\n\nodds &lt;- function(p) p / (1 - p)\n\nn &lt;- 1e4 # large N\nk &lt;- 4\nb1 &lt;- log(3)\nz1 &lt;- log(2)\ndat &lt;- data.frame(x = rep(c(0, 1), each = n))\ndat &lt;- sim_ord_latent(~x, ~x, beta = log(3), zeta = log(2), prob0 = rep(1/k, k), data = dat, link = \"logit\")\n\n# model with scale effects, log(odds ratios) are different\n\nfit &lt;- clm(y ~ x, scale = ~ x, data = dat, link = \"logit\")\n\npreds &lt;- predict(fit, newdata = data.frame(x = c(0, 1)))$fit\npcum &lt;- t(apply(preds, 1, cumsum))[, 1:(k - 1)]\n\napply(pcum, 2, function(c) log(odds(c[1])/odds(c[2])))\n\n         1          2          3 \n-0.0436072  0.5202012  1.0755903 \n\n# model without scale effects, log(odds ratios) are equal\n\nfit0 &lt;- clm(y ~ x, scale = ~ 1, data = dat, link = \"logit\")\npreds &lt;- predict(fit0, newdata = data.frame(x = c(0, 1)))$fit\npcum &lt;- t(apply(preds, 1, cumsum))[, 1:(k - 1)]\napply(pcum, 2, function(c) log(odds(c[1])/odds(c[2])))\n\n        1         2         3 \n0.6107211 0.6107211 0.6107211 \n\n\n\ncat_latent_plot(c(0, 0 + b1), s = c(exp(0), (exp(0) + z1)), prob0 = rep(1/k, k), link = \"logit\")\n\n\n\n\n\n\n\n\nActually we can model this dataset using a non-proportional odds model. The model correctly capture the more complex probability structure but we are not able to recover the parameters because we simulated a scale effect.\n\nfit_nominal &lt;- clm(y ~ 1, nominal = ~ x, data = dat, link = \"logit\")\nsummary(fit_nominal)\n\nformula: y ~ 1\nnominal: ~x\ndata:    dat\n\n link  threshold nobs  logLik    AIC      niter max.grad cond.H \n logit flexible  20000 -26016.82 52045.65 8(0)  3.82e-12 7.8e+01\n\nThreshold coefficients:\n                Estimate Std. Error z value\n1|2.(Intercept) -1.12600    0.02325 -48.421\n2|3.(Intercept) -0.01120    0.02000  -0.560\n3|4.(Intercept)  1.09063    0.02305  47.320\n1|2.x            0.04227    0.03271   1.292\n2|3.x           -0.51759    0.02879 -17.981\n3|4.x           -1.07663    0.03052 -35.281\n\n\nEssentially we estimated two sets of thresholds one for the group 0 and one for the group 1. The difference with the previous model (beyond the fact that data are generated under a scale-location model) is that we need more parameters in a non-proportional odds model compared to the scale-location model. This is also more relevant when including more predictors. Also the likelihood ratio test correctly suggest that there is no relevant difference in the likelihood of the two models.\n\nanova(fit, fit_nominal)\n\nLikelihood ratio tests of cumulative link models:\n \n            formula: nominal: scale: link: threshold:\nfit         y ~ x    ~1       ~x     logit flexible  \nfit_nominal y ~ 1    ~x       ~1     logit flexible  \n\n            no.par   AIC logLik LR.stat df Pr(&gt;Chisq)\nfit              5 52044 -26017                      \nfit_nominal      6 52046 -26017  0.0478  1     0.8269"
  },
  {
    "objectID": "supplementary/location-scale-models.html#scale-effects",
    "href": "supplementary/location-scale-models.html#scale-effects",
    "title": "Location-Scale Ordinal Models",
    "section": "",
    "text": "The default ordinal regression model assume that the variance of the underlying latent distribution is the same across conditions. This is similar to a standard linear regression assuming the homogeneity of variances.\nFor example, when comparing two groups or conditions we can run a standard linear model (i.e., a t-test) assuming homogeneity of variances or using the Welch t-test (see Delacre et al., 2017) that relaxes the assumption. In addition, there are the so-called location-scale models that allows to include predictors also for the scale (e.g., the variance) of the distribution.\nThis can be done also in ordinal regression where instead of assuming the same variance between conditions, the linear predictors can be included. The Equations 1 and 2 expand the standard model including the linear predictor on the scale of the latent distribution.\n\\[\nP(Y \\leq k) = g^{-1}\\left(\\frac{\\alpha_k - \\mathbf{X}\\boldsymbol{\\beta}}{e^{\\boldsymbol{Z}\\boldsymbol{\\zeta}}}\\right)\n\\tag{1}\\]\n\\[\nY^\\star = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\;\\;\\; \\epsilon_i \\sim \\mathcal{N}(0, e^{Z\\zeta})\n\\tag{2}\\]\nWhere \\(X\\zeta\\) is the linear predictor for the scale of the distribution. By default for both the logit and probit model the scale is fixed to 1. On scale-location models we put predictors on both parameters. Given that the scale cannot be negative we use a log link function \\(\\eta = \\text{log}(X\\zeta)\\). As suggested by Tutz & Berger (2017) location-scale models can be considered as a more parsimonious approach compared to partially or completely relaxing the proportional odds assumption. Allowing the scale to be different as a function of the predictor create more modelling flexibility. Furthermore, two groups could be theoretically different only in the scale of the latent distribution with a similar location. In this example, the only way to capture group differences is by including a scale effect. The Figure 1 depict the impact of having different scales between two groups on the ordinal probabilities but the same location. Clearly, there is no location effect and the two groups are predicted to be the same considering only the location effect.\n\ncat_latent_plot(location = c(0, 0), scale = c(1, 2), prob0 = rep(1/4, 4), link = \"logit\", plot = FALSE) |&gt; \n  cowplot::plot_grid(plotlist = _, nrow = 1)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nTutz (2022) provide a very clear and intuitive explanation of what happen when including a scale effect and how to interpret the result. As suggested before, the scale-location model allow to independently predict changes in the location and the scale. While location shifts are simply interpreted as increasing/decreasing the latent \\(\\mu\\) or the odds of responding a certain category scale effects are not straightforward. As the scale increase (e.g., the variance increase) there is an higher probability mass on extreme categories. On the other side as the scale decrease, responses are more concentrated on specific categories.\nThe location parameter determine the category and the scale determine the concentration around the category. For example, if one group have a certain latent mean \\(\\mu_1\\) and a small scale \\(\\sigma^2 = 1/3\\) (thus one third compared to the standard version of the distribution), all responses will be focused on categories around the latent mean. On the other side, increasing the scale will increase the cumulative probabilities for all categories and for values that tends to infinity extreme categories are preferred. The scale parameter can somehow be interpreted as the response style.\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\nThe location-scale model can be simulated using the sim_ord_latent() function and providing the predictors for the scale parameters. Given the log link function, predictors are provided on the log scale. For example, we simulate the effect of a binary variable \\(x\\) representing two independent groups predicting the \\(k = 5\\) response. We simulate a location effect of \\(\\beta_1 = 0.5\\) (in probit scale) and \\(\\zeta_1 = \\text{log}(2) = 0.70\\). The first group has a \\(\\sigma = 1\\) and the second group has \\(\\sigma = 2\\). Again we simulate that the baseline probabilities are uniform for the first group. The \\(\\log\\) link function (and \\(e\\) as inverse link function) is used to make sure that the variance is always positive. If the first group has a scale of 1 and the second group a scale of 2, the \\(\\zeta_1 = \\log(\\frac{s_2}{s_1}) = \\log(\\frac{2}{1}) \\approx 0.7\\). Thus the two groups have a log difference in terms of scale of 0.7 (or a ratio of 2).\n\nk &lt;- 5  # number of options (1)\nn &lt;- 1e5 # number of observations (1)\nb1 &lt;- 0.5 # beta1, the shift in the latent distribution (2)\nz1 &lt;- log(2) # zeta1, the change in the scale\nprobs0 &lt;- rep(1/k, k) # probabilities when x = 0 (3)\nalphas &lt;- prob_to_alpha(probs0, link = \"probit\") # get true thresholds from probabilities (3)\ndat &lt;- data.frame(x = rep(c(0, 1), each = n/2))\ndat &lt;- sim_ord_latent(~x, scale = ~x, beta = b1, zeta = z1, prob0 = probs0, data = dat, link = \"probit\")\nfit &lt;- clm(y ~ x, scale = ~ x, data = dat, link = \"probit\")\nsummary(fit)\n\nformula: y ~ x\nscale:   ~x\ndata:    dat\n\n link   threshold nobs  logLik     AIC       niter max.grad cond.H \n probit flexible  1e+05 -151151.38 302314.76 11(0) 7.09e-07 3.2e+01\n\nCoefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx  0.50451    0.01176   42.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlog-scale coefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx 0.701164   0.008418   83.29   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n     Estimate Std. Error z value\n1|2 -0.835177   0.006287 -132.84\n2|3 -0.254419   0.005425  -46.90\n3|4  0.248568   0.005411   45.93\n4|5  0.829602   0.006277  132.17\n\n\nTo better understand the impact of assuming (or simulating) a different latent scale we fit \\(k - 1\\) binomial regressions and check the estimated coefficients. We are not simulating a specific beta for each outcome but simulating a scale effect is actually impacting the regression coefficients. When generating data for a binary outcome the linear predictor is composed by \\(\\eta = \\beta_0 + \\beta_1x\\). The threshold \\(\\alpha\\) and slope of the function can be estimated using \\(\\alpha = -\\frac{\\beta_0}{\\beta_1}\\) and the slope is \\(\\frac{1}{\\beta_1}\\) (Faraggi et al., 2003; Knoblauch & Maloney, 2012). Under the proportional odds assumption, there is only a change in thresholds \\(\\alpha\\) this a shift in the sigmoid along the \\(x\\) axis. When including a scale effect a change in the sigmoid is combined with a change in the slope.\n\nset.seed(2023)\nk &lt;- 4\nn &lt;- 1e5\nb1 &lt;- 3\nd1 &lt;- log(2)\ndat &lt;- data.frame(x = runif(n))\ndat &lt;- sim_ord_latent(~x, ~x, beta = b1, zeta = d1, prob0 = rep(1/k, k), data = dat, link = \"probit\")\n\ndat$y1vs234 &lt;- ifelse(dat$y &lt;= 1, 1, 0)\ndat$y12vs34 &lt;- ifelse(dat$y &lt;= 2, 1, 0)\ndat$y123vs4 &lt;- ifelse(dat$y &lt;= 3, 1, 0)\n\ndat$y &lt;- ordered(dat$y)\nfit &lt;- clm(y ~ x, scale = ~x, data = dat, link = \"probit\")\n\nfit1vs234 &lt;- glm(y1vs234 ~ x, data = dat, family = binomial(link = \"probit\"))\nfit12vs34 &lt;- glm(y12vs34 ~ x, data = dat, family = binomial(link = \"probit\"))\nfit123vs4 &lt;- glm(y123vs4 ~ x, data = dat, family = binomial(link = \"probit\"))\n\nfits &lt;- list(y1vs234 = fit1vs234, fit12vs34 = fit12vs34, fit123vs4 = fit123vs4)\n\nlapply(fits, function(x) coef(x)) |&gt; \n  bind_rows(.id = \"model\") |&gt; \n  mutate(xp = list(seq(0, 5, 0.01))) |&gt; \n  unnest(xp) |&gt; \n  ggplot(aes(x = xp, y = `(Intercept)` + x * xp)) +\n  geom_line(aes(color = model)) +\n  theme_minimal(15)\n\n\n\n\n\n\n\n\n\n\n\nWhen fitting a model with clm() the way of relaxing the proportional odds or parallel slopes assumption is including what is called a nominal effect. Basically when a predictor is included as a nominal effect, there will be \\(k - 1\\) coefficients (where \\(k\\) is the number of ordered levels). In this way, the odds ratio or the difference in \\(z\\) scores will not be the same across \\(k\\) levels.\nIf a scale effect is included in clm() is not immediately clear what happens to the proportional odds assumption. In fact, we are still estimating a single \\(\\beta\\) but we have extra parameters on the scale of the distribution. However, changing the scale of the distribution is the same as changing the slope. This is clear from Figure 3 where changing the scale of the latent distribution is the same as changing the slope of the cumulative probability function. The slope (\\(\\beta_1\\)) is related to the scale of the distribution (logistic or normal) as \\(\\beta_1 = \\frac{1}{s}\\). In fact, if we include a scale effect we are changing the scale of the underlying latent distribution and thus the slope of of the functions.\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nIn fact, if we simulate a model with a scale effect, odds ratios or \\(z\\) differences are no longer constant.\nWe will simulate the following situation where a binary predictor \\(x\\) has an effect both on location and scale of the logistic distribution.\n\nodds &lt;- function(p) p / (1 - p)\n\nn &lt;- 1e4 # large N\nk &lt;- 4\nb1 &lt;- log(3)\nz1 &lt;- log(2)\ndat &lt;- data.frame(x = rep(c(0, 1), each = n))\ndat &lt;- sim_ord_latent(~x, ~x, beta = log(3), zeta = log(2), prob0 = rep(1/k, k), data = dat, link = \"logit\")\n\n# model with scale effects, log(odds ratios) are different\n\nfit &lt;- clm(y ~ x, scale = ~ x, data = dat, link = \"logit\")\n\npreds &lt;- predict(fit, newdata = data.frame(x = c(0, 1)))$fit\npcum &lt;- t(apply(preds, 1, cumsum))[, 1:(k - 1)]\n\napply(pcum, 2, function(c) log(odds(c[1])/odds(c[2])))\n\n         1          2          3 \n-0.0436072  0.5202012  1.0755903 \n\n# model without scale effects, log(odds ratios) are equal\n\nfit0 &lt;- clm(y ~ x, scale = ~ 1, data = dat, link = \"logit\")\npreds &lt;- predict(fit0, newdata = data.frame(x = c(0, 1)))$fit\npcum &lt;- t(apply(preds, 1, cumsum))[, 1:(k - 1)]\napply(pcum, 2, function(c) log(odds(c[1])/odds(c[2])))\n\n        1         2         3 \n0.6107211 0.6107211 0.6107211 \n\n\n\ncat_latent_plot(c(0, 0 + b1), s = c(exp(0), (exp(0) + z1)), prob0 = rep(1/k, k), link = \"logit\")\n\n\n\n\n\n\n\n\nActually we can model this dataset using a non-proportional odds model. The model correctly capture the more complex probability structure but we are not able to recover the parameters because we simulated a scale effect.\n\nfit_nominal &lt;- clm(y ~ 1, nominal = ~ x, data = dat, link = \"logit\")\nsummary(fit_nominal)\n\nformula: y ~ 1\nnominal: ~x\ndata:    dat\n\n link  threshold nobs  logLik    AIC      niter max.grad cond.H \n logit flexible  20000 -26016.82 52045.65 8(0)  3.82e-12 7.8e+01\n\nThreshold coefficients:\n                Estimate Std. Error z value\n1|2.(Intercept) -1.12600    0.02325 -48.421\n2|3.(Intercept) -0.01120    0.02000  -0.560\n3|4.(Intercept)  1.09063    0.02305  47.320\n1|2.x            0.04227    0.03271   1.292\n2|3.x           -0.51759    0.02879 -17.981\n3|4.x           -1.07663    0.03052 -35.281\n\n\nEssentially we estimated two sets of thresholds one for the group 0 and one for the group 1. The difference with the previous model (beyond the fact that data are generated under a scale-location model) is that we need more parameters in a non-proportional odds model compared to the scale-location model. This is also more relevant when including more predictors. Also the likelihood ratio test correctly suggest that there is no relevant difference in the likelihood of the two models.\n\nanova(fit, fit_nominal)\n\nLikelihood ratio tests of cumulative link models:\n \n            formula: nominal: scale: link: threshold:\nfit         y ~ x    ~1       ~x     logit flexible  \nfit_nominal y ~ 1    ~x       ~1     logit flexible  \n\n            no.par   AIC logLik LR.stat df Pr(&gt;Chisq)\nfit              5 52044 -26017                      \nfit_nominal      6 52046 -26017  0.0478  1     0.8269"
  },
  {
    "objectID": "supplementary/location-scale-models.html#location-shift-models",
    "href": "supplementary/location-scale-models.html#location-shift-models",
    "title": "Location-Scale Ordinal Models",
    "section": "Location Shift Models",
    "text": "Location Shift Models\nThe location-shift model is considered an alternative way to include dispersion effects (together with location effects) (Tutz, 2022; Tutz & Berger, 2017; Tutz & Berger, 2020). The core of the models is including predictors on the thresholds \\(\\alpha\\) to decrease/increase the probability of response categories \\(k\\) together with the parameters on the location \\(\\boldsymbol{\\beta}\\)\nThey are similar to the location-scale models in terms of parsimony but different in terms of parameters interpretation. Given that these models cannot be directly fitted by the ordinal package we are not going into details but we included some references to understand the model parametrization. The models are implemented in the ordDisp R package (https://cran.r-project.org/web/packages/ordDisp/index.html).\nHowever, as noted by Berger and Tutz1 there is a correspondence between the partial proportional odds model and the location-shift model. Let’s simulate a model with a scale effect where the true model has a scale effect:\n\nlibrary(ordinal)\nlibrary(ordDisp)\n\nn &lt;- 1e3\nk &lt;- 3 # number of ordinal levels\nprobs0 &lt;- rep(1/k, k) # baseline probabilities\n\ndat &lt;- data.frame(x = rep(c(0, 1), each = n))\n\ndat &lt;- sim_ord_latent(~x, ~x, \n                      beta = b1, zeta = z1, \n                      prob0 = probs0, \n                      link = \"logit\", \n                      data = dat, \n                      simulate = TRUE)\n\nhead(dat)\n\n  x y          ys\n1 0 1 -2.51596242\n2 0 3  4.14198274\n3 0 2 -0.06790548\n4 0 2  0.43393823\n5 0 1 -1.86065820\n6 0 3  0.87618582\n\n\nNow let’s fit the location-shift model and the non-proportional odds model:\n\nfit_shift &lt;- ordDisp(y ~ x|x, data = dat)\nfit_nopo &lt;- clm(y ~ 1, nominal = ~x, data = dat, link = \"logit\")\n\nsummary(fit_shift)\n## \n## Call:\n## vglm(formula = formula1, family = cumulative(parallel = FALSE ~ \n##     1, reverse = reverse), data = DM, form2 = formula2, xij = formula3, \n##     checkwz = FALSE)\n## \n## Coefficients: \n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept):1 -0.71724    0.06736 -10.649  &lt; 2e-16 ***\n## (Intercept):2  0.65884    0.06671   9.876  &lt; 2e-16 ***\n## xx            -0.43974    0.08410  -5.229 1.71e-07 ***\n## xz            -0.63838    0.08481  -7.527 5.18e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Names of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2])\n## \n## Residual deviance: 4203.753 on 3996 degrees of freedom\n## \n## Log-likelihood: -2101.876 on 3996 degrees of freedom\n## \n## Number of Fisher scoring iterations: 3 \n## \n## No Hauck-Donner effect found in any of the estimates\n## \n## \n## Exponentiated coefficients:\n##        xx        xz \n## 0.6442065 0.5281488\nsummary(fit_nopo)\n## formula: y ~ 1\n## nominal: ~x\n## data:    dat\n## \n##  link  threshold nobs logLik   AIC     niter max.grad cond.H \n##  logit flexible  2000 -2101.88 4211.75 6(0)  9.75e-08 3.2e+01\n## \n## Threshold coefficients:\n##                 Estimate Std. Error z value\n## 1|2.(Intercept) -0.71724    0.06736 -10.649\n## 2|3.(Intercept)  0.65884    0.06671   9.876\n## 1|2.x           -0.12055    0.09634  -1.251\n## 2|3.x           -0.75892    0.09198  -8.251\n\nFirstly, there is a correspondence in the baseline thresholds:\n\n# location-shift model\ncoef(fit_shift)[1:(k - 1)]\n## (Intercept):1 (Intercept):2 \n##    -0.7172447     0.6588411\n\n# non proportional odds model\ncoef(fit_nopo)[1:(k - 1)]\n## 1|2.(Intercept) 2|3.(Intercept) \n##      -0.7172447       0.6588411\n\nThen also linear predictors \\(\\eta\\) are the same:\n\n# let's see the linear predictor for x = 1\n\n# location-shift model\npredict(fit_shift)[n + 1, ]\n## logitlink(P[Y&lt;=1]) logitlink(P[Y&lt;=2]) \n##         -0.8377921         -0.1000835\n\n# non proportional odds model\npredict(fit_nopo, newdata = data.frame(x = 1), type = \"linear.predictor\")$eta1[, -k]\n##          1          2 \n## -0.8377921 -0.1000835"
  },
  {
    "objectID": "supplementary/location-scale-models.html#footnotes",
    "href": "supplementary/location-scale-models.html#footnotes",
    "title": "Location-Scale Ordinal Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.researchgate.net/profile/Gerhard-Tutz/publication/305082385_Modelling_of_Varying_Dispersion_in_Cumulative_Regression_Models/links/578115fd08ae9485a43bd0f0/Modelling-of-Varying-Dispersion-in-Cumulative-Regression-Models.pdf↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulating ordinal data",
    "section": "",
    "text": "This is the online supplementary material for the tutorial-paper Ordinal regression models made easy. A tutorial on parameter interpretation, data simulation, and power analysis.\nIn the next links there are useful materials and examples that extended what presented in the tutorial-paper.\n\nUseful resources: a collection of books, blog posts, R packages and more about ordinal data\nOrdinal notes: a miscellanea of code snippets and quick examples about ordinal regression models\nLocation-scale models: a tutorial to fit, interpret and simulate location-scale cumulative link models.\nSimulating a GLM: a quick example on how to simulate data for a Generalized Linear Model (GLM).\nMixed-effects models: a quick example on how to simulate data for a Generalized Linear Model (GLM)."
  },
  {
    "objectID": "supplementary/mixed-effects-ordinal.html",
    "href": "supplementary/mixed-effects-ordinal.html",
    "title": "Random-effects Models",
    "section": "",
    "text": "We can extended the base CM model by including random-effects. For example, if the same participant responds to the same item/trial multiple times or responds to multiple items we need to take into account the nested data-structure. Agresti (2010) formalized the random-intercept CM model in Equation 1.\n\\[\nP(Y \\leq k) = g^{-1}[(\\alpha_k + u_i) - \\mathbf{X}\\boldsymbol{\\beta}]\n\\tag{1}\\]\nWhere \\(u_i\\) is the by-subject adjustment to the overall intercept-threshold. As in standard mixed-effects models, the random-effect of the intercept is sampled from a normal distribution with \\(\\mu = 0\\) and standard deviation \\(\\sigma_{u}\\), thus \\(u_i \\sim \\mathcal{N}(0, \\sigma_u)\\). We can use the same simulation strategies (sampling from multinomial distribution or latent approach) of the paper but introducing the random-effect. We simulate \\(N = 100\\) subjects divided into two groups. Each subject performs \\(100\\) trial responding on a ordinal item.\n# Simulation parameters\nset.seed(2024)\nk &lt;- 4\nprobs0 &lt;- rep(1/k, k)\nN &lt;- 100 # sample size\nn &lt;- N/2\nnt &lt;- 100 # number of trials\nb1 &lt;- log(3)\nsb0 &lt;- 0.5 # intercept standard deviation\ngroup &lt;- c(0, 1) # group\nalpha &lt;- prob_to_alpha(probs0, link = \"logit\")\n\n# Data structure\ndat0 &lt;- dat1 &lt;- expand.grid(id = 1:n, trial = 1:nt)\ndat0$group &lt;- 0\ndat1$group &lt;- 1\ndat0$id &lt;- rep(1:n, nt)\ndat1$id &lt;- rep((n + 1):N, nt)\ndat &lt;- rbind(dat0, dat1)\n\nalphai &lt;- rnorm(N, 0, sb0)"
  },
  {
    "objectID": "supplementary/mixed-effects-ordinal.html#sampling-from-multinomial-distribution",
    "href": "supplementary/mixed-effects-ordinal.html#sampling-from-multinomial-distribution",
    "title": "Random-effects Models",
    "section": "Sampling from multinomial distribution",
    "text": "Sampling from multinomial distribution\n\nset.seed(2024)\ncump &lt;- lapply(alpha, function(a) plogis(with(dat, (a + alphai[id]) - (b1 * group))))\np &lt;- t(apply(cbind(0, data.frame(cump), 1), 1, diff))\nnames(p) &lt;- paste0(\"py\", 1:k)\ndat &lt;- cbind(dat, p)\n\ndat$y &lt;- apply(dat[, 4:ncol(dat)], 1, function(p) sample(1:k, 1, replace = TRUE, prob = p))\ndat$y &lt;- ordered(dat$y)\n\nfit &lt;- clmm(y ~ group + (1|id), data = dat, link = \"logit\")\nsummary(fit)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: y ~ group + (1 | id)\ndata:    dat\n\n link  threshold nobs  logLik    AIC      niter     max.grad cond.H \n logit flexible  10000 -12676.72 25363.43 324(1014) 4.61e-03 1.4e+02\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 0.2582   0.5081  \nNumber of groups:  id 100 \n\nCoefficients:\n      Estimate Std. Error z value Pr(&gt;|z|)    \ngroup   1.1178     0.1086   10.29   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -1.14160    0.07813 -14.611\n2|3 -0.04105    0.07697  -0.533\n3|4  1.06992    0.07777  13.757"
  },
  {
    "objectID": "supplementary/mixed-effects-ordinal.html#sampling-from-the-latent-variable",
    "href": "supplementary/mixed-effects-ordinal.html#sampling-from-the-latent-variable",
    "title": "Random-effects Models",
    "section": "Sampling from the latent variable",
    "text": "Sampling from the latent variable\n\nset.seed(2024)\nystar &lt;- with(dat, alphai[id] + (b1*group)) + rlogis(nrow(dat), 0, 1)\ndat$y2 &lt;- ordered(findInterval(ystar, alpha) + 1)\nfit2 &lt;- clmm(y2 ~ group + (1|id), data = dat, link = \"logit\")\nsummary(fit2)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: y2 ~ group + (1 | id)\ndata:    dat\n\n link  threshold nobs  logLik    AIC      niter    max.grad cond.H \n logit flexible  10000 -12795.34 25600.68 302(910) 1.32e-03 1.5e+02\n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id     (Intercept) 0.2729   0.5224  \nNumber of groups:  id 100 \n\nCoefficients:\n      Estimate Std. Error z value Pr(&gt;|z|)    \ngroup   1.0646     0.1111    9.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -1.07709    0.07986 -13.487\n2|3  0.01468    0.07883   0.186\n3|4  1.11972    0.07965  14.057\n\n\nIn both simulations we are able to recover the parameters. For fitting the model we use the clmm() function that allows specifying the random-effects structure. The syntax is the same as the lme4 package for standard mixed-effects models.\nWe can extend the simulation including also random slopes. The group effect need to be a within effect now, thus we can imagine to have two conditions in the experiment with 100 trials each. We can extend the Equation 1 with Equation 2. \\(\\beta_{1_i}\\) are the by-subjects adjustments to the overall \\(\\beta_1\\) effect, still sampled from a normal distribution.\n\\[\nP(Y \\leq k) = g^{-1}[(\\alpha_k + u_i) - (\\beta_1 + \\beta_{1_i})X_1]\n\\tag{2}\\]\nLet’s use the latent formulation directly:\n\nset.seed(2024)\ndat &lt;- expand.grid(id = 1:N, trials = 1:nt, cond = c(0, 1))\nsb1 &lt;- 0.2 # slope standard deviation\nalphai &lt;- rnorm(N, 0, sb0)\nb1i &lt;- rnorm(N, 0, sb1)\n\nystar &lt;- with(dat, alphai[id] + ((b1 + b1i[id]) * cond)) + rlogis(nrow(dat), 0, 1)\ndat$y &lt;- ordered(findInterval(ystar, alpha) + 1)\nfit &lt;- clmm(y ~ cond + (cond|id), data = dat, link = \"logit\", )\nsummary(fit)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: y ~ cond + (cond | id)\ndata:    dat\n\n link  threshold nobs  logLik    AIC      niter     max.grad cond.H \n logit flexible  20000 -25434.35 50882.69 626(2508) 1.09e-02 1.1e+02\n\nRandom effects:\n Groups Name        Variance Std.Dev. Corr   \n id     (Intercept) 0.29342  0.5417          \n        cond        0.06647  0.2578   -0.023 \nNumber of groups:  id 100 \n\nCoefficients:\n     Estimate Std. Error z value Pr(&gt;|z|)    \ncond  1.10553    0.03729   29.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -1.08130    0.05828 -18.553\n2|3  0.03904    0.05758   0.678\n3|4  1.15537    0.05818  19.858"
  },
  {
    "objectID": "supplementary/simulating-glm.html",
    "href": "supplementary/simulating-glm.html",
    "title": "Simulating Generalized Linear Models",
    "section": "",
    "text": "Introduction\nThis document present a general workflow to simulate data for a binomial generalized linear model. This simulation is similar to what is presented in the paper for the special case of an ordinal regression.\n\n\nSimulation\nThe general formula for a binomial GLM is presented in Equation 1. Where \\(g^{-1}(\\cdot)\\) is the inverse of the link function. In the case of a binomial regression with a logit link function \\(g(p) = \\text{logit}(p) = log(\\frac{p}{1 - p})\\) and \\(g^{-1}[logit(p)] = \\text{invlogit}(p) = \\frac{e^p}{1 + e^p}\\).\n\\[\nP(Y = 1|\\mathbf{X}) = g^{-1}(\\mathbf{X} \\boldsymbol{\\beta})\n\\tag{1}\\]\nThe linear combination of parameters \\(\\eta = \\mathbf{X} \\boldsymbol{\\beta}\\) define the true probabilities. Then the random part of the model (in this case a logistic distribution) define the random variability around the true values. To make an example, let’s assume to predict \\(P(Y = 1)\\) with a numerical variable \\(x\\) coming from an uniform distribution \\(x \\sim \\mathcal{U}(0, 1)\\). As in standard regression we have two parameters:\n\n\\(\\beta_0\\): the intercept. The probability of having 1 when all predictors are fixed to zero (\\(P(Y = 1|x = 0)\\)).\n\\(\\beta_1\\): the slope. Is the increase in the log-odds of \\(Y = 1\\) for a unit increase in \\(x\\).\n\nThus we can choose two values for the parameters and plot the true relationship between \\(x\\) and \\(P(Y = 1)\\). The link function is qlogis() and the inverse of the link function is plogis(). Parameters are expressed in the scale of the of the link function.\n\nb0 &lt;- qlogis(0.01) # probability of success when x = 0, in the link function space\nb1 &lt;- 10 # increase in the log odds of success for a unit increase in x\nx &lt;- seq(0, 1, 0.01) # x values\np &lt;- plogis(b0 + b1 * x)\n\nplot(x, p, type = \"l\")\n\n\n\n\n\n\n\n\nThe curve depicts the true probability of success for each value of \\(x\\). To include the random error we need to sample the observed values from a Binomial distribution using the vector of probabilities p. We sample a vector of 0 and 1.\n\nns &lt;- length(x) # number of subjects\ny &lt;- rbinom(ns, 1, p)\nsummary(p)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0100  0.1096  0.5999  0.5396  0.9481  0.9955 \n\npar(mfrow = c(1,2))\n\n# binary values\nplot(x, jitter(y, amount = 0.05))\nlines(x, p)\n\n# grouped values\nxc &lt;- cut(x, seq(0, 1, 0.1), include.lowest = TRUE)\nyc &lt;- tapply(y, xc, mean)\n\nplot(as.integer(unique(xc))/10, yc)\nlines(x, p)\n\n\n\n\n\n\n\n\nIn practical terms, the \\(\\eta\\) define the true probability of success for each observation (i.e., combination of predictors \\(\\mathbf{X}\\)) and then the random part is introduced by sampling from the assumed probability distribution.\nThen we can fit the logistic regression using glm() and we should recover the simulation parameters. Increasing the number of trials/observations will reduce the distance between the simulated the true values.\n\ndat &lt;- data.frame(x, y)\nfit &lt;- glm(y ~ x, data = dat, family = binomial(link = \"logit\"))\nsummary(fit)\n\n\nCall:\nglm(formula = y ~ x, family = binomial(link = \"logit\"), data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -3.2173     0.6743  -4.772 1.83e-06 ***\nx             7.4514     1.3814   5.394 6.89e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.338  on 100  degrees of freedom\nResidual deviance:  80.606  on  99  degrees of freedom\nAIC: 84.606\n\nNumber of Fisher Scoring iterations: 5\n\n# model\ncoef(fit)\n\n(Intercept)           x \n  -3.217343    7.451356 \n\n# truth\nc(b0 = b0, b1 = b1)\n\n      b0       b1 \n-4.59512 10.00000 \n\n\nChanging \\(\\beta_0\\) will affect the lower bound of the sigmoid curve while \\(\\beta_1\\) determine the slope of the function. Figure 1 depicts logistic curves with different parameters.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nThis method is implemented in the paper when sampling from a categorical distribution. In the case of ordinal data we need \\(k - 1\\) equations where \\(k\\) is the number of ordered categories."
  }
]