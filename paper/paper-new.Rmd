---
title             : "Ordinal regression models made easy. A tutorial on parameter interpretation, data simulation, and power analysis."
shorttitle        : "Ordinal regression models made easy"

author: 
  - name          : "Filippo Gambarota"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "filippo.gambarota@unipd.it"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Gianmarco Altoè"
    affiliation   : "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Department of Developmental Psychology and Socialization, University of Padova, Italy"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  Abstract here
  
keywords          : "ordinal, likert, simulations, power"
wordcount         : "X"

bibliography      : "`r filor::fil()$bib`"

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output: 
  papaja::apa6_pdf:
    latex_engine: xelatex
    header_includes:
      - \usepackage{booktabs, caption, longtable, colortbl, array, hhline}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      dev = "pdf",
                      fig.align = "center",
                      message = FALSE,
                      warning = FALSE)
```

```{r packages, include = FALSE}
library(tidyverse)
library(ordinal)
library(here)
library(flextable)
devtools::load_all()
```

```{r funs, include = FALSE}
funs <- filor::get_funs(here("R", "utils.R"))
```

# Introduction

Psychological research make an extensive use of ordinal data. One of the main reason is probably the usage of Likert scales [@Likert1932-xw]. Ordinal data refers to a specific type of measurement scale [@Stevens1946-te] where ordered numbers are assigned to a variable. Compared to nominal scale and as the name suggest the labels are ordered. Compared to interval or ratio scales there is no explicit assumption about the distance between labels. (rivedi questo). An example is asking people the degree of agreement about a certain statement using a scale from 1 (no agreement) to 7 (total agreement). Answering 4 compared to 2 suggest an higher agreement but we cannot affirm that there is two times the agreement compared to the second answer. @Stevens1946-te suggest that for ordinal variables is appropriate to calculate ranks-based statistics (e.g., median or percentiles) instead of metric statistics (e.g., mean or standard deviation). This distinction in terms of the appropriateness of certain descriptive statistics is also relevant when modeling data. In Psychology especially when using item-based measures (questionnaires, surveys, etc.) the common practice is using a normal linear regression that makes an explicit assumption about metric features of the response variable. @Liddell2018-wu reviewed the psychological literature using likert-based measures and reported how the majority of papers used metric-based statistical models. In the same work, @Liddell2018-wu showed extensive examples and simulations about the potential pitfalls of treating an ordinal variable as metric. They reported problems in terms of lack of power, inversion of the effects and distortion in estimating the effect size (rivedi meglio qui). (vedi se c'è qualche altro lavoro che fa vedere questa cosa).

### Ordinal regression models

Despite the actual modeling proposal by @Liddell2018-wu, there is a class of regression models taking into account the ordinal nature of the response variable without making metric assumptions. We can class this general class of models as *ordinal regression*. The nomenclature of these models can be confusing mainly because there are several subclasses of models with different assumptions and structures [@Tutz2022-dg]. @Tutz2022-dg and @Burkner2019-aw provide a clear and updated taxonomy of ordinal regression models. We can identify three main classes: *cumulative models* [@McCullagh1980-cw], *sequential models* [@Tutz1990-fe] and *adjacent category models*. The cumulative is the mostly used model assuming the existence of a latent variable that when segmented using thresholds produces the observed ordinal variable. The psychological process underlying the response is clearly formalized in the signal detection theory framework where the respondent (vedi come sistemare). The sequential model as suggested by the name is appropriate when modelling sequential processes. Assuming to have 5 response options, the sequential model assume that responding 3 means already reaching the states 1 and 2. A clear example is proposed by @Burkner2019-aw where the marriage duration in years is predicted as a function of some explanatory variables. For each level of the response variable there is a latent distribution where the step between a marriage year $k = 1$ and the next years $k > 1$ is modeled by the sequential regression. When comparing $k$ with $k > 1$, everything lower than $k$ is assumed to be already reached. The adjacent category model compare the category $k$ with $k + 1$ still assuming a latent distribution for each $k$. As suggested by @Tutz2022-dg the adjacent-category model can be seen as a series of binary binomial regressions taking into account the order of the categories.


## notation

- idea di mettere $F$ o qualcosa di simile per la pdf ($\Phi$ e.g., per la normale)
- $Y^\star$ per la latente
- $k$ numero di livelli ordinali
- $\alpha$ per le soglie

per notazione https://people.vcu.edu/~dbandyop/BIOS625/CLM_R.pdf e @Burkner2019-aw

per la parte di notazione e quella più formale anche @DeCarlo2010-lj

## cumulative vs others

- cumulative quando una variabile continua latente viene tagliata in modo osservato. Le likert possono essere gestite in questo modo

from @Burkner2019-aw:

> For  many  ordinal  variables,  the  assumption  of  a   single underlying continuous variable, as in cumulative  models, may not be appropriate. If the response can  be understood as being the result of a sequential pro- cess, such that a higher response category is possible  only after all lower categories are achieved, the sequen- tial model proposed by Tutz (1990) is usually appropri- ate.

vedi se fare k-1 logistiche è più o meno la stessa cosa

in realtà credo che l'adiaject category e sequential model comunque abbiamo la proportional odds assumption

```{r}
#| eval: false
library(brms)
library(ordinal)

dat <- data.frame(x = rnorm(100))
dat <- sim_ord_latent(~x, By = log(2), probs = c(0.5, 0.2, 0.2, 0.1), data = dat, link = "logit")

fit_clm <- clm(y ~ x, data = dat, link = "logit")
fit_cum <- brm(y ~ x, data = dat, family = cumulative(link = "logit"))
fit_seq <- brm(y ~ x, data = dat, family = sratio(link = "logit"))
fit_adj <- brm(y ~ x, data = dat, family = acat(link = "logit"))

```

come sottolinea @Burkner2019-aw e @Tutz2020-xq i sequential model sono adatti per un certo tipo di dati dove l'interesse è vedere l'impatto dei predittori sulla transizione ad un categoria $r$ considerando che la categoria $r - 1$ è stata raggiunta.

@Burkner2019-aw suggest that while cumulative and sequentials models have a clear empirical distinction the adjiacent category model is usually used for computational advantages. The probability of passing from one category to the other is modeled as a function of predictors.


## Risolvere un pochino la nomenclatura

https://cran.r-project.org/web/packages/ordinal/vignettes/clm_article.pdf

> The name, cumulative link models is adopted from Agresti (2002), but the model class has
been referred to by several other names in the literature, such as ordered logit models and
ordered probit models (Greene and Hensher 2010) for the logit and probit link functions. The
cumulative link model with a logit link is widely known as the proportional odds model due
to McCullagh (1980) and with a complementary log-log link, the model is sometimes referred
to as the proportional hazards model for grouped survival times.

Da citare in qualche modo @Kemp2021-dj
    
> “As a matter of fact, most of the scales used widely and effectively by psychologists are ordinal scales. In the strictest propriety the ordinary statistics involving means and standard deviations ought not to be used with these scales, for these statistics imply a knowledge of something more than the relative rank-order of data. On the other hand for this ‘illegal’ statisticizing there can be invoked a kind of pragmatic sanction: In numerous instances it leads to fruitful results.” (Stevens, 1946, p. 679.)

> It is straightforward to construct ordinal scales that do not involve rank ordering. For example, one can take the first element encountered and arbitrarily assign it the number 100. If the next element encountered is smaller it is given a smaller arbitrary number, 53 say. If the third element is between these two, it can be given the number 86, and so on. If this construction method is used, the difference between the elements assigned the numbers 70 and 80 will not in any important sense be equal to the difference between the elements assigned 90 and 100, and the intervals between the numbers are not really interpretable. Note, too, that monotonic transformations of the scale essentially leave the mea- sure unaffected.

interessante come @Cliff2016-ck descriva che la maggiorparte dellle domande di ricerca in psicologia siano in riferimento alla location di un costrutto e possano essere gestite in modo ordinale


The use of ordinal data is widespread in Psychology. Usually items from questionnaires are created using Likert scales where a certain psychological traits is measured using an intuitive scale with 3-5 or more anchor points. These measure cannot be considered *metric* measure on interval or ratio scales [stevens] but the categories are ordered.

Other examples here

Despite the usage of ordinal variables, statistical models made for these type of data are rarely used in Psychology. @Liddell2018-wu reported that the majority of published papers using likert-like measures used standard methods to analyze the data. In practical terms, they use *metric* models where the response variable cannot be considered fully numeric (vedi se c'è un termine tipo scala a rapporti).

altro sui modelli

Theoretically, using a *metric* for ordinal data is not appropriate but understanding the actual impact is not straightforward. @Liddell2018-wu did a comprehensive work about pitfalls of analzying ordinal data as metric. They showed that metric models produce higher type-1 and type-2 errors compared to the ordinal models. In particular, given the bounded nature of ordinal data, difference in the underlying latent distribution are not always captured by the metric model that simply estimate the mean of the ordinal variable. This is even more relevant when the underluing variance of the ordinal variables are not homogeneous. Furthermore they presented some situations where the metric model could be wrong in the opposite direction, finding an effect with the wrong sign (type-s error). 

Finally, they also demonstrated that even in the best condition where e.g. comparing two groups the variance are equal, the ordinal model is more powerful than the metric model given the underestimation of the true effect size from the latter.

> Some authors have argued that, despite the ordinal character of individual Likert items, averaged ordinal items can have an emergent property of an interval scale and so it is appropriate to apply metric methods to the averaged values (e.g., Carifio & Perla, 2007, 2008).

vedi come gestire questo

anche questo è importante
> Ordered-probit models typically assume that the thresholds (θ k) are the same across all groups because the thresholds are theoretically linked to the response measure, not to the predictor value. For example, when asked, “How happy are you?” with response options ‘1’ = very unhappy, ‘2’ = mildly unhappy, ‘3’ = neutral, ‘4’ = mildly happy, ‘5’ = very happy,” the latent thresholds between ordinal levels are assumed to be implicit in the phrasing of the question, regardless of other aspects of the respondent or situation. In other words, the thresholds are assumed to be part of the measurement procedure, not dependent on the value of the predictor or covariate. This can be technically referred to as a type of measurement invariance.

Da citare come paper iniziale sui modelli [@McCullagh1980-cw].

questo per la tassonomia dei diversi modelli ordinali [@Tutz2022-dg]

da vedere magari per una critica tipo Gomila sul binary model [@Robitzsch2020-la]

## Metriv vs ordinal models

- the main difference is that the metric model assign a number to each label of the discrete variable assuming that the distance is the same. [@Liddell2018-wu]

## lm on latent vs ordinal

as suggested https://people.vcu.edu/~dbandyop/BIOS625/CLM_R.pdf, running a lm on the latent variables gives similar parameter as the clm. Of course, we are able to do this with real data given that the ordinal variable is the observed version of an unobserved latent variable. But in simulation this is useful to understand what the cumulative model is doing.

```{r}
dat <- data.frame(
  x1 = rnorm(1e5),
  x2 = rnorm(1e5)
)

dat <- sim_ord_latent(~x1 + x2, By = c(1, 0.5), probs = rep(1/5, 5), link = "probit", data = dat)

fit <- lm(ys ~ x1 + x2, data = dat)
summary(fit)

clm(y ~ x1 + x2, data = dat, link = "probit")
coef(fit)/sigma(fit)

```

## Kruscke parametrization

@Liddell2018-wu and @Kruschke2015-re proposed an alternative parametrization to understand the model parameters. They used a probit model where thresholds and regression parameters are estimated on the scale of the ordinal variable compared to standard ordinal regression where they refers to the quantile of the latent variables for the threshold and z score or odds ratio for the regression coefficients. They implemented the model in Jags and provided some equations and R functions to convert from the standard parametrization to the proposed one.

The proposed model is fitted within a Bayesian framework using either Jags (citation) or Stan (citation). Kruskche proposed a simple method to convert the parameters fitted with a standard model within the proposed parametriazion. The main improvement regards mapping the values (for thresholds $\alpha_i$ and slopes $\beta$) from latent standard distribution (gaussian or logistic) into the scale of the $y$ ordinal value. The scale of the variable depend on the numeric labels assigned to ordered categories. There is an additional feature of the proposed parametrization where the first and the last thresholds are fixed respectively to $\alpha_1 + 0.5$ and $\alpha_k - 0.5$ where other thresholds are estimated.

@Kurz2023-zv explain how to convert a model fitted with the `brms` package (citation) (an R package for regression modeling using *stan*) into the Kruschke [-@Kruschke2015-re] parametrization.

Using the function proposed by @Kruschke2015-re (metti ref ad osf) and the equations presented in @Kurz2023-zv the `clm_to_ord()` function convert parameters fitted with the `clm` function into the corresponding parameters for the latent $Y^\star$ variable considering the actual range of values (from 1 to $k$).

```{r, echo=FALSE, results='asis'}
filor::print_fun(funs$clm_to_ord)
```

```{r}
#| echo: true

dat <- data.frame(
  x = rnorm(1e5)
)

dat <- sim_ord_latent(~x, By = 1, probs = rep(1/5, 5), link = "probit", data = dat)
fit <- clm(y ~ x, data = dat, link = "probit")
clm_to_ord(fit)
```

## Gelman parametrization

rivedi questa parametrizzazione in @Gelman2020-tg

```{r}
#| eval: false
dat <- data.frame(x = runif(1e5, 0, 1))
dat <- sim_ord_latent(~x, By = 2, probs = c(0.5, 0.2, 0.1, 0.1, 0.1), data = dat, link = "logit")
fit_clm <- clm(y ~ x, data = dat, link = "logit")

num_latent_plot(dat$x, 2, probs = c(0.5, 0.2, 0.1, 0.1, 0.1), link = "logit")

summary(fit_clm)

ths <- -(fit_clm$alpha/fit_clm$beta)
betas <- 1/fit_clm$beta

dat <- cbind(dat, dummy_ord(dat$y))

fit <- glm(y1234vs5 ~ x, data = dat, family = binomial(link = "logit"))

coefs <- coef(fit)

x <- seq(0, 1, 0.01)

plot(x, plogis(coefs[1] + coefs[2] * x), type = "l")


-(fit_clm$alpha/fit_clm$beta)
1/fit_clm$beta

```



## Odds ratios and cumulative odds ratios

- https://online.stat.psu.edu/stat504/lesson/4/4.1

In this section we introduce the main concept to understand the effects of a logit model.

## Demonstrating the proportional odds assumption

basically given that the effect of $\beta$ is constant the odds ratio is independent from the thresholds $\alpha_j$ [@Liu2023-bp]. In the current paper we are simulating data using a single set of $\beta$s thus the proportional odds assumption is true. @Liu2023-bp reviewed the available methods to test this assumption.

- https://hbiostat.org/ordinal/impactpo.pdf blog su proportional odds, mi pare di capire che non sia così problematica come assunzione

sarebbe da capire quali sono i rischi di assumere questo ed eventualmente se simulare non proportional odds è troppo complicato.


Let's simulate the effect of a binary predictor on ordinal scale 1-5:

```{r}
#| echo: true
b1 <- log(3) # log odds ratio
n <- 1e4
x <- rep(c("a", "b"), each = n/2)
dat <- data.frame(x = x)
probs <- rep(1/5, 5) # for the group "a", uniform probabilities
dat <- sim_ord_latent(~x, By = b1, probs = probs, data = dat, link = "logit")
fit <- clm(y ~ x, data = dat, link = "logit")
pr <- predict(fit, data.frame(x = unique(x)))$fit
pr
```
Basically the proportional odds suggest that:

$$
\text log(\frac{P(y \leq 1)}{P(y > 1)})
$$
Is the same regardless the level of the $x$ predictor. Thus:

$$
\text{log}\left(\frac{\frac{P(y \leq 1|x_0)}{P(y > 1|x_0)}}{\frac{P(y \leq 2|x_0)}{P(y > 2|x_0)}}\right) = \text{log}\left(\frac{\frac{P(y \leq 1|x_1)}{P(y > 1|x_1)}}{\frac{P(y \leq 2|x_1)}{P(y > 2|x_1)}}\right)
$$

```{r}
#| echo: true
a_or1vs2345 <- filor::odds(pr[1, 1]) # 1 vs 2 3 4 5
a_or12vs345 <- filor::odds(sum(pr[1, 1:2])) # 1 vs 2 3 4 5

b_or1vs2345 <- filor::odds(pr[2, 1]) # 1 vs 2 3 4 5
b_or12vs345 <- filor::odds(sum(pr[2, 1:2])) # 1 vs 2 3 4 5

c(xa = log(a_or1vs2345 / a_or12vs345), xb = log(b_or1vs2345 / b_or12vs345))
```

Proportional odds can be also visualized by plotting the cumulative probabilities of $y$, in terms of $g(P(y \leq j))$ (where $g()$ is the logit link function) as a function of the predictor $x$. If the proportional odds assumptions holds the slopes are parallel (also known as parallel regression assumption). The Figure \@ref(fig:poa-plot-example) depicts the assumption of proportional odds in the probability and logit space.

```{r poa-plot-example, fig.cap="POA assumption"}
k <- 5 # number of levels for y
grid <- data.frame(x = seq(0, 1, 0.1)) # numeric predictor
probs <- get_probs(~x, 5, rep(1/k, k), data = grid, link = "logit") # probabilities
cprobs <- apply(probs, 1, cumsum, simplify = FALSE)
cprobs <- data.frame(do.call(rbind, cprobs))
names(cprobs) <- sprintf("y<=%s", 1:k)
cprobs <- cprobs[, -ncol(cprobs)] # remove the last
grid <- cbind(grid, cprobs)

grid <- grid |> 
  pivot_longer(starts_with("y")) |> 
  mutate(lp = qlogis(value))

poa_probs_plot <- grid |> 
  ggplot(aes(x = x, y = value, color = name)) +
  geom_line() +
  theme_minimal(15) +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        axis.title.y = element_blank()) +
  ggtitle("Probability")

poa_lp_plot <- grid |> 
  ggplot(aes(x = x, y = lp, color = name)) +
  geom_line() +
  theme_minimal(15) +
  theme(axis.title.y = element_blank()) +
  ggtitle("Linear Predictor")

bl <- cowplot::get_legend(
  poa_probs_plot
)

poa_plot <- cowplot::plot_grid(
  poa_probs_plot + theme(legend.position = "none"),
  poa_lp_plot + theme(legend.position = "none")
)

cowplot::plot_grid(poa_plot, bl, ncol = 1, rel_heights = c(0.9, 0.1))
```

@Harrell-2015-no proposed a very intuitive plot to assess the proportional odds assumption and eventually the degree of deviation from the ideal case. Basically predictor is plotted against the logit of the cumulative probability. Distances between pairs of symbols should be similar across levels of the predictors. Numerical predictors can be binned before plotting the corresponding logit. Figure \@ref(fig:harrel-poa-plot) depicts an example with simulated data satisfying the proportional odds assumption.

```{r harrel-poa-plot, fig.cap="Harrel Poa Plot"}
k <- 5 # number of levels for y
grid <- data.frame(x = seq(0, 1, 0.1)) # numeric predictor
probs <- get_probs(~x, 5, rep(1/k, k), data = grid, link = "logit") # probabilities
cprobs <- apply(probs, 1, cumsum, simplify = FALSE)
cprobs <- data.frame(do.call(rbind, cprobs))
names(cprobs) <- sprintf("y<=%s", 1:k)
cprobs <- cprobs[, -ncol(cprobs)] # remove the last
grid <- cbind(grid, cprobs)

grid <- grid |> 
  pivot_longer(starts_with("y")) |> 
  mutate(lp = qlogis(value))

grid |> 
  ggplot(aes(x = lp, y = x)) +
  geom_point(aes(color = name, 
                 shape = name),
             size = 3) +
  theme_minimal(20) +
  xlab("Logit") +
  theme(legend.position = "bottom",
        legend.title = element_blank())
```

From a statistical point of view, the proportional odds assumption can be assessed by fitting $k - 1$ binomial regressions and checking if the estimated $\beta$ is similar between regressions. The regressions are estimated by creating $k - 1$ dummy variables from the ordinal $y$. The code below show that simulating data with the proportional odds create $k - 1$ binomial regression with similar $\beta$s. In fact, fitting $k - 1$ binomial regressions can be considered also an alternative to fitting an ordinal regression [see @Gelman2020-tg] with more flexibility in parameters (there will be $k - 1$ regression coefficients instead of a single one) but losing the latent intepretation (magari qualcosa di più qui).

```{r}
#| echo: true
k <- 4
n <- 1e5
dat <- data.frame(
  x = runif(n)
)

dat <- sim_ord_latent(~x, By = 5, probs = rep(1/k, k), data = dat, link = "logit")

# create dummy variables

dat$y1vs234 <- ifelse(dat$y <= 1, 1, 0)
dat$y12vs34 <- ifelse(dat$y <= 2, 1, 0)
dat$y123vs4 <- ifelse(dat$y <= 3, 1, 0)

fit1vs234 <- glm(y1vs234 ~ x, data = dat, family = binomial(link = "logit"))
fit12vs34 <- glm(y12vs34 ~ x, data = dat, family = binomial(link = "logit"))
fit123vs4 <- glm(y123vs4 ~ x, data = dat, family = binomial(link = "logit"))

car::compareCoefs(fit123vs4, fit12vs34, fit1vs234)
```


spiegare qui la differenza tra proportional and non proportional odds @Tutz2020-xq.

## Logit vs Probit model

When fitting an ordinal regression the two mostly used link functions are the *probit* and *logit*. From the distribution point of view the two functions are very similar. The *probit* model is based on a cumulative Gaussian distribution while the *logit* model is based on a logistic distribution. Figure \@ref(fig:logit-vs-probit) depict the two cumulative distributions.

```{r logit-vs-probit, fig.cap="Logit vs Probit"}
par(mfrow = c(1,2))

# density

curve(dnorm(x), -5, 5, lwd = 2, ylab = "Density", main = "Probability Density", cex.lab = 1.2, cex.axes = 1.2)
curve(dlogis(x), -5, 5, lwd = 2, col = "firebrick", add = TRUE)

# cdf

curve(pnorm(x), ylim = c(0, 1), xlim = c(-5, 5), y = "Probability", lwd = 2, main = "Cumulative Probability",
      cex.lab = 1.2, cex.axes = 1.2)
curve(plogis(x), ylim = c(0, 1), xlim = c(-5, 5), y = "Probability", col = "firebrick", lwd = 2, add = TRUE)
```

Given the different underlying distribution, the parameters have a different interpretation under the two models. The probit model assume a latent standard normal distribution with and $\sigma = 1$. The logit model assume a logistic distribution with $\sigma^2 = \pi^2/3$. Thus regression coefficients $\beta$ represents the increase in standard deviations units. The interpretation in terms of latent distribution is particularly useful for the probit model where $\beta$s can be interpreted in a Cohen's $d$ like manner. Furthermore, there is the possibility to directly map parameters from signal detection theory [@Stanislaw1999-jr; @Green1966-gy] into an ordinal probit regression. In practical terms, the thresholds are the criterion cut-offs and the $\beta_1$ is the d$'$ parameter [@DeCarlo1998-ay; @Knoblauch2012-to].

The latent formulation of the ordinal regression model allow to think in standard linear regression terms for choosing and interpreting model parameters before converting into the probability space. Both the standard Normal and the logistic distribution are defined with a location ($\mu$) and a scale ($s$) parameter. For the Normal distribution, $\mu$ is the mean and scale is the variance ($\sigma^2$). For the logistic distribution the variance is $\sigma^2=\frac{s^2\pi^2}{3}$. Thus fixing $\mu$ and $\sigma^2 = 1$ (the default), the two distributions are similar with the logistic having more variance. For this reason, the latent formulation for parameter intepretation is particularly useful for the probit model because the $\beta$ is directly interpreted in standard deviaton units that by default are fixed to 1. With a binary predictor $x$, $\beta$ is the shift of the latent distribution increasing $x$ by one unit. This can be directly interpreted as a standardized mean difference effect size (e.g., Cohen's $d$). This is the same for the logistic distribution but a unit increase in $x$ shift the latent distribution by $\sigma = \sqrt{\frac{s^2 \pi^2}{3}} = \frac{s\pi}{\sqrt{3}}$.^[In the case of a standard logistic distribution ($s^2 = 1$), the standard deviation is $\frac{\pi}{\sqrt{3}}$].


## Simulating data

There are mainly two ways to simulate data. The first method concerns simulating starting from the latent formulation of the ordinal model. Basically we can simulate the underlying latent distribution and then fixing the thresholds converting the latent continuous variable into the observed ordinal variable.

### simulating using probabilities

The first method to simulate ordinal data as a function of predictors is by calculating the true probabilities $g^{-1}(\eta)$ as a function of predictors and then sample from a multinomial distribution. This is similar to the general method to simulate data for a generalized linear model where the linear predictors $\eta$ are calculated and data are generated from the corresponding probability distribution of the random component. The following code box simulate a $n$ binary trials with a continuous predictor $x$.

```{r}
xl <- seq(0, 1, 0.1)
x <- rep(xl, 10)
n <- length(x)
b0 <- qlogis(0.01)
b1 <- 8
lp <- b0 + b1 * x # linear predictor
y <- rbinom(n, 1, plogis(lp)) # sampling from a binomial distribution using the vector of probabilities
p <- tapply(y, x, mean)
plot(unique(x), p, pch = 19)
curve(plogis(x, -(b0/b1), 1/b1), add = TRUE, lwd = 2, col = "firebrick")
```

We can apply the same idea to an ordinal outcome but we need $k - 1$ equations where $k$ is the number of ordinal levels. Let's simulate a similar design with a continuous $x$ predictor and $k = 4$ options. We fix the baseline probabilities where $x = 0$ as uniform thus $p(y_1) = p(y_2) = ... p(y_k) = 1/k$. The general workflow for the simulation is:

1. Define $n$ (number of observations) and $k$ (number of ordinal outcomes)
2. Define the regression coefficients $\beta$s
3. Define the baseline probabilities when $x = 0$. These probabilities can be converted to the corresponding thresholds $\alpha$ choosing the appropriate link function (*logit* or *probit*).
4. Calculate the linear predictors $\eta$ using $k - 1$ equations combining the predictors $\mathbf{X}$ and regression coefficients $\mathbf{\beta}$.
5. Apply the inverse of the link function $g^{-1}(\eta)$ on the linear predictor and calculate the cumulative probabilities $p(y \leq 1|x), p(y \leq 2|x), ... p(y \leq k - 1|x)$.
6. Calculate for each observation the probability of the $k$ outcome as the difference between cumulative probabilities. $p(y = 1) = 0 - p(y <= 1), p(y = 2) = p(y <= 2) - p(y <= 1)$ (sistema qua). This is implemented in R by adding a columns of zeros and ones respectively at the beginning and end of the matrix of cumulative probabilities.
7. Sample $n$ outcomes from a multinomial distribution, choosing between $k$ alternatives with the calculated probabilities.

(rivedi se fare con for)
This can be easily implemented in R. We are using a `for` loop to improve the clarity of the workflow but using the `*apply` family of functions can improve the efficiency of the computations.

```{r}
k <- 4  # number of options (1)
n <- 1e5 # number of observations (1)
b1 <- 0.5 # beta1, the shift in the latent distribution (2)
probs0 <- rep(1/k, k) # probabilities when x = 0 (3)
alphas <- prob_to_alpha(probs0, link = "probit") # get true thresholds from probabilities (3)
dat <- data.frame(x = rnorm(n)) # create dataframe
lp <- lapply(alphas, function(a) a - b1 * dat$x) # k - 1 linear predictors (4)
names(lp) <- sprintf("g[p(y <= %s)]", 1:(k - 1)) # giving appropriate names
```

The linear predictors can be seen in the Figure ref. The $g$ in this case is the cumulative *probit* function $\Phi$.

```{r}
#| echo: false
lpt <- lp
names(lpt) <- latex2exp::TeX(sprintf("$g[p(y \\leq %s)]$", 1:(k - 1)))
datl <- cbind(dat, lpt)
datl <- tidyr::pivot_longer(datl, 2:ncol(datl), names_to = "cumy", values_to = "lp")

ggplot(datl, aes(x = x, y = lp, color = cumy)) +
  facet_wrap(~cumy, labeller = label_parsed) +
  geom_hline(yintercept = alphas, lty = "dashed") +
  geom_line(show.legend = FALSE) +
  theme_minimal(20) +
  ylab("Linear Predictor")
```

Now we can apply the inverse of the link function $g^{-1} = \Phi^{-1}$ to calculate the corresponding cumulative probabilities.

```{r}
cump <- lapply(lp, pnorm) # inverse of the link function (5)
cump <- data.frame(cump)
p <- apply(cbind(0, cump, 1), 1, diff, simplify = FALSE) # probability of each k outcome (6)
p <- do.call(rbind, p)
```

```{r}
p <- data.frame(p)
names(p) <- sprintf("p_y%s", 1:k)
filor::trim_df(data.frame(p))
```

We can plot the expected effect of $\beta_1$ on the $k$ probabilities using the `num_latent_plot()` function (see Figure \@ref(fig:sim-with-probs-lat-plot)). Finally we sample using the `sample()` function using the calculated probabilities.

```{r sim-with-probs-lat-plot, fig.cap="sim-with-probs-lat-plot"}
num_latent_plot(x = dat$x, b1 = b1, probs = probs0, link = "probit")
```

```{r}
dat$y <- apply(p, 1, function(ps) sample(1:k, 1, prob = ps))
```

To check the simulation result we can increase the number of observations, fit the model using the `ordinal::clm()` function and assess the recovery of simulated parameters.

```{r}
dat$y <- ordered(dat$y) # make an ordered factor in R where 1 < 2 < 3 < 4
fit <- clm(y ~ x, data = dat, link = "probit")
```

```{r tab-sim-probs}
truth <- c(alphas, b1)
names(truth) <- c(names(fit$alpha), names(fit$beta))
clm_table(fit, truth = truth, caption = "caption") |> 
  flextable_to_rmd()
```

### simulating using latent distribution

An equivalent but more efficient way to simulate an ordinal outcome is using the latent formulation of the model. This require simulating a standard linear regression using the appropriate data generation function (*logistic* or *normal*) and the cutting the latent values according to the thresholds $\alpha$. The workflow is slighlty different compared to the previous paragraph:

1. Define $n$ (number of observations) and $k$ (number of ordinal outcomes)
2. Define the regression coefficients $\beta$
<!-- controlla che le soglie non fanno riferimento a quando è 0 ma concettualmente si. essendo che non cambiano non è rilevante a quale x si faccia riferimento -->
3. Define the baseline probabilities when $x = 0$. These probabilities can be converted to the corresponding thresholds $\alpha$ choosing the appropriate link function (*logit* or *probit*).
4. Calculate the linear predictors $\eta$ using $k - 1$ equations combining the predictors $\mathbf{X}$, the regression coefficients $\mathbf{\beta}$ and the error $\mathbf{\epsilon}$ sampled from the appropriate probability distribution.
5. Cut the latent variable into $k$ areas using the thresholds $\alpha$ and assign the corresponding ordinal value. This step simply checks if the latent value $Y^{\star}_i$ is between two threshold values and assign the corresponding value. This can be done using the `cut()` or the `findInterval()` functions.

```{r}
k <- 4  # number of options (1)
n <- 1e5 # number of observations (1)
b1 <- 0.5 # beta1, the shift in the latent distribution (2)
probs0 <- rep(1/k, k) # probabilities when x = 0 (3)
alphas <- prob_to_alpha(probs0, link = "probit") # get true thresholds from probabilities (3)
dat <- data.frame(x = rnorm(n)) # create dataframe
dat$ystar <- b1 * dat$x + rnorm(n, 0, 1) # linear predictor (4)
dat$y <- findInterval(dat$ystar, alphas) + 1 # cut the latent distribution (5). The + 1 because the first category is 0 by default.
```

The Figure (put crossref) depict the simulated $Y^{*}$ and the corresponding ordinal value. As for the previous simulation we can fit the model using `clm()` and check the estimated parameters.

```{r}
alpha_n <- latex2exp::TeX(sprintf("$\\alpha_{%s}$", th_names(k)))

dat |> 
  slice_sample(n = 1000) |> 
  mutate(y = findInterval(ystar, alphas) + 1) |> 
  ggplot(aes(x = x, y = ystar)) +
  geom_point(alpha = 0.6, aes(color = factor(y)), size = 3) +
  theme_minimal(15) +
  theme(legend.position = "bottom",
        legend.title = element_blank()) +
  ylab(latex2exp::TeX("$Y^{*}$")) +
  geom_hline(yintercept = alphas) +
  annotate("label", x = min(dat$x), y = alphas, label = alpha_n)
```

```{r}
dat$y <- ordered(dat$y) # make an ordered factor in R where 1 < 2 < 3 < 4
fit <- clm(y ~ x, data = dat, link = "probit")
```

```{r tab-sim-latent}
truth <- c(alphas, b1)
names(truth) <- c(names(fit$alpha), names(fit$beta))
clm_table(fit, truth)
```

The simulation using the latent formulation of the model is implemented in the `sim_ord_latent()` function.

```{r}
#| results: asis

filor::print_fun(funs$sim_ord_latent)
```


## Scale Effects

The default ordinal regression model assume that the variance of th underlying latent distribution is the same across condition. This is similar to a standard linear regression assuming homogeneity of variance. For example, when comparing two groups or conditions we can run a standard linear model (i.e., a t-test) assuming homogeneity of variances or using the Welch t-test [see @Delacre2017-qy]. In addition, there are the so-called location-scale models that allows to include predictors also for the scale (e.g., the variance) of the distribution. This can be done also in ordinal regression where instead of assuming the same variance between conditions, the linear predictors can be included. Figure \@ref(fig:example-unequal-variance) depict an example of a comparison between two groups where the two underlyning latent distributions have unequal variance.

spiegare qui che i location-scale models sono sostanzialmente un cambiamento delle soglie (e non dei beta) in funzione di predittori @Tutz2020-xq

dire che gli scale effects possono essere visti come un'alterativa più parsimoniosa a modelli non-proportional odds @Christensen2019-cz

se citi brms per modelli più complicati, ricordati di specificare che:

> That is, s = 1/disc. Further, because disc must be strictly positive, it is by default modeled on the log scale.

un modo intuitivo per spiegare gli scale effects è di far capire la relazione tra slope e scale. La slope della funzione cumulative corrisponde alla varianza. Se la slope è piuù steep, la varianza sarà minore e la discrimizione maggiore (in `brms` si chiama desc)

```{r}
#| eval: false
library(distributional)
library(ggdist)
library(tidyverse)

s <- c(0.5, 1, 2, 3)

dat <- data.frame(
  m = 0,
  s = s
)

dat$dist <- dist_normal(dat$m, dat$s)

par(mfrow = c(1, 2))
curve(dnorm(x, m = dat$m[1], sd = dat$s[1]), -5, 5, lwd = 2)
lapply(2:nrow(dat), function(i) curve(dnorm(x, dat$m[i], dat$s[i]), add = TRUE, col = i, lwd = 2))

curve(pnorm(x, m = dat$m[1], sd = dat$s[1]), -5, 5, lwd = 2)
lapply(2:nrow(dat), function(i) curve(pnorm(x, dat$m[i], dat$s[i]), add = TRUE, col = i, lwd = 2))

# x <- seq(0, 1, 0.001)
# p <- pnorm(qnorm(0.01) + 5 * x)
# y <- rbinom(length(x), 1, p)
# 
# plot(x, p)
# curve(pnorm(x, -(qnorm(0.01)/5), 1/5))
```

si può citare anche questo @Cox1995-ur come riferimento per i modelli

una distinzione tra location-shift and location-scale models @Tutz2017-de dove i primi oltre ad un cambiamento di location prevedono un cambiamento di soglie mentre i secondi modellano la differenza di soglie come un cambiamento della scala (varianza) che quindi permette di aumetare o diminuire l'area associata ad un determinato intervallo.

sempre in @Tutz2017-de propongono il location-scale/shift come una via di mezzo tra il proportional odds ed il non proportional odds

### Simulating scale effects

The location-scale model can be simulated using the `sim_ord_latent()` function and providing the predictors for the scale parameters. Given the `log` link function, predictors are provided on the `log` scale. For example, we simulate the effect of a binary variable $x$ representing two independent groups predicting the $k = 5$ response. We simulate a *location* effect of $\beta_1 = 0.5$ (in *probit* scale) and $\zeta_1 = \text{log}(2) = 0.70$. The first group has a $\sigma = 1$ and the second group has $\sigma = 2$. Again we simulate that the baseline probabilities are uniform for the first group.

```{r}
k <- 5  # number of options (1)
n <- 1e5 # number of observations (1)
b1 <- 0.5 # beta1, the shift in the latent distribution (2)
z1 <- log(2) # zeta1, the change in the scale
probs0 <- rep(1/k, k) # probabilities when x = 0 (3)
alphas <- prob_to_alpha(probs0, link = "probit") # get true thresholds from probabilities (3)
dat <- data.frame(x = rep(c(0, 1), each = n/2))
dat <- sim_ord_latent(~x, scale = ~x, By = b1, Bscale = z1, probs = probs0, data = dat, link = "probit")
fit <- clm(y ~ x, scale = ~ x, data = dat, link = "probit")
```

The table (put ref) reports the simulation results.

```{r}
#| eval: false
truth <- c(alphas, b1, z1)
names(truth) <- c(names(fit$alpha), names(fit$beta), names(fit$zeta))
clm_table(fit, truth)
```

## Visualizing effects

Before starting with the actual simulation it is useful to plot the predicted probabilities of the response variable $y$ as a function of the predictors. The `cat_latent_plot()` and the `num_latent_plot()` functions are able to visualize the effect of a single categorical or numerical predictor. Specifying the mean and standard deviations of the latent variable in each condition (for the categorical version) or the $\beta$ (for the numerical version) the function compute the expected probability for each level of the ordinal variable $y$ and visualize the result. In this way the $\beta$ can be choose in a meaningful way.

```{r, echo=FALSE, results='asis'}
filor::print_fun(c(funs$cat_latent_plot, funs$num_latent_plot))
```

For example, the Figure \@ref(fig:ex-categorical) depicts the effect of a categorical predictor on a 5-level $y$ assuming uniform probabilities for the reference level and a mean difference on a standardized Normal distribution of 1. The Figure \@ref(fig:ex-numerical) depicts the effect of a continuous predictor $x$ sampled from a standard Normal distribution assuming a $\beta = 1$.

```{r ex-categorical, fig.cap="Example categorical", fig.width=8, fig.height=8}
cat_latent_plot(m = c(0, 1), s = 1, probs = rep(1/5, 5), link = "probit")
```

```{r ex-numerical, fig.cap="Example numerical", fig.width=8, fig.height=8}
num_latent_plot(x = rnorm(100), b1 = 1, probs = c(0.4, 0.2, 0.2, 0.05, 0.05), link = "probit")
```

## Disclaimer about the functions

The current paper proposed a simplified way with some functions to generate ordinal data. For more complex simulations such as simulating correlated ordinal data the `simstudy` package [https://kgoldfeld.github.io/simstudy/articles/ordinal.html](https://kgoldfeld.github.io/simstudy/articles/ordinal.html) proposed a very comprehensive set of data simulation function also for ordinal data.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
