---
title             : "Ordinal regression models made easy. A tutorial on parameter interpretation, data simulation, and power analysis."
shorttitle        : "Ordinal regression models made easy"

author: 
  - name          : "Filippo Gambarota"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "filippo.gambarota@unipd.it"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Gianmarco Altoè"
    affiliation   : "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Department of Developmental Psychology and Socialization, University of Padova, Italy"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  Abstract here
  
keywords          : "ordinal, likert, simulations, power"
wordcount         : "X"

bibliography      : "`r filor::fil()$bib`"

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output: 
  papaja::apa6_pdf:
    latex_engine: xelatex
    header_includes:
      - \usepackage{booktabs, caption, longtable, colortbl, array, hhline, bm}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      dev = "pdf",
                      fig.align = "center",
                      size = "scriptsize",
                      message = FALSE,
                      warning = FALSE)
```

```{r hooks, include = FALSE}
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r packages, include = FALSE}
library(tidyverse)
library(ordinal)
library(here)
library(flextable)
library(cowplot)
library(viridis)
library(ggeffects)
devtools::load_all()
```

```{r funs, include = FALSE}
funs <- filor::get_funs(here("R", "utils.R"))
```

```{r ggplot, include = FALSE}
mtheme <- function(){
  ggthemes::theme_pander(15) +
    theme(legend.position = "bottom",
        plot.title = element_text(hjust = 0.5),
        strip.text = element_text(face = "bold"),
        panel.grid.minor = element_blank())
}

theme_set(mtheme())

# palettes
options(ggplot2.continuous.colour="viridis")
options(ggplot2.continuous.fill = "viridis")
scale_color_discrete <- scale_colour_viridis_d
scale_fill_discrete <- scale_fill_viridis_d
```

```{r include = FALSE}
# figures
source(here("scripts", "figures.R"))
rfigs <- readRDS(here("objects", "r-figs.rds"))
```

# Introduction

Psychological research make an extensive use of ordinal data. One of the main reason is probably the usage of Likert scales [@Likert1932-xw]. Ordinal data refers to a specific type of measurement scale [@Stevens1946-te] where ordered numbers are assigned to a variable. Compared to nominal scale and as the name suggest the labels are ordered. Compared to interval or ratio scales there is no explicit assumption about the distance between labels. (rivedi questo). An example is asking people the degree of agreement about a certain statement using a scale from 1 (no agreement) to 7 (total agreement). Answering 4 compared to 2 suggest an higher agreement but we cannot affirm that there is two times the agreement compared to the second answer. @Stevens1946-te and @Kemp2021-dj suggested that for ordinal variables is appropriate to calculate ranks-based statistics (e.g., median or percentiles) instead of metric statistics (e.g., mean or standard deviation). This distinction in terms of the appropriateness of certain descriptive statistics is also relevant when modeling data. Treating ordinal data as metric refers to assuming the labels as actual integer numbers thus assuming a fixed and know distance between levels [@Liddell2018-wu].

In Psychology especially when using item-based measures (questionnaires, surveys, etc.) the common practice is using a normal linear regression that makes an explicit assumption about metric features of the response variable. @Liddell2018-wu reviewed the psychological literature using likert-based measures and reported how the majority of papers used metric-based statistical models. In the same work, @Liddell2018-wu showed extensive examples and simulations about the potential pitfalls of treating an ordinal variable as metric [but see @Robitzsch2020-la for an alternative perspective]. They reported problems in terms of lack of power, inversion of the effects and distortion in estimating the effect size (rivedi meglio qui). (vedi se c'è qualche altro lavoro che fa vedere questa cosa). Some authors suggested that despite individual items being ordinal, averaging multiple items could solve the problem of applying metric models (e.g., Carifio & Perla, 2007, 2008 metti queste citazioni). @Liddell2018-wu suggested that in some conditions even averaging multiple items and applying metric models could be problematic. @Cliff2016-ck suggest that most of the research questions in behavioral sciences can be considered as ordinal (*is the score $x$ higher than the score $y$*?) concerning variables where the most appropriate measurement scale is probably ordinal.

qualcosa ancora qui

## Ordinal regression models

Despite the actual modeling proposal by @Liddell2018-wu, there is a class of regression models taking into account the ordinal nature of the response variable without making metric assumptions. We can class this general class of models as *ordinal regression*. The nomenclature of these models can be confusing mainly because there are several types of models with different assumptions and structures [@Tutz2022-dg]. @Tutz2022-dg and @Burkner2019-aw provide a clear and updated taxonomy of ordinal regression models. We can identify three main classes: *cumulative models* [@McCullagh1980-cw], *sequential models* [@Tutz1990-fe] and *adjacent category models*. The cumulative is the mostly used model assuming the existence of a latent variable that when segmented using thresholds produces the observed ordinal variable. The psychological process underlying the response is clearly formalized in the signal detection theory framework where the respondent (vedi come sistemare). The sequential model as suggested by the name is appropriate when modelling sequential processes. Assuming to have 5 response options, the sequential model assume that responding 3 means already reaching the states 1 and 2. A clear example is proposed by @Burkner2019-aw where the marriage duration in years is predicted as a function of some explanatory variables. For each level of the response variable there is a latent distribution where the step between a marriage year $k = 1$ and the next years $k > 1$ is modeled by the sequential regression. When comparing $k$ with $k > 1$, everything lower than $k$ is assumed to be already reached [@Tutz2020-xq]. The adjacent category model compare the category $k$ with $k + 1$ still assuming a latent distribution for each $k$. As suggested by @Tutz2022-dg the adjacent-category model can be seen as a series of binary binomial regressions taking into account the order of the categories. @Burkner2019-aw suggested that adjacent-category model can be chosen for its mathematical convenience and there is no a clear empirical distinction as for the cumulative vs sequential model. 

In the current paper we put the focus on the cumulative model for several reasons. The first reason is that the latent formulation of the model is particularly convenient both for parameter interpretation and data simulation. The second reason is that several psychological variables can be formalized as a latent continuous construct that when measured is collected using ordinal items. Furthermore, also the signal detection theory framework models the decision process assuming a latent distribution for the signal and noise where thresholds are the response criteria [e.g., @DeCarlo2010-lj].

```{r fig-ordinal-models, fig.cap=filor::get_caption(), out.width="100%"}
knitr::include_graphics("img/fig-ordinal-models.png")
```

## Model notation

metti qualcosa sulla nomenclatura

> The name, cumulative link models is adopted from Agresti (2002), but the model class has
been referred to by several other names in the literature, such as ordered logit models and
ordered probit models (Greene and Hensher 2010) for the logit and probit link functions. The
cumulative link model with a logit link is widely known as the proportional odds model due
to McCullagh (1980) and with a complementary log-log link, the model is sometimes referred
to as the proportional hazards model for grouped survival times.

In this section we introduce some notations for the cumulative model that is used through the paper and in the R code. We proposed a notation as consistent as possible with the main textbooks and seminal papers and with the `ordinal` R package used in the tutorial. We define $Y_k$ as the observed ordinal response with $1, \dots,k$ levels and $Y^\star$ is the underlying latent variable. The latent variable is segmented using $k - 1$ thresholds $\alpha_1, \dots, \alpha_{k - 1}$. Similarly to the generalized linear models framework, we define $g(x) = \eta$ as the link function that maps that transforms probabilities into the linear predictor $\eta$. To transform $\eta$ into probabilities we use the inverse of the link function $x = g^{-1}(\eta)$. The specific link function define the type of model and require a different R function. For example, when assuming a Gaussian distribution we are fitting a model with a `probit` link function is the cumulative distribution function $g(x) = \Phi^{-1}(x) = \eta$ and the inverse of the link function is the inverse cumulative distribution function (or quantile function) defined $x = g^{-1}(\eta) = \Phi(\eta)$. When modelling an ordinal variable in a cumulative link model we actually modelling the cumulative probability $P(Y < k), k = 1, \dots, k - 1$. Equation \@ref(eq:prob-cum-model1) depict the general cumulative model including predictors $\mathbf{X}$ and regression coefficients $\boldsymbol{\beta}$. The minus sign in $\mathbf{X} \boldsymbol{\beta}$ is used to give the usual interpretation of standard regression models where an increase in $\beta$ corresponds to an increase in the probability of higher categories of $Y$ (metti il libro di agresti sui dati ordinali).

\begin{equation} 
P(Y \leq k) = g^{-1}(\alpha_k - \mathbf{X} \boldsymbol{\beta}), \;\;k = 1, \dots, k - 1
(\#eq:prob-cum-model1)
\end{equation}

The $\mathbf{X} \boldsymbol{\beta}$ is the linear predictor $\eta$ that is the cumulative probability $P(Y \leq k)$ transformed using the link function $g()$. To obtain the probability of a single outcome $P(Y = k)$ we can compute the difference between cumulative probabilities as shown in Equation \@ref(eq:eq:prob-cum-model2).

\begin{equation} 
P(Y = k) = g^{-1}(\alpha_k - \eta) -  g^{-1}(\alpha_{k - 1} - \eta), \;\;k = 1, \dots, k - 1
(\#eq:prob-cum-model2)
\end{equation}

There are always two special cases when computing the probability of a single outcome $Y$ that is when $Y = 1$ and $Y = k$. In the first case the cumulative probability is calculated from $-\infty$ that is the same as temporary assuming an $\alpha_0 = -\infty$. In the second case ($Y = 1$) the probability is calculated as $P(Y = k) = 1 - g^{-1}(\alpha_{k - 1} - \eta)$ that is the same as assuming a temporary threshold $\alpha_k = +\infty$. The Figure \@ref(fig:fig-explain-cumulative) shows how the single probabilities of the ordinal outcome are calculated from cumulative probabilities.

```{r fig-explain-cumulative, fig.cap=filor::get_caption()}
probs <- c(0.4, 0.2, 0.3, 0.1)
th <- prob_to_alpha(probs, link = "probit")
lat <- data.frame(m = 0, s = 1)
lat$norm <- distributional::dist_normal(lat$m, lat$s)

thl <- latex2exp::TeX(sprintf("$\\alpha_{%s}$", 0:length(probs)))

ggplot(lat, aes(dist = norm)) +
  ggdist::stat_slab(orientation = "horizontal",
                    aes(fill = after_stat(sprintf("P(Y = %s)", findInterval(x, th) + 1)))) +
  geom_vline(xintercept = c(-4, th, 4)) +
  theme_minimal(15) +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        axis.text.x = element_text(size = 17),
        axis.title.x = element_blank(),
        axis.title.y = element_blank()) +
  annotate("label", x = th, y = 0, label = thl[2:(length(thl) - 1)], size = 6) +
  annotate("label", x = c(-4, 4), y = 0, label = thl[c(1, length(thl))], size = 6, fill = "grey") +
  scale_x_continuous(breaks = seq(-4, 4, 2), 
                     labels = c(latex2exp::TeX("$-\\infty$"), "-2", "0", "2", latex2exp::TeX("$\\infty$")))
```

<!-- notazione https://people.vcu.edu/~dbandyop/BIOS625/CLM_R.pdf e @Burkner2019-aw per la parte di notazione e quella più formale anche @DeCarlo2010-lj -->

The model presented in Equation \@ref(eq:prob-cum-model1) can be considered similar to a standard binomial generalized linear model. In this case we have $k - 1$ equations, one for each threshold $\alpha$ and we model the cumulative probability. The same model can be written as in Equation \@ref(eq:latent-model) assuming a continuous latent variable $Y^\star$ explained by the linear combination of predictors $\eta = \mathbf{X} \boldsymbol{\beta}$ as in a standard linear regression model.

\begin{equation} 
Y^\star_i = \eta + \epsilon_i
(\#eq:latent-model)
\end{equation}

The crucial part is $\epsilon_i$ that is the random part of the model coming from a certain probability distribution. For a *probit* model, errors are sampled from a standard Gaussian distribution while for a *logit* model from a standard logistic distribution. Following the notation by @Tutz2022-dg, the observed ordinal value $Y_i = k$ comes from $Y^\star_i$ belonging to the interval defined by the thresholds $Y_i = k  \iff \alpha_{k - 1} < Y^\star_i < \alpha_{k}$ where $- \infty = \alpha_0 < \alpha_1 < \dots< \alpha_{k - 1} < \alpha_k = \infty$.

In the basic version of the model, the thresholds $\alpha_k$ are considered as fixed and being part of the measurement procedure [@Liddell2018-wu] and do not vary as a function of the predictors. In a more sophisticated version of the model called location-shift [@Tutz2022-dg], both the location $\mu$ and the thresholds $\alpha_k$ can vary as a function of the predictors. @Tutz2022-dg described also another version of the model called location-scale [@Tutz2022-dg; @Rigby2005-ko; @Cox1995-ur] where the location $\mu$ and the scale $\sigma^2$ of the distribution can vary as a function of the predictors.

## Alternative parametrizations

Several parametrizations of the ordinal models have been proposed by different authors. @Liddell2018-wu and @Kruschke2015-re proposed a Bayesian *probit* model where the latent mean and standard deviations are estimated along with the threshold. The interesting part is that the thresholds are transformed to be on the scale of the ordinal variable (e.g, from 1 to 4 when $k = 4$) and not in terms of $z$ scores. @Kurz2023-zv explain how to convert a model fitted with the `brms` package (citation) (an R package for regression modeling using *stan*) into the Kruschke [-@Kruschke2015-re] parametrization. @Gelman2020-tg proposed three alternative parametrizations focusing on different definition of the thresholds. The current tutorial is focused on the standard parametrization of the cumulative link model as described in the equations above.

## Link function

The cumulative link model implemented in Equations \@ref(eq:prob-cum-model1) and \@ref(eq:latent-model) can be considered the general formulation that requires specifying the link function $g(\cdot)$ or the latent distribution $\epsilon_i \sim D(\mu, \sigma^2)$. Among several available functions the *logit* and *probit* models are the most common. The *logit* model assume a *logit* link function and a logistic distribution as latent variable. On the other side, the *probit* model assume a Gaussian distribution. The two models provide similar results but the parameters intepretation and the way data can be simulated is different. In the next sections we will illustrate the differences and simulation strategies. Figure \@ref(fig:fig-logit-vs-probit) depicts the two distributions while Table \@ref(tab:tab-model-summary) summarise the presented cumulative models with the proposed link function and the corresponding R code. In terms of parameters, both distributions can be defined with a location $\mu$ and a scale $s$ parameter. The standard normal distribution has $\mu = 0$ and $s = 1$. Furthermore the variance corresponds to the scale $s^2 = \sigma^2 = 1$. The variance of the logistic distribution is $\sigma^2 = \frac{s^2\pi^2}{3}$. The standard logistic distribution has $\mu = 0$ and $s^2 = 1$ thus the standard deviation simplified to  $\frac{\pi}{\sqrt{3}} \approx 1.81$. In practical terms, the standard version of the two distributions are very similar with the logistic having more variance. The R implementation of these functions consist in the `q` and ``

```{r fig-logit-vs-probit, fig.cap=filor::get_caption()}
dplot <- ggplot(data = NULL) +
  stat_function(fun = dnorm, aes(color = "Normal"), lwd = 1) +
  stat_function(fun = dlogis, aes(color = "Logistic"), lwd = 1) +
  xlim(c(-7, 7)) +
  theme(legend.position = "bottom")

pplot <- ggplot(data = NULL) +
  stat_function(fun = pnorm, aes(color = "Normal"), lwd = 1) +
  stat_function(fun = plogis, aes(color = "Logistic"), lwd = 1) +
  xlim(c(-7, 7))


plot_grid_common_legend(
  dplot, 
  pplot + theme(axis.title.y = element_blank()), nrow = 2
)
```

```{r tab-model-summary-old}
#| eval: false
models_table <- data.frame(
       stringsAsFactors = FALSE,
                  Model = c("Model", "Cumulative Logit", "Cumulative Probit"),
          Link.Function = c("Equation",
                            "$\\text{logit}(p) = \\text{log}(\\frac{p}{1-p})$",
                            "$z = \\Phi(p)$"),
          Link.Function = c("R Code", "`qlogis()`", "`qnorm()`"),
  Inverse.Link.Function = c("Equation",
                            "$\\frac{e^{\\text{logit}(p)}}{1 + e^{\\text{logit}(p)}}$","$\\Phi^{-1}(z)$"),
  Inverse.Link.Function = c("R Code", "`plogis()`", "`pnorm()`")
)

models_table[-1, ] |> 
  flextable() |> 
  set_header_labels(values = list(Model = "Model", 
                                  Link.Function = "Equation",
                                  Link.Function.1 = "R Code",
                                  Inverse.Link.Function = "Equation",
                                  Inverse.Link.Function.1 = "R Code")) |> 
  add_header_row(values = c("", "Link Function", "Link Function", "Inverse Link Function", "Inverse Link Function")) |> 
  merge_h(1, part = "header") |> 
  theme_booktabs(bold_header = TRUE) |> 
  align(part = "all", align = "center") |> 
  set_caption(caption = filor::get_caption())
```

```{r tab-model-summary}
library(kableExtra)
models_table <- data.frame(
       stringsAsFactors = FALSE,
                  Model = c("Model", "Cumulative Logit", "Cumulative Probit"),
          Link.Function = c("Equation",
                            "$\\text{logit}(p) = \\text{log}(\\frac{p}{1-p})$",
                            "$z = \\Phi(p)$"),
          Link.Function = c("R Code", "`qlogis()`", "`qnorm()`"),
  Inverse.Link.Function = c("Equation",
                            "$\\frac{e^{\\text{logit}(p)}}{1 + e^{\\text{logit}(p)}}$","$\\Phi^{-1}(z)$"),
  Inverse.Link.Function = c("R Code", "`plogis()`", "`pnorm()`")
)

models_table[-1, ] |> 
  kable(col.names = c("Model", "Equation", "R Code", "Equation", "R Code"), row.names = FALSE,
        booktabs = TRUE, format = "latex", caption = filor::get_caption(), escape = FALSE) |> 
  kable_styling(full_width = FALSE) |> 
  add_header_above(header = c("", "Link Function" = 2, "Inverse Link Function" = 2))
```

# Model fitting

For fitting the cumulative models we used the `ordinal` package [@Christensen2019-cz]. Despite the presence of other possibilities the `ordinal` package provide the most complete and intuitive way to implement the ordinal models. The syntax is very similar to standard linear models in R and default functions to calculate predictions, perform model comparison, extract relevant model information are implemented similarly to standard regression modelling.^[For a very complete overview of the ordinal package see @Christensen2019-cz and the Package documentation https://cran.r-project.org/web/packages/ordinal/ordinal.pdf].

The function to fit the model is `clm()` and the model equation is specified using the R formula syntax `y ~ x` where `y` is the ordinal dependent variable and `x` is one or more predictors eventually including also the interactions. The package also implements mixed-effects models (see the `clmm()` function) including random intercepts and slopes but the topic is beyond the scope of the current tutorial.

When fitting the model the crucial arguments are the `formula`, the `link` function and the `data`. More advanced arguments are the `nominal`, `scale` and `threshold`.

- `formula`: the formula `y ~ x` with the dependent variable and predictors.
- `link`: is the link function. In this tutorial we consider only the *logit* and *probit* link but other link functions are available.
- `data`: is the dataset where with the variables included in the `formula`
- `nominal`: formula with predictors where the proportional odds assumption (See Section ) is relaxed (i.e., partial or non proportional odds)
- `scale`: formula with predictors for the scale (standard deviation) parameter. This argument allow to fit a scale-location model (see Section ). The main `formula` argument refers to predictors on the location parameter (i.e., the mean $\mu$).
- `threshold`: different structures for estimating the thresholds. The default is `threshold = "flexible"` where $k - 1$ threshold (where $k$ is the number of ordinal levels for $Y$) are estimated.

We can start by fitting a simple model, highlighting the crucial parameters where the detailed explanation will be expanded in the next sections. Table contains simulated data from $n = 100$ participants rating the agreement about a certain item with $k = 4$ ordered options. The participants are divided into two groups (predictor $x$). We can fit a cumulative link model with `clm()` function and check the model summary.

```{r tab-dataset-example1}
b1 <- log(3) # log odds ratio
k <- 4
n <- 100
x <- rep(c("a", "b"), each = n/2)
dat <- data.frame(x = x)
probs <- rep(1/k, k) # for the group "a", uniform probabilities
dat <- sim_ord_latent(~x, By = b1, prob = probs, data = dat, link = "logit")
dat |> 
  mutate(id = 1:n(), .before = x) |> 
  select(-ys) |> 
  filor::trim_df() |> 
  mtab(type = "kable", caption = filor::get_caption())
```

```{r eda-example-1}
funs <- list(mean = mean, median = median, sd = sd)
lapply(funs, function(f) tapply(as.numeric(dat$y), dat$x, f))
prop.table(table(dat$y, dat$x), margin = 2)
```


```{r model-example-1}
fit <- clm(y ~ x, data = dat, link = "logit")
summary(fit)
```

The two main sections of the model summary are the *Coefficients* section reporting the regression coefficients $\beta$ and the *Threshold* section reporting the $\alpha$ estimation. Given that $k = 4$ we have $k - 1 = 3$ thresholds and a single $\beta$ associated with the $x$ effect. As in standard regression models, when $x$ is a categorical predictor with $j$ level, we will estimate $j - 1$ regression coefficients where the interpretation depends on the contrast coding [for an overview about contrasts coding schemes see @Schad2020-ht]^[In R the default is the dummy coding where a factor of $j$ levels is converted into $j - 1$ dummy variables. By default, the first level of the factor is taken as the reference level and the $j - 1$ coefficients represent the comparison between the other levels and the reference.]

# Interpreting parameters

## *Logit Model*. Odds and odds ratio

aggiungere cosa significano i parametri nella regressione

The odds of a probability $p$ is defined as $\frac{p}{1 - p}$ thus the probability of success divided by the probability of failure. The odds takes value ranging from 0 to $\infty$. For example with a probability of $p = 0.8$ we have an odds of $4$, thus we have a 4 successes for each failure. The same as having $p = 0.2$ and an odds of $0.25$ means that for each $0.25$ successes we have a failure or that we have $4$ failures for each success. When comparing two groups or conditions we can compare the two odds calculating an odds ratio. The odds ratio is the mostly used statistics to compare groups or conditions on a binary outcome. An odds ratio of $4$ means that the odds of success at the numerator is 4 times higher than the odds of success at the denominator. The Figure \@ref(fig:fig-odds-example) shows the relationship between probabilities and odds. The logit transformation is about taking the logarithm of the odds creating a symmetric function ranging from $-\infty$ to $\infty$ with $p = 0.5$ as the midpoint because $\text{log}(\frac{0.5}{1 - 0.5}) = 0$. The standard logistic regression with two outcome model the logit transformed probability.

```{r fig-odds-example, fig.cap=filor::get_caption()}
p <- seq(0, 1, 0.001)

dd <- data.frame(
  p,
  odds = filor::odds(p)
)

dd$lodds <- log(dd$odds)

dd |> 
  tidyr::pivot_longer(c(odds, lodds)) |> 
  dplyr::mutate(name = ifelse(name == "odds", "Odds", "Log(Odds)")) |> 
  ggplot(aes(x = p, y = value)) +
  geom_line() +
  facet_wrap(~name, scales = "free") +
  theme(axis.title.y = element_blank()) +
  xlab("Probability")
```

Odds and odds ratios are clearly defined with $k = 2$ outcomes. With an ordinal variable and a cumulative model we can use the cumulative odds ratio. With e.g. $k = 4$ outcomes we have $k - 1$ models determined by the cumulative probability in terms of $P(Y \leq 1), \dots, P(Y \leq k - 1)$.

## Proportional odds assumption

Following again the taxonomy by @Tutz2022-dg, each of the presented ordinal regression model has a basic version making the  proportional odds assumption. There are more advanced versions of the model relaxing this assumption completely (*non proportional odds* models) and partially (*partial proportional odds* models) (rivedi se la nomenclatura va bene). The proportional odds assumption is a crucial point when interpreting model parameters. Can be formalized as in Equation (). Basically if we use the *logit* link function $g()$ the $\beta$s are interpreted as standard odds ratio in the logistic regression. Given that we have $k > 2$ alternatives there is the need of $k - 1$ equations (as reported in Equation ()). The porportional odds is assuming that the regression coefficients $\beta$ are independent from the thresholds $\alpha$ thus the effect is the same regardless of the $Y$ level. Thus $\beta_1 = \beta_2 \dots = \beta_{k - 1}$. The Figure () depicts the proportional odds assumption for the $k - 1$ logistic curves both for probabilities and linear predictors $\eta$.

```{r fig-prop-odds, fig.cap=filor::get_caption()}
rfigs$fig_prop_odds
```

The proportional odds assumption is convenient because regardless the number of $k$, the $\beta_j$ ($j$ being the number of regression coefficients) effect is assumed to be the same. The model is more parsimonious compared to estimating $k - 1$ coefficients for each $\beta_j$ as in the multinomial regression or the non-proportional version of the ordinal regression. Clearly this can be a strong assumption in some conditions. There are several methods to test the proportional odds assumption [see @Liu2023-bp for an overview]. @Tutz2020-xq suggested that using alternative as the location-shift or location-scale models, the assumption can be made (improving model parsimony) but still having a more flexible modeling framework. In the next section, we provide more details about the location-scale model in terms of parameters interpretation and data simulation.

To make a practical example, we can use again the the dataset presented in Table. We can check the proportional odds by extrating fitted probabilities and manually computing the odds ratios.

<!-- - https://hbiostat.org/ordinal/impactpo.pdf blog su proportional odds, mi pare di capire che non sia così problematica come assunzione -->

Equation \@ref(eq:prop-odds) formalize the proportional odds assumption. Basically the cumulative log odds ratio comparing $P(Y_k|x_0)$ with $P(Y_k|x_1)$ is the same regardless the specific $k$ or threshold $\alpha$.

\begin{equation}
\text{logit} (\frac{P(Y \leq 1 |x_1)}{P(Y \leq 1 |x_0)}) = \dots = \text{logit} (\frac{P(Y \leq k - 1 |x_1)}{P(Y \leq k -1 |x_0)})
(\#eq:prop-odds)
\end{equation}

This can be shown by fitting the model with `clm()` that by default assume the proportional odds and using the predicted probabilities to compute the odds ratios. In the following code we computed the odds ratio comparing $x_a$ and $x_b$ when $k \leq 1$ and $k \leq 2$. The log odds ratio is the same.

```{r}
#| echo: true

# fitting the model
fit <- clm(y ~ x, data = dat, link = "logit")

# extracting the predicted probabilities for the two groups
pr <- predict(fit, data.frame(x = unique(x)))$fit

odds <- function(p) p / (1 - p) # function to compute odds

# group a
a_or1vs2345 <- odds(pr[1, 1]) # 1 vs 2 3 4 5
a_or12vs345 <- odds(sum(pr[1, 1:2])) # 1 vs 2 3 4 5

# group b
b_or1vs2345 <- odds(pr[2, 1]) # 1 vs 2 3 4 5
b_or12vs345 <- odds(sum(pr[2, 1:2])) # 1 vs 2 3 4 5

c(xa = log(a_or1vs2345 / a_or12vs345), xb = log(b_or1vs2345 / b_or12vs345))
```

Clearly, the odds ratio here is exactly the same because the model assume the proportional odds assumption putting this constraint on the predicted probabilities. The estimated $\beta$ for each $k - 1$ equation could differ and the amount of heterogeneity is actually a way to test the proportional odds with real data. In the supplementary materials we implemented an intuitive plot proposed by @Harrell-2015-no to visually assess the proportional odds assumption.

### Proportional odds and *probit* model

The proportional odds assumption is relevant only for the *logit* model because of the link function that put $\beta$ as log odds. However the concept of the proportional odds that is more generally called parallel slopes assumption is assumed also for the probit model. Equation \@ref(eq:probit-prop-odds) depict the parallel slopes assumption for a probit model. Basically the difference in $z$ for a unit increase in $x$ (i.e., the slope) is the same regardless the $Y$ level. This can be directly demonstrated in the same way as for the logit model. We use the same dataset presented in Table \@ref(tab:tab-prop-ord-ex) but fitting a probit model. The difference between the $z$ scores comparing the two groups calculated on the cumulative probability is the same regardless the $k - 1$ level.

$$
z_{x_1 - x_0} = \Phi(P \leq k|x_0) - \Phi(P \leq k|x_1) = \dots = \Phi(P \leq k - 1|x_0) - \Phi(P \leq k - 1|x_1)
$$

```{r}
#| echo: true

# fitting the model
fit <- clm(y ~ x, data = dat, link = "probit")

# extracting the predicted probabilities for the two groups
pr <- predict(fit, data.frame(x = unique(x)))$fit

# group a
a_1vs2345 <- qnorm(pr[1, 1]) # 1 vs 2 3 4 5
a_12vs345 <- qnorm(sum(pr[1, 1:2])) # 1 2 vs 3 4 5

# group b
b_1vs2345 <- qnorm(pr[2, 1]) # 1 vs 2 3 4 5
b_12vs345 <- qnorm(sum(pr[2, 1:2])) # 1 2 vs 3 4 5

c(xa = a_1vs2345 - a_12vs345, xb = b_1vs2345 - b_12vs345)
```

### Beyond the proportional odds

qualcosa che spieghi come andare oltre e qualche proposta per testare l'assunzione.

## *Probit Model* $z$ scores

The alternative to fitting a cumulative logit model is using a standard normal distribution thus fitting a *probit* model. The main difference is that the latent distribution is now a Gaussian distribution with $\mu = 0$ and $\sigma^2 = 1$ and the link function is the cumulative gaussian distribution function $\Phi$ implemented in R with the `pnorm()` function. The inverse of the Gaussian cumulative distribution function is $\Phi^{-1}$ implemented in R with the `qnorm()` function. The logistic and normal distribution are very similar, with the logistic having more variance.

The main difference regards the parameters interpretation. In the logit model the $\beta$ are the log odds ratio. For categorical variables they represents the increase in the log odds of moving from one level to the other while for numerical variables is the increase in the log odds for a unit increase in $x$. For the probit model, the $\beta$ is the increase in terms of $z$ scores for a unit increase in $x$. This is very convenient especially for categorical variables because parameters can be intepreted as a Cohen's $d$ like measure. Thinking about the latent distributions, the $\beta$ is the shift in the latent mean comparing two or more groups or the slope of latent scores as a function of a numeric $x$. The intepretation in terms of shifting the latent mean holds also for the logistic model. However, the standard deviation of the standard logistic regression is $\frac{\pi}{\sqrt{3}} \approx 1.81$^[Actually the variance of the logistic distribution is $\frac{s^2\pi^2}{3}$ and the standard deviation $\frac{s\pi}{\sqrt{3}}$ where $s$ is the scale of the distribution. For the standard logistic distribution $s = 1$ (as for the standard normal distribution). For the standard normal distribution $s^2 = \sigma^2 = 1$]. The $\beta$ for the logistic distribution can be interpreted as the location shift of the latent logistic distribution by $\frac{\pi}{\sqrt{3}}$ standard deviations. 

Another advantage of the probit model is the possibility to directly map parameters from signal detection theory [@Stanislaw1999-jr; @Green1966-gy] into the model coefficients. In practical terms, the thresholds $\alpha$ are the criterion cut-offs and the $\beta_1$ is the d$'$ parameter [@DeCarlo1998-ay; @Knoblauch2012-to].

qualcosa in più qui

## Simulating data

There are mainly two ways to simulate data. The first method concerns simulating starting from the latent formulation of the ordinal model. Basically we can simulate the underlying latent distribution and then fixing the thresholds converting the latent continuous variable into the observed ordinal variable.

### Simulating from a multinomial distribution

The first method to simulate ordinal data as a function of predictors is by calculating the true probabilities $g^{-1}(\eta)$ as a function of predictors and then sample from a multinomial distribution^[In R we are using the `sample()` function that simulate values from a *categorical* distribution. The *categorical* distribution is considered a special case of the multinomial distribution]. This is similar to the general method to simulate data for a generalized linear model where the linear predictors $\eta$ are calculated and data are generated from the corresponding probability distribution of the random component. The following code box simulate a $n$ binary trials with a continuous predictor $x$.

```{r}
xl <- seq(0, 1, 0.1)
x <- rep(xl, 10)
n <- length(x)
b0 <- qlogis(0.01)
b1 <- 8
lp <- b0 + b1 * x # linear predictor
y <- rbinom(n, 1, plogis(lp)) # sampling from a binomial distribution using the vector of probabilities
p <- tapply(y, x, mean)
plot(unique(x), p, pch = 19)
curve(plogis(x, -(b0/b1), 1/b1), add = TRUE, lwd = 2, col = "firebrick")
```

Essentially the `lp` ($\eta_i$) vector contains true probability of success ($p_i|x_i = g^{-1}(\eta_i)$) for the $x_i$ level of the predictor. Then to introduce the random part of the model, we can use $p_i$ to generate binary values from a Binomial (in this specific case a Bernoulli) distribution. This create `y` that is the vector of 0-1 values generated using the true probabilities of success. The supplementary materials contains a more extensive example for simulating data for generalized linear models.

We can apply the same idea to an ordinal outcome but we need $k - 1$ equations (compared to the previous example) where $k$ is the number of ordinal levels. Let's simulate a similar design with a continuous $x$ predictor and $k = 4$ options. We fix the baseline probabilities where $x = 0$ as uniform thus $p(y_1) = p(y_2) = ... p(y_k) = 1/k$. The proposed workflow for the simulation is:

1. Define $n$ (number of observations) and $k$ (number of ordinal outcomes)
2. Define the regression coefficients $\beta$s.
3. Define the baseline probabilities when $x = 0$. These probabilities can be converted to the corresponding thresholds $\alpha$ choosing the appropriate link function (*logit* or *probit*).
4. Calculate the linear predictors $\eta$ using $k - 1$ equations combining the predictors $\mathbf{X}$ and regression coefficients $\boldsymbol{\beta}$.
5. Apply the inverse of the link function $g^{-1}(\eta)$ on the linear predictor and calculate the cumulative probabilities $p(y \leq 1|x), p(y \leq 2|x), ... p(y \leq k - 1|x)$.
6. Calculate for each observation the probability of the $k$ outcome as the difference between cumulative probabilities. For example, the probability of $P(y = 2) = P(y \leq 2) - P(y \leq 1)$. This is implemented in R by adding a columns of zeros and ones respectively at the beginning and end of the matrix of cumulative probabilities because the probability of the first $Y = 1$ level is the cumulative probability from $- \infty$ to the first threshold and the probability of $Y = k$ is obtained removing $P(Y \leq k - 1)$ from one.
7. Sample $n$ outcomes from a multinomial distribution, choosing between $k$ alternatives with the calculated probabilities.

```{r}
k <- 4  # number of options (1)
n <- 1e5 # number of observations (1)
b1 <- 0.5 # beta1, the shift in the latent distribution (2)
probs0 <- rep(1/k, k) # probabilities when x = 0 (3)
alphas <- prob_to_alpha(probs0, link = "probit") # get true thresholds from probabilities (3)
dat <- data.frame(x = rnorm(n)) # create dataframe
lp <- lapply(alphas, function(a) a - b1 * dat$x) # k - 1 linear predictors (4)
names(lp) <- sprintf("g[p(y <= %s)]", 1:(k - 1)) # giving appropriate names
```

The linear predictors ($\eta$) can be seen in the Figure ref. The $g$ in this case is the cumulative *probit* function $\Phi$.

```{r}
#| echo: false
lpt <- lp
names(lpt) <- latex2exp::TeX(sprintf("$g[p(y \\leq %s)]$", 1:(k - 1)))
datl <- cbind(dat, lpt)
datl <- tidyr::pivot_longer(datl, 2:ncol(datl), names_to = "cumy", values_to = "lp")

ggplot(datl, aes(x = x, y = lp, color = cumy)) +
  facet_wrap(~cumy, labeller = label_parsed) +
  geom_hline(yintercept = alphas, lty = "dashed") +
  geom_line(show.legend = FALSE) +
  theme_minimal(20) +
  ylab("Linear Predictor")
```

Now we can apply the inverse of the link function $g^{-1} = \Phi^{-1}$ to calculate the corresponding cumulative probabilities.

```{r}
cump <- lapply(lp, pnorm) # inverse of the link function (5)
cump <- data.frame(cump)
p <- apply(cbind(0, cump, 1), 1, diff, simplify = FALSE) # probability of each k outcome (6)
p <- do.call(rbind, p)
```

```{r}
p <- data.frame(p)
names(p) <- sprintf("p_y%s", 1:k)
filor::trim_df(data.frame(p))
```

We can plot the expected effect of $\beta_1$ on the $k$ probabilities using the `num_latent_plot()` function (see Figure \@ref(fig:sim-with-probs-lat-plot)). Finally we sample using the `sample()` function using the calculated probabilities.

```{r sim-with-probs-lat-plot, fig.cap="sim-with-probs-lat-plot"}
num_latent_plot(x = dat$x, b1 = b1, probs = probs0, link = "probit")
```

```{r}
dat$y <- apply(p, 1, function(ps) sample(1:k, 1, prob = ps))
```

To check the simulation result we can increase the number of observations, fit the model using the `ordinal::clm()` function and assess the recovery of simulated parameters.

```{r}
dat$y <- ordered(dat$y) # make an ordered factor in R where 1 < 2 < 3 < 4
fit <- clm(y ~ x, data = dat, link = "probit")
```

```{r tab-sim-probs}
truth <- c(alphas, b1)
names(truth) <- c(names(fit$alpha), names(fit$beta))
#clm_table(fit, truth = truth)
```

### Simulating from the latent distribution

An equivalent but more efficient way to simulate an ordinal outcome is using the latent formulation of the model. This require simulating a standard linear regression using the appropriate data generation function (*logistic* or *normal*) and the cutting the latent values according to the thresholds $\alpha$. The workflow is slightly different compared to the previous approach.

1. Define $n$ (number of observations) and $k$ (number of ordinal outcomes)
2. Define the regression coefficients $\beta$
<!-- controlla che le soglie non fanno riferimento a quando è 0 ma concettualmente si. essendo che non cambiano non è rilevante a quale x si faccia riferimento -->
3. Define the baseline probabilities when $x = 0$. These probabilities can be converted to the corresponding thresholds $\alpha$ choosing the appropriate link function (*logit* or *probit*).
4. Calculate the linear predictors $\eta$ using $k - 1$ equations combining the predictors $\mathbf{X}$, the regression coefficients $\boldsymbol{\beta}$ and the error $\mathbf{\epsilon}$ sampled from the appropriate probability distribution.
5. Cut the latent variable into $k$ areas using the thresholds $\alpha$ and assign the corresponding ordinal value. This step simply checks if the latent value $Y^{\star}_i$ is between two threshold values and assign the corresponding value. This can be done using the `cut()` or the `findInterval()` functions.

```{r}
k <- 4  # number of options (1)
n <- 1e5 # number of observations (1)
b1 <- 0.5 # beta1, the shift in the latent distribution (2)
probs0 <- rep(1/k, k) # probabilities when x = 0 (3)
alphas <- prob_to_alpha(probs0, link = "probit") # get true thresholds from probabilities (3)
dat <- data.frame(x = rnorm(n)) # create dataframe
dat$ystar <- b1 * dat$x + rnorm(n, 0, 1) # linear predictor (4)
dat$y <- findInterval(dat$ystar, alphas) + 1 # cut the latent distribution (5). The + 1 because the first category is 0 by default.
```

The Figure (put crossref) depict the simulated $Y^{*}$ and the corresponding ordinal value. As for the previous simulation we can fit the model using `clm()` and check the estimated parameters.

```{r}
alpha_n <- latex2exp::TeX(sprintf("$\\alpha_{%s}$", th_names(k)))

dat |> 
  slice_sample(n = 1000) |> 
  mutate(y = findInterval(ystar, alphas) + 1) |> 
  ggplot(aes(x = x, y = ystar)) +
  geom_point(alpha = 0.6, aes(color = factor(y)), size = 3) +
  theme_minimal(15) +
  theme(legend.position = "bottom",
        legend.title = element_blank()) +
  ylab(latex2exp::TeX("$Y^{*}$")) +
  geom_hline(yintercept = alphas) +
  annotate("label", x = min(dat$x), y = alphas, label = alpha_n)
```

```{r}
dat$y <- ordered(dat$y) # make an ordered factor in R where 1 < 2 < 3 < 4
fit <- clm(y ~ x, data = dat, link = "probit")
```

```{r tab-sim-latent}
truth <- c(alphas, b1)
names(truth) <- c(names(fit$alpha), names(fit$beta))
#clm_table(fit, truth)
```

The simulation using the latent formulation of the model is implemented in the `sim_ord_latent()` function.

```{r}
#| results: asis
filor::print_fun(funs$sim_ord_latent)
```

### Choosing parameters values

#### Thresholds $\alpha$

The two proposed simulation workflows can be really flexible for simulating complex models with multiple predictors and interactions. The critical part of the simulation is setting appropriate and empirically meaningful parameters values. The thresholds are usually not the crucial part of the statistical inference and model interpretations. In fact, they can be considered as the intercept in standard linear models. In any case to set meaningful $\alpha$ values the best strategy is to convert them into the corresponding probabilities according to the link function. The thresholds are essentially quantiles of the latent distribution (logistic or normal) without a direct empirical interpretation. The distance between the thresholds determine the probabililty assigned to a specific $Y$ value. The function `show_alpha()` allow to produce a meaningful plot starting from the thresholds or the probabilities. For example, the Figure \@ref(fig:fig-show-th-example) depict the threshold/probabilities associated with a four-level ordinal variable.

```{r}
#| eval: false
show_alpha(prob = rep(1/4, 4), link = "probit", plot = c("latent", "probs"))
```

```{r fig-show-th-example}
p1 <- show_alpha(prob = rep(1/4, 4), link = "probit", plot = c("latent", "probs"))
p2 <- show_alpha(prob = c(0.5, 0.3, 0.1, 0.1), link = "probit", plot = c("latent", "probs"))
p3 <- show_alpha(prob = c(0.1, 0.2, 0.5, 0.2), link = "probit", plot = c("latent", "probs"))
plot_grid(p1 + rm_legend(), p2 + rm_legend(), p3, ncol = 1)
```

Thus the thresholds can be fixed according to the desired probabilities distribution. For example, when simulating the difference between two groups on an ordinal variable, the threshold can be set thinking about the response distribution in the reference group. The `alpha_to_prob()` and `prob_to_alpha()` functions can be used to convert between probabilities and thresholds.

```{r}
#| collapse: true
# k = 4, thus k probabilities and k - 1 thresholds
prob_to_alpha(prob = c(0.4, 0.1, 0.3, 0.2), link = "probit")
alpha_to_prob(alpha = c(-2, 0, 1), link = "probit")
```

#### Regression coefficients $\beta$

As reported in the sections above, the regression coefficients have a different interpretation according to the link function. Fortunately, both parameters have an intuitive interpretation given that they are directly expressed in a interpretable scale. For probit models we can think about standardized mean differences (e.g., Cohen's $d$) between categorical variables or standardized regression coefficient for a numerical predictor. For logit models we can fix the parameters thinking about the odds ratios. Meaningful odds ratios can be derived from previous literature, meta-analyses or converting to other effect sizes. For example, @Sanchez-Meca2003-ji proposed some equations to convert between odds ratios and Cohen's $d$. Using their approach, a Cohen's $d$ of 0.5 usually considered a plausible medium effect size corresponds to and odds ratio of $\approx 2.47$. Another more informative strategy is about plotting the predicted probabilities $g^{-1}(\eta)$ associated with certain regression coefficients. The `cat_latent_plot()` and `num_latent_plot()` functions can be used to see the impact of choosing a specific regression coefficients for respectively a categorical (Figures \@ref(fig:fig-example-cat-latent)) and numerical predictor (Figures \@ref(fig:fig-example-num-latent)).

```{r fig-example-cat-latent, fig.cap=filor::get_caption()}
#| echo: true
cat_latent_plot(m = c(0, 0.5), s = 1, probs = rep(1/4, 4), link = "logit", plot = "both")
```

```{r fig-example-num-latent, fig.cap=filor::get_caption()}
#| echo: true
num_latent_plot(x = runif(100), b1 = 2, probs = c(0.6, 0.2, 0.1, 0.1), link = "probit")
```

## Scale Effects

The default ordinal regression model assume that the variance of the underlying latent distribution is the same across condition. This is similar to a standard linear regression assuming the homogeneity of variance. For example, when comparing two groups or conditions we can run a standard linear model (i.e., a t-test) assuming homogeneity of variances or using the Welch t-test [see @Delacre2017-qy]. In addition, there are the so-called location-scale models that allows to include predictors also for the scale (e.g., the variance) of the distribution. This can be done also in ordinal regression where instead of assuming the same variance between conditions, the linear predictors can be included. The Equation (number here) expand the previous model including the linear predictor on the scale of the latent distribution.

\begin{equation}
g(P(Y \leq k)) = \frac{\alpha_k - X\beta}{e^{X\zeta}}
\end{equation}

\begin{equation} 
Y^\star_i = \eta + \epsilon_i
\epsilon_i \sim \mathcal{N}(0, e^{X\zeta})
\end{equation}

Where $X\zeta$ is the linear predictor $\eta$ for the scale of the distribution. By dafult for both the logit and probit model the scale is fixed to 1. On scale-location models we put predictors on both parameters. Given that the scale cannot be negative we use a log link function $\eta = \text{log}(X\zeta)$. Figure \@ref(fig:example-unequal-variance) depict an example of a comparison between two groups where the two underlying latent distributions have unequal variance. Ignoring the heterogeneity of variances could lead to underestimation of the effect size, reduced power and inflated type-1 error rate (qualche ref qui). Furtermore, as suggested by @Tutz2017-de location-scale models can be considered as a more parsimonius approach compared to partially or completely relaxing the proportional odds assumption. Allowing the scale to be different as a function of the predictor create more modelling flexbility. Furthemore, two groups could be theoretically different only in the scale of the latent distribution with a similar location. In this example, the only way to capture group differences is by including a scale effect. The Figure \@ref(fig:fig-scale-effect-example) depict the impact of having different scales between two groups on the ordinal probabilities.

@Tutz2022-dg provide a very clear and intuitive explanation of what happen when including a scale effect and how to interpret the result. As suggested before, the scale-location model allow to independently predict changes in the location and the scale. While location shifts are simply interpreted as increasing/decreasing the latent $\mu$ or the odds of responding a certain category scale effects are not straightforward. As the scale increase (e.g., the variance increase) there is an higher probability mass on extreme categories. On the other side as the scale decrease, responses are more concentrated on single categories. The categories are determined by the location parameter. For example, if one group have a certain latent mean $\mu_1$ and a small scale $\sigma^2 = 1/3$ (thus one third compared to the standard version of the distribution), all responses will be focused on categories around the latent mean. On the other side, increasing the scale will increase the cumulative probabilities for all categories and for values that tends to infinity extreme categories are preferred. Clearly, the scale parameter can somehow be interpreted as the response style (concentrated or variable). This is conceptually similar but implemented and interpreted differently to shift-location models [@Tutz2022-dg; Tutz2020-xq] where thresholds are allowed to vary as a function of predictors. Both models tries to increase the flexibility of modelling the probability structure of the responses by adding extra paprameters to predict the probabilities of the ordinal responses.

```{r}
sigma_high <- cat_latent_plot(m = c(0, 0), 
                s = c(1, 2), 
                probs = rep(1/4, 4), 
                link = "probit", 
                plot = "both",
                title = latex2exp::TeX("$\\sigma^2_{g_2} = 2$"))
sigma_low <- cat_latent_plot(m = c(0, 0), 
                s = c(1, 0.5), 
                probs = rep(1/4, 4), 
                link = "probit", 
                plot = "both",
                title = latex2exp::TeX("$\\sigma^2_{g_2} = 0.5$"))
sigma_high_mu1 <- cat_latent_plot(m = c(0, 1), 
                s = c(1, 2), 
                probs = rep(1/4, 4), 
                link = "probit", 
                plot = "both",
                title = latex2exp::TeX("$\\sigma^2_{g_2} = 2$, $\\mu_2 = 1$"))
sigma_low_mu1 <- cat_latent_plot(m = c(0, 1), 
                s = c(1, 0.5), 
                probs = rep(1/4, 4), 
                link = "probit", 
                plot = "both",
                title = latex2exp::TeX("$\\sigma^2_{g_2} = 0.5$, $\\mu_2 = 1$"))
plot_grid(sigma_high, sigma_low, sigma_high_mu1, sigma_low_mu1)
```

### Simulating scale effects

The location-scale model can be simulated using the `sim_ord_latent()` function and providing the predictors for the scale parameters. Given the `log` link function, predictors are provided on the `log` scale. For example, we simulate the effect of a binary variable $x$ representing two independent groups predicting the $k = 5$ response. We simulate a *location* effect of $\beta_1 = 0.5$ (in *probit* scale) and $\zeta_1 = \text{log}(2) = 0.70$. The first group has a $\sigma = 1$ and the second group has $\sigma = 2$. Again we simulate that the baseline probabilities are uniform for the first group.

```{r}
k <- 5  # number of options (1)
n <- 1e5 # number of observations (1)
b1 <- 0.5 # beta1, the shift in the latent distribution (2)
z1 <- log(2) # zeta1, the change in the scale
probs0 <- rep(1/k, k) # probabilities when x = 0 (3)
alphas <- prob_to_alpha(probs0, link = "probit") # get true thresholds from probabilities (3)
dat <- data.frame(x = rep(c(0, 1), each = n/2))
dat <- sim_ord_latent(~x, scale = ~x, By = b1, Bscale = z1, prob = probs0, data = dat, link = "probit")
fit <- clm(y ~ x, scale = ~ x, data = dat, link = "probit")
```

The table (put ref) reports the simulation results.

```{r}
#| eval: false
truth <- c(alphas, b1, z1)
names(truth) <- c(names(fit$alpha), names(fit$beta), names(fit$zeta))
#clm_table(fit, truth)
```

To better understand the impact of assuming (or simulating) a different latent scale we fit $k - 1$ binomial regressions and check the estimated coefficients. We are not simulating a specific beta for each outcome but simulating a scale effect is actually impacting the regression coefficients. When generating data for a binary outcome the linear predictor is composed by $\eta = \beta_0 + \beta_1x$. The threshold $\alpha$ and slope of the function can be estimated using $\alpha = -\frac{\beta_0}{\beta_1}$ and the slope is $\frac{1}{\beta_1}$ [@Knoblauch2012-to; @Faraggi2003-hm]. Under the proportional odds assumption, there is only a change in thresholds $\alpha$ this a shift in the sigmoid along the $x$ axis. When including a scale effect a change in the sigmoid is combined with a change in the slope.

```{r}
set.seed(2023)
k <- 4
n <- 1e5
b1 <- 3
d1 <- log(2)
dat <- data.frame(x = runif(n))
dat <- sim_ord_latent(~x, ~x, By = b1, Bscale = d1, prob = rep(1/k, k), data = dat, link = "probit")

dat$y1vs234 <- ifelse(dat$y <= 1, 1, 0)
dat$y12vs34 <- ifelse(dat$y <= 2, 1, 0)
dat$y123vs4 <- ifelse(dat$y <= 3, 1, 0)

dat$y <- ordered(dat$y)
fit <- clm(y ~ x, scale = ~x, data = dat, link = "probit")

fit1vs234 <- glm(y1vs234 ~ x, data = dat, family = binomial(link = "probit"))
fit12vs34 <- glm(y12vs34 ~ x, data = dat, family = binomial(link = "probit"))
fit123vs4 <- glm(y123vs4 ~ x, data = dat, family = binomial(link = "probit"))

fits <- list(y1vs234 = fit1vs234, fit12vs34 = fit12vs34, fit123vs4 = fit123vs4)

lapply(fits, function(x) coef(x)) |> 
  bind_rows(.id = "model") |> 
  mutate(xp = list(seq(0, 5, 0.01))) |> 
  unnest(xp) |> 
  ggplot(aes(x = xp, y = `(Intercept)` + x * xp)) +
  geom_line(aes(color = model))
```


## 2x2 interaction

A common research design could be a 2x2 factorial design. In this example we have 2 main effects and the interaction. Regardless the link function, we can focus the model equation on the linear predictor. Equation depicts the linear predictor for a 2x2 interaction. With a 2x2 design, a good strategy is to use sum to zero contrasts for the two factors. By default R use dummy coding but here we set the contrasts for $X_1$ and $X_2$ as $0.5$ and $-0.5$. In this way $\beta_1$ will be the main effect of $X_1$, $\beta_2$ the main effect of $X_2$ and $\beta_3$ the interaction (thus the difference of differences).

$$
g(P(Y \leq k))^{-1} = \alpha_k - \beta_1X_{1_i} + \beta_2X_{2_i} + \beta_3X_{1_i}X_{2_i}
$$

```{r}
n <- 1e5
dat <- expand.grid(x1 = c("a", "b"), x2 = c("c", "d"), n = 1:n)
dat$x1 <- factor(dat$x1)
dat$x2 <- factor(dat$x2)
contrasts(dat$x1) <- c(0.5, -0.5)
contrasts(dat$x2) <- c(0.5, -0.5)
bprobs <- rep(1/5, 5)

dat <- sim_ord_latent(~x1*x2, By = c(0, 1, 0.5), prob = bprobs, link = "probit", data = dat)
fit <- clm(y ~ x1 * x2, data = dat, link = "probit")
summary(fit)
```

The parameters intepretation is the same as introduced in the general case. The thresholds $\alpha$ are fixed the determines the probabilities $P(Y = k)$ when all predictors are zero. In this case by doing `alpha_to_prob(fit$alpha, link = "probit")` we should recover the `bprobs` vector. `x1` is the main effect thus the difference in $z$ scores between *a* and *b* averaging over `x2`. The same holds for `x2`. `x1:x2` is the difference of differences in $z$ scores thus $(ad - ac) - (bd - bc)$.

To visualize the effects there is the `ggeffect::ggpredict()` function that compute the predictions according to the specified predictors combination. In this case we can plot the interaction using the estimated probability for each group and $Y$ response.

```{r}
# terms =  c("x1", "x2") means to plot the interaction
plot(ggpredict(fit, terms = c("x1", "x2")))
```

## Numerical by factor interaction

Another common scenario is to compute and interaction between a numerical variable $x$ and a factor $g$. For simplicity we simulate the factor with two levels and the numerical variable sampled from an uniform distribution $x \sim \mathcal{U}(0, 1)$. Again, the thresholds $\alpha$ represents the baseline probabilities when $x = 0$ and $g = 0$.

```{r}
n <- 1e5
dat <- data.frame(x = runif(n),
                  g = rep(c("a", "b"), each = n/2))
dat$g <- factor(dat$g)
contrasts(dat$g) <- c(0.5, -0.5)
bprobs <- rep(1/5, 5)

dat <- sim_ord_latent(~ x * g, By = c(0.3, 0.5, 0.5), prob = bprobs, link = "probit", data = dat)
fit <- clm(y ~ x * g, data = dat, link = "probit")
summary(fit)
```

Again, `x` is the slope of the numerical predictor averaging over `g` (given that `g` has been coded with sum to zero contrasts) thus the increase in $z$ scores for a unit increase in `x`. `g1` is the main effect of the factor evaluated when $x = 0$. To change the interpretation of `g1` a possibility is to center `x` differently (e.g., mean centering). The `x:g1` is the difference in slopes between the two groups. 

Regardless of the model complexity, the `sim_ord_latent()` function can be expanded by including more predictors. Clearly, as the model complexity increase, choosing meaningful and empirically plausible parameters became more challenging.

# Power Analysis

In the previous sections we introduced how to simulate standard and scale-location cumulative models and how to fit the corresponding models using the `clm()` function from the `ordinal` package. In this section we introduce how to compute the power for some common scenarios. We present an example in a very general form that can be expanded for specific use cases. For example, by simulating a model fixing all effects to zero it is possibile to estimate the type-1 error rate or fixing the sample size and varying the effect size it is possible to do a sensitivity analysis focusing on the smallest effect size of interest (ref qui). The general workflow for a power simulation can be summarized as:

1. Choose the research design, e.g., 2x2 factorial design
2. Indentifify the specific effect to focus on. For example the interaction effect, a main effect or a specific contrast
3. Write the model equation similarly to what presented in the previous sections and extract the model coefficients
4. Choose one of the proposed strategy for simulating data and follows the corresponding steps
5. Fit the model and extract the relevant statistics e.g., the p-value of the interaction
6. Repeat the simulation a large number of times (e.g, 10000) and store the relevant values from each simulation
7. Summarise the simulation results in the appropriate way. For example, the power of the interaction is the number of times the p-value for the interaction test is lower than the critical level divided by the number of simulations.

To make a practical example, we simulate the power of a cumulative link model in detecting an effect of $d = 0.4$ (probit model) with $k = 6$ and $n = 40$ participants per group. We fix the baseline probabilities in the first group as uniform.

descrizione migliore

```{r}
n <- 40
k <- 6
d <- 0.4
nsim <- 1e3 # higher is better, just for example
bprobs <- rep(1/k, k)
alpha <- 0.05 # critical alpha

p <- rep(NA, nsim) # preallocation for speed
dat <- data.frame(group = rep(c("a", "b"), each = n))

for(i in 1:nsim){
  sim <- sim_ord_latent(~group, By = d, prob = bprobs, link = "probit", data = dat)
  fit <- clm(y ~ group, data = sim, link = "probit")
  p[i] <- summary(fit)$coefficients["groupb", "Pr(>|z|)"] # extract the pvalue
}
```

Then to compute the power we can simply compute the proportion of p values lower than the critical level. This can be done in R as `mean(p <= alpha)` resulting in an estimated power of `r mean(p <= alpha)`. Clearly using only a single condition is not really informative thus we can vary the sample size across plausible values and check the corresponding power curve.

```{r}
#| cache: true
n <- c(10, 40, 60, 100, 200)
power <- rep(NA, length(n))

for(i in 1:length(n)){
  p <- rep(NA, 0) # preallocation for speed
  dat <- data.frame(group = rep(c("a", "b"), each = n[i]))
  for(j in 1:nsim){
    sim <- sim_ord_latent(~group, By = d, prob = bprobs, link = "probit", data = dat)
    fit <- clm(y ~ group, data = sim, link = "probit")
    p[j] <- summary(fit)$coefficients["groupb", "Pr(>|z|)"] # extract the pvalue
  }
  power[i] <- mean(p <= alpha)
}
```

```{r}
power_df <- data.frame(power = power, n = n)

ggplot(power_df, aes(x = n, y = power)) +
  geom_point(size = 3) +
  geom_line() +
  ylim(c(0, 1)) +
  ylab("Power") +
  xlab("Sample Size (per group)") +
  geom_hline(yintercept = 0.8, lty = "dashed", color = "firebrick")
```

## Disclaimer about the functions

The current paper proposed a simplified way with some functions to generate ordinal data. For more complex simulations such as simulating correlated ordinal data the `simstudy` package [https://kgoldfeld.github.io/simstudy/articles/ordinal.html](https://kgoldfeld.github.io/simstudy/articles/ordinal.html) proposed a very comprehensive set of data simulation function also for ordinal data.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::

(ref:fig-ordinal-models) Theoretical figure about ordinal models. Voglio fare una figura come questa in @Burkner2019-aw ma con qualche modifica per fare vedere i tre modelli.

(ref:fig-prop-odds) caption here

(ref:tab-prop-ord-ex) caption here

(ref:fig-logit-vs-probit) caption here

(ref:tab-model-summary) caption here

(ref:fig-example-cat-latent) caption here

(ref:fig-example-num-latent) caption here

(ref:fig-explain-cumulative) caption here