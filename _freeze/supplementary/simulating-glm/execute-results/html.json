{
  "hash": "d0b70f103545343193a00fcd30e20384",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulating Generalized Linear Models\"\nformat: \n  html:\n    toc: true\nbibliography: \"https://raw.githubusercontent.com/filippogambarota/bib-database/main/references.bib\"\ncsl: \"https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\"\n---\n\n\n\n\n# Introduction\n\nThis document present a general workflow to simulate data for a binomial generalized linear model. This simulation is similar to what is presented in the paper for the special case of an ordinal regression.\n\n# Simulation\n\nThe general formula for a binomial GLM is presented in @eq-glm. Where $g^{-1}(\\cdot)$ is the inverse of the link function. In the case of a binomial regression with a *logit* link function $g(p) = \\text{logit}(p) = log(\\frac{p}{1 - p})$ and $g^{-1}[logit(p)] = \\text{invlogit}(p) = \\frac{e^p}{1 + e^p}$.\n\n$$\nP(Y = 1|\\mathbf{X}) = g^{-1}(\\mathbf{X} \\boldsymbol{\\beta})\n$$ {#eq-glm}\n\nThe linear combination of parameters $\\eta = \\mathbf{X} \\boldsymbol{\\beta}$ define the true probabilities. Then the random part of the model (in this case a *logistic* distribution) define the random variability around the true values. To make an example, let's assume to predict $P(Y = 1)$ with a numerical variable $x$ coming from an uniform distribution $x \\sim \\mathcal{U}(0, 1)$. As in standard regression we have two parameters:\n\n-   $\\beta_0$: the intercept. The probability of having 1 when all predictors are fixed to zero ($P(Y = 1|x = 0)$).\n-   $\\beta_1$: the slope. Is the increase in the log-odds of $Y = 1$ for a unit increase in $x$.\n\nThus we can choose two values for the parameters and plot the true relationship between $x$ and $P(Y = 1)$. The link function is `qlogis()` and the inverse of the link function is `plogis()`. Parameters are expressed in the scale of the of the link function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- qlogis(0.01) # probability of success when x = 0, in the link function space\nb1 <- 10 # increase in the log odds of success for a unit increase in x\nx <- seq(0, 1, 0.01) # x values\np <- plogis(b0 + b1 * x)\n\nplot(x, p, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](simulating-glm_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe curve depict the true probability of success for each value of $x$. To include the random error we need to sample the observed values from a Binomial distribution using the vector of probabilities `p`. We sample a vector of 0 and 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- length(x) # number of subjects\ny <- rbinom(ns, 1, p)\nhead(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 0 0 0 0 0\n```\n\n\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\n\n# binary values\nplot(x, jitter(y, amount = 0.05))\nlines(x, p)\n\n# grouped values\nxc <- cut(x, seq(0, 1, 0.1), include.lowest = TRUE)\nyc <- tapply(y, xc, mean)\n\nplot(as.integer(unique(xc))/10, yc)\nlines(x, p)\n```\n\n::: {.cell-output-display}\n![](simulating-glm_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIn practical terms, the $\\eta$ define the true probability of success for each observation (i.e., combination of predictors $\\mathbf{X}$) and then the random part is introduced by sampling from the assumed probability distribution.\n\nThen we can fit the logistic regression using `glm()` and we should recover the simulation parameters. Increasing the number of trials/observations will reduce the distance between the simulated the true values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- data.frame(x, y)\nfit <- glm(y ~ x, data = dat, family = binomial(link = \"logit\"))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x, family = binomial(link = \"logit\"), data = dat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -4.9500     0.9935  -4.982 6.28e-07 ***\nx            10.2098     1.9354   5.275 1.32e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 139.93  on 100  degrees of freedom\nResidual deviance:  62.82  on  99  degrees of freedom\nAIC: 66.82\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n\n```{.r .cell-code}\n# model\ncoef(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x \n  -4.949978   10.209815 \n```\n\n\n:::\n\n```{.r .cell-code}\n# truth\nc(b0 = b0, b1 = b1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      b0       b1 \n-4.59512 10.00000 \n```\n\n\n:::\n:::\n\n\nChanging $\\beta_0$ will affect the lower bound of the sigmoid curve while $\\beta_1$ determine the slope of the function. @fig-ex-sigmoid depicts logistic curves with different parameters.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](simulating-glm_files/figure-html/fig-ex-sigmoid-1.png){#fig-ex-sigmoid width=672}\n:::\n:::\n\n\nThis method is implemented in the paper when sampling from a categorical distribution. In the case of ordinal data we need $k - 1$ equations where $k$ is the number of ordered categories.\n",
    "supporting": [
      "simulating-glm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}