{
  "hash": "2364b9ca55d7cafd54864883a8a06343",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Ordinal Notes\"\nformat: \n  html:\n    toc: true\n---\n\n\n\n\n## $-\\mathbf{X}\\boldsymbol{\\beta}$ vs $\\mathbf{X}\\boldsymbol{\\beta}$ parametrization\n\nIn the tutorial we used the $\\alpha_k -\\mathbf{X}\\boldsymbol{\\beta}$ parametrization (thus with the minus sign) because this force the $\\beta$ to have the interpretation as in standard regression models. Usually, $\\beta$ is the increase in $y$ for a unit increase in $x$. A negative $\\beta$ means that the expected value of $y$ decrease for an increase in $x$. Let's see an example using the positive sign for $\\beta$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 4 # number of ordinal outcomes\nx <- c(0, 1) # a binary predictor\nb1 <- log(3) # log odds ratio comparing x1 and x0\nprobs0 <- rep(1/k, k)\nalpha <- prob_to_alpha(probs0, link = \"logit\")\nX <- matrix(x, nrow = 2)\n\n# positive sign\n(lp <- lapply(alpha, function(a) c(a + X %*% b1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`1|2`\n[1] -1.098612  0.000000\n\n$`2|3`\n[1] 0.000000 1.098612\n\n$`3|4`\n[1] 1.098612 2.197225\n```\n\n\n:::\n\n```{.r .cell-code}\ncump <- lapply(lp, plogis)\ncump <- cbind(0, data.frame(cump), 1)\np <- data.frame(t(apply(cump, 1, diff)))\nnames(p) <- paste0(\"y\", 1:k)\np$x <- x\np\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    y1   y2   y3   y4 x\n1 0.25 0.25 0.25 0.25 0\n2 0.50 0.25 0.15 0.10 1\n```\n\n\n:::\n\n```{.r .cell-code}\np |> \n  pivot_longer(starts_with(\"y\")) |> \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k + X\\\\beta$\"))\n```\n\n::: {.cell-output-display}\n![](ordinal-notes_files/figure-html/unnamed-chunk-2-1.svg){width=672}\n:::\n:::\n\n\nClearly a positive $\\beta$ create higher probability for lower $Y$ categories. This can be somehow not intuitive thus we can use the negative sign.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# negative sign\n\n(lp <- lapply(alpha, function(a) c(a - X %*% b1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`1|2`\n[1] -1.098612 -2.197225\n\n$`2|3`\n[1]  0.000000 -1.098612\n\n$`3|4`\n[1] 1.098612 0.000000\n```\n\n\n:::\n\n```{.r .cell-code}\ncump <- lapply(lp, plogis)\ncump <- cbind(0, data.frame(cump), 1)\np <- data.frame(t(apply(cump, 1, diff)))\nnames(p) <- paste0(\"y\", 1:k)\np$x <- x\n\np |> \n  pivot_longer(starts_with(\"y\")) |> \n  ggplot(aes(x = x, y = value, fill = name)) +\n  geom_col(position = position_dodge()) +\n  ylab(\"Probability\") +\n  ylim(c(0, 1)) +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") +\n  ggtitle(latex2exp::TeX(\"$P(Y \\\\leq k) = \\\\alpha_k - X\\\\beta$\"))\n```\n\n::: {.cell-output-display}\n![](ordinal-notes_files/figure-html/unnamed-chunk-3-1.svg){width=672}\n:::\n:::\n\n\nUsing the second parametrization, with positive $\\beta$ we have higher probability for higher $Y$ categories and the opposite.\n\nThe negative-sign parametrization is implicit when simulating from the latent distribution. Let's see an example. We use a continuous $x$ predictor because it is easier to see the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1e3\nx <- runif(1e3)\nB <- 3\nys_p <- x * B + rnorm(n)\nys_n <- x * -B + rnorm(n)\ny_p <- findInterval(ys_p, alpha) + 1\ny_n <- findInterval(ys_n, alpha) + 1\n\npar(mfrow = c(1,2))\nplot(x, ys_p, col = y_p, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = 3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\nlegend(\"bottomleft\", fill = 1:k, legend = paste0(\"Y\", 1:k))\n\nplot(x, ys_n, col = y_n, pch = 19, ylim = c(-7, 7), main = latex2exp::TeX(\"$\\\\Y^{*} = X \\\\beta + \\\\epsilon$, $\\\\beta = -3$\"), ylab = latex2exp::TeX(\"$Y^{*}$\"))\nabline(h = alpha, lty = \"dashed\")\n```\n\n::: {.cell-output-display}\n![Simulated data using the latent variable approach. The dotted lines are the thresholds $\\alpha$](ordinal-notes_files/figure-html/unnamed-chunk-4-1.svg){width=672}\n:::\n:::\n\n\n## Checking the impact of $\\mathbf{\\beta}$\n\nChoosing one or more plausible $\\beta_j$ values can be challenging. For a single $\\beta$ we can easily think about the odds ratio (for a *logit* model) or the Cohen's $d$ (for a *probit*) model. With multiple predictors and their interactions is not easy to fix plausible values. A good strategy is to try different values and check the impact on the predicted probabilities. In practice, we need to compute the predicted probabilities using the model equation for the $k$ ordinal outcomes. This can be easily done with the `sim_ord_latent()` function, fixing the `simulate = FALSE` parameter. In this way, only the predicted probabilities are computed. Let's see an example for a single $x$ sampled for a standard normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 4\ndat <- data.frame(x = seq(-4, 4, 0.1))\nb1 <- 0.5\nprobs0 <- rep(1/k, k)\ndat <- sim_ord_latent(~x, beta = b1, prob0 = probs0, data = dat, simulate = FALSE)\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     x       yp1      yp12     yp123        y1        y2         y3         y4\n1 -4.0 0.7112346 0.8807971 0.9568355 0.7112346 0.1695625 0.07603839 0.04316453\n2 -3.9 0.7008582 0.8754466 0.9547226 0.7008582 0.1745885 0.07927594 0.04527742\n3 -3.8 0.6902712 0.8698915 0.9525114 0.6902712 0.1796203 0.08261987 0.04748860\n4 -3.7 0.6794810 0.8641271 0.9501979 0.6794810 0.1846461 0.08607076 0.04980214\n5 -3.6 0.6684954 0.8581489 0.9477778 0.6684954 0.1896536 0.08962886 0.05222221\n6 -3.5 0.6573231 0.8519528 0.9452469 0.6573231 0.1946297 0.09329410 0.05475309\n```\n\n\n:::\n:::\n\n\nThen we can plot the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  pivot_longer(matches(\"^y[1-9]\")) |> \n  ggplot(aes(x = x, y = value, color = name)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](ordinal-notes_files/figure-html/unnamed-chunk-6-1.svg){width=672}\n:::\n:::\n\n\nIn this case the $\\beta_1 = 0.5$ can be considered a plausible value. Let's see what happens increasing it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(x = seq(-4, 4, 0.1)) |> \n  sim_ord_latent(~x, beta = 4, prob0 = probs0, data = _, simulate = FALSE) |> \n  pivot_longer(matches(\"^y[1-9]\")) |> \n  ggplot(aes(x = x, y = value, color = name)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](ordinal-notes_files/figure-html/unnamed-chunk-7-1.svg){width=672}\n:::\n:::\n\n\nWe can clearly see the difference and probably $\\beta = 4$ can be considered too large. To note, the same result can be achieved using the `num_latent_plot()` (that under the hood uses the `sim_ord_latent()` function).\n\nLet's make now an example, with an interaction between a continous and categorical predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- expand_grid(x = seq(-4, 4, 0.1), g = c(\"a\", \"b\"))\ndat$g <- factor(dat$g)\ncontrasts(dat$g) <- c(-0.5, 0.5)\nbeta <- c(b1 = 0.5, b2 = 1, b3 = 0.1)\ndat <- sim_ord_latent(~ x * g, beta = beta, prob0 = probs0, data = dat, simulate = FALSE)\n\ndat |> \n  pivot_longer(matches(\"^y[1-9]\")) |> \n  ggplot(aes(x = x, y = value, color = name, lty = g)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](ordinal-notes_files/figure-html/unnamed-chunk-8-1.svg){width=672}\n:::\n:::\n\n\nWe can see the impact of $\\beta_3 = 0.1$ that is the difference in slopes between the two groups. We can also use another plot to better see the group effect on each $Y$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat |> \n  pivot_longer(matches(\"^y[1-9]\")) |> \n  ggplot(aes(x = x, y = value, color = g)) +\n  geom_line() +\n  facet_wrap(~name)\n```\n\n::: {.cell-output-display}\n![](ordinal-notes_files/figure-html/unnamed-chunk-9-1.svg){width=672}\n:::\n:::\n",
    "supporting": [
      "ordinal-notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}