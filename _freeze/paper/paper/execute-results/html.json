{
  "hash": "7a539d4b7648ae2a437fe9e35bbe2952",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle             : \"Ordinal regression models made easy. A tutorial on parameter interpretation, data simulation, and power analysis.\"\nshorttitle        : \"Ordinal regression models made easy\"\n\nauthor: \n  - name          : \"Filippo Gambarota\"\n    affiliation   : \"1\"\n    corresponding : yes    # Define only one corresponding author\n    address       : \"Postal address\"\n    email         : \"filippo.gambarota@unipd.it\"\n    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)\n      - \"Conceptualization\"\n      - \"Writing - Original Draft Preparation\"\n      - \"Writing - Review & Editing\"\n  - name          : \"Gianmarco AltoÃ¨\"\n    affiliation   : \"1\"\n    role:\n      - \"Writing - Review & Editing\"\n      - \"Supervision\"\n\naffiliation:\n  - id            : \"1\"\n    institution   : \"Department of Developmental Psychology and Socialization, University of Padova, Italy\"\n\nauthornote: |\n  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.\n\n  Enter author note here.\n\nabstract: |\n  Abstract here\n  \nkeywords          : \"ordinal, likert, simulations, power\"\nwordcount         : \"X\"\n\nbibliography      : \"https://raw.githubusercontent.com/filippogambarota/bib-database/main/references.bib\"\n\nfloatsintext      : yes\nlinenumbers       : no\ndraft             : no\nmask              : no\n\nfigurelist        : no\ntablelist         : no\nfootnotelist      : no\nparams:\n  floatplaceh: true\n\nclassoption       : \"man\"\noutput: \n  papaja::apa6_pdf:\n    latex_engine: xelatex\n    header_includes:\n      - \\usepackage{booktabs, caption, longtable, colortbl, array, hhline, mathbf}\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n \\normalsize\n\n\n\n\n# Introduction\n\nPsychological research make an extensive use of ordinal data. One of the main reason is probably the usage of Likert scales [@Likert1932-xw]. Ordinal data refers to a specific type of measurement scale [@Stevens1946-te] where ordered numbers are assigned to a variable. Compared to nominal scale and as the name suggest the labels are ordered. Compared to interval or ratio scales there is no explicit assumption about the distance between labels. An example is asking people the degree of agreement about a certain statement using a scale from 1 (no agreement) to 7 (total agreement). Answering 4 compared to 2 suggest an higher agreement but we cannot affirm that there is two times the agreement compared to the second answer. @Stevens1946-te and @Kemp2021-dj suggested that for ordinal variables is appropriate to calculate ranks-based descriptive statistics (e.g., median or percentiles) instead of metric statistics (e.g., mean or standard deviation) and using appropriate inferential tools [e.g., @Cliff1996-qo; @Agresti2010-rz]. This distinction in terms of the appropriateness of certain descriptive statistics is also relevant when modeling data. Treating ordinal data as metric refers to assuming the labels as actual integer numbers thus assuming a fixed and know distance between levels [@Liddell2018-wu]. More generally, @Cliff2016-ck suggest that most of the research questions in behavioral sciences can be considered as ordinal (*is the score* $x$ higher than the score $y$?) concerning variables where the most appropriate measurement scale is probably ordinal.\n\nIn Psychology especially when using item-based measures (questionnaires, surveys, etc.) the common practice is using a normal linear regression that makes an explicit assumption about metric features of the response variable. @Liddell2018-wu reviewed the psychological literature using likert-based measures and reported how the majority of papers used metric-based statistical models. In the same work, @Liddell2018-wu showed extensive examples and simulations about the potential pitfalls of treating an ordinal variable as metric [but see @Robitzsch2020-la for an alternative perspective]. They reported problems in terms of lack of power, inversion of the effects (e.g., finding a negative effect when the true effect is positive) and distorted effect size estimates. Some authors suggested that individual ordinal items and a collection of ordinal (averaged or summed) items can safely be considered and analyzed as metric [@Carifio2008-qd; @Carifio2007-kk; but see @Jamieson2004-xf]. Despite @Liddell2018-wu provide some example that averaging ordinal items and applying metric models is not appropriate, in the current paper we discuss only cases where there is a single ordinal outcome (e.g., a single item, question, etc.).\n\nFor the tutorial, we assume the reader is already familiar with basic R programming and introductory theory about linear regression. (VEDI SE SCRIVERE MEGLIO O QUALCOSA DI PIU). The current tutorial proposed a set of custom R functions (available on Github ) to understand and simulate basic ordinal regression models. More details and extended examples are available on the supplementary materials.\n\n## Ordinal regression models\n\nDespite the actual modeling proposal by @Liddell2018-wu, there is a class of regression models taking into account the ordinal nature of the response variable without metric assumptions. We can name this general class of models as *ordinal regression*. The actual statistical nomenclature can be confusing mainly because there are several types of models with different assumptions and structures [@Tutz2022-dg]. @Tutz2022-dg and @Burkner2019-aw provide a clear and updated taxonomy of ordinal regression models. We can identify three main classes: *cumulative models* [CM, @McCullagh1980-cw; @Agresti2010-rz], *sequential models* [@Tutz1990-fe] and *adjacent category models*. The cumulative is the mostly used model assuming the existence of a latent variable that categorized using a set of thresholds produces the observed ordinal variable.\n\nThe *sequential model* as suggested by the name is appropriate when modelling sequential processes. Assuming to have five response options, the sequential model assume that responding \"3\" assume a sequential process where steps \"1\" and \"2\" are already reached. A clear example is proposed by @Burkner2019-aw where the marriage duration in years is predicted as a function of some explanatory variables. For each level of the response variable there is a latent distribution where the step between a marriage year $k = 1$ and the next years $k > 1$ is modeled by the sequential model. When comparing $k$ with $k > 1$, everything lower than $k$ is assumed to be already reached [@Tutz2020-xq]. The adjacent category model compare the category $k$ with $k + 1$ still assuming a latent distribution for each $k$. As suggested by @Tutz2022-dg the adjacent-category model can be seen as a series of binary binomial regressions taking into account the order of the categories. @Burkner2019-aw suggested that adjacent-category model can be chosen for its mathematical convenience and there is no a clear empirical intepretation as for the cumulative vs sequential model.\n\nIn the current paper we put the focus on the CM for several reasons. The first reason is that the latent formulation of the model is particularly convenient both for parameter interpretation and data simulation. The second reason is that several psychological variables can be formalized as a latent continuous variable observed as an ordinal item. Furthermore, CM are also used to model data under a signal detection theory framework [e.g., @DeCarlo2010-lj]. The Figure \\@ref(fig:fig-explain-cumulative) depict the overall structure of the *cumulative* model.\n\n## Model notation\n\nIn this section we introduce some notation for the CM that is used through the paper and in the R code. We proposed a notation as consistent as possible with the literature and the `ordinal` R package used in the tutorial. We define $Y_k$ as the observed ordinal response with $1, \\dots, k$ levels and $Y^\\star$ is the underlying latent variable. The latent variable is segmented using $k - 1$ thresholds $\\alpha_1, \\dots, \\alpha_{k - 1}$. Similarly to the generalized linear models framework, we define $g(x) = \\eta$ as the link function that maps probabilities into the linear predictor $\\eta$. To transform back $\\eta$ into probabilities we use the inverse of the link function $x = g^{-1}(\\eta)$. The specific link function define the type of model and require a different R function. For example, when assuming a Gaussian distribution we are fitting a model with a `probit` link function is the cumulative distribution function $g(x) = \\Phi^{-1}(x) = \\eta$ and the inverse of the link function is the inverse cumulative distribution function (or quantile function) defined $x = g^{-1}(\\eta) = \\Phi(\\eta)$. When modelling an ordinal variable in a cumulative link model we actually modelling the cumulative probability $P(Y \\leq k), k = 1, \\dots, k - 1$[^1]. Equation \\@ref(eq:prob-cum-model1) depict the general cumulative model including predictors $\\mathbf{X}$ and regression coefficients $\\boldsymbol{\\beta}$. The minus sign in $\\mathbf{X} \\boldsymbol{\\beta}$ is used to interpret the $\\beta$ as in the standard regression model [@Agresti2010-rz] where higher $\\beta$ values corresponds to increased probability of responding higher $k$ categories.\n\n[^1]: As done by @Agresti2010-rz, when referring to $P(Y \\leq k)$ we are implicitly conditioning on a particular $x$ value $P(Y \\leq k | x_i)$\n\n\\begin{equation} \nP(Y \\leq k) = g^{-1}(\\alpha_k - \\mathbf{X} \\boldsymbol{\\beta}), \\;\\;k = 1, \\dots, k - 1\n(\\#eq:prob-cum-model1)\n\\end{equation}\n\nThe $\\mathbf{X} \\boldsymbol{\\beta}$ is the linear predictor $\\eta$ that is the cumulative probability $P(Y \\leq k)$ transformed using the link function $g(\\cdot)$. To obtain the probability of a single outcome $P(Y = k)$ we can compute the difference between cumulative probabilities as shown in Equation \\@ref(eq:prob-cum-model2).\n\n\\begin{equation} \nP(Y = k) = g^{-1}(\\alpha_k - \\eta) -  g^{-1}(\\alpha_{k - 1} - \\eta), \\;\\;k = 1, \\dots, k - 1\n(\\#eq:prob-cum-model2)\n\\end{equation}\n\nThere are always two special cases when computing the probability of a single outcome $Y$ that is when $Y = 1$ and $Y = k$. In the first case the cumulative probability is calculated from $-\\infty$ that is the same as temporary assuming an $\\alpha_0 = -\\infty$. In the second case ($Y = 1$) the probability is calculated as $P(Y = k) = 1 - g^{-1}(\\alpha_{k - 1} - \\eta)$ that is the same as assuming a temporary threshold $\\alpha_k = +\\infty$. The Figure \\@ref(fig:fig-explain-cumulative) shows how the single probabilities of the ordinal outcome are calculated from cumulative probabilities.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-explain-cumulative)](paper_files/figure-html/fig-explain-cumulative-1.pdf){#fig-explain-cumulative fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 1 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n<!-- notazione https://people.vcu.edu/~dbandyop/BIOS625/CLM_R.pdf e @Burkner2019-aw per la parte di notazione e quella piÃ¹ formale anche @DeCarlo2010-lj -->\n\nThe same model can be written in the so-called latent formulation as reported in Equation \\@ref(eq:latent-model). The model is no longer directly about the cumulative probabilities but focused on the continuous latent variable $Y^\\star$ as a function of the linear predictor $\\eta = \\mathbf{X} \\boldsymbol{\\beta}$ similar to a standard linear regression.\n\n\\begin{equation} \nY^\\star_i = \\mathbf{X}\\boldsymbol{\\beta} + \\epsilon_i\n(\\#eq:latent-model)\n\\end{equation}\n\nThe crucial part is $\\epsilon_i$ that is the random component of the model coming from a certain probability distribution. For a *probit* model, errors are sampled from a standard Gaussian distribution while for a *logit* model from a standard logistic distribution. Following the notation by @Tutz2022-dg, the observed ordinal value $Y_i = k$ comes from $Y^\\star_i$ belonging to the interval defined by the thresholds $Y_i = k \\iff \\alpha_{k - 1} < Y^\\star_i < \\alpha_{k}$ where $- \\infty = \\alpha_0 < \\alpha_1 < \\dots< \\alpha_{k - 1} < \\alpha_k = \\infty$.\n\nIn the basic version of the model, the thresholds $\\alpha_k$ are considered as fixed and being part of the measurement procedure [@Liddell2018-wu] and do not vary as a function of the predictors. In a more sophisticated version of the model called location-shift [@Tutz2022-dg], both the location $\\mu$ and the thresholds $\\alpha_k$ can vary as a function of the predictors.\n\nThe model can be also formalized in alternative ways. @Liddell2018-wu and @Kruschke2015-re proposed a Bayesian version of the model with a different threshold parametrization. @Gelman2020-tg proposed three alternative parametrizations focusing on different definition of the thresholds.\n\n@Tutz2022-dg described also another version of the model called location-scale [@Tutz2022-dg; @Rigby2005-ko; @Cox1995-ur] where the location $\\mu$ and the scale $\\sigma^2$ of the distribution can vary as a function of the predictors.\n\n## Link function\n\nThe cumulative link model implemented in Equations \\@ref(eq:prob-cum-model1) and \\@ref(eq:latent-model) can be considered the general formulation that requires specifying the link function $g(\\cdot)$ or the errors distribution $\\epsilon_i \\sim D(\\mu, \\sigma^2)$. Among several available functions the *logit* and *probit* models are the most common. The *logit* model assume a *logit* link function and a logistic distribution as latent variable. On the other side, the *probit* model assume a Gaussian distribution.\n\nThe two models provide similar results with a different parameters interpretation. In the next sections we will illustrate the differences and simulation strategies. Figure \\@ref(fig:fig-logit-vs-probit) depicts the two distributions while Table \\@ref(tab:tab-model-summary) summarise the presented cumulative models with the proposed link function and the corresponding R code.\n\nIn terms of parameters, both distributions can be defined with a location $\\mu$ and a scale $s$ parameter. The standard normal distribution has $\\mu = 0$ and $s = 1$. Furthermore the variance corresponds to the scale $s^2 = \\sigma^2 = 1$. The variance of the logistic distribution is $\\sigma^2 = \\frac{s^2\\pi^2}{3}$. The standard logistic distribution has $\\mu = 0$ and $s^2 = 1$ thus the standard deviation simplified to $\\frac{\\pi}{\\sqrt{3}} \\approx 1.81$. In practical terms, fixing $\\mu$ and $s$ lead to an higher standard deviation for the logistic distribution.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-logit-vs-probit)](paper_files/figure-html/fig-logit-vs-probit-1.pdf){#fig-logit-vs-probit fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 2 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\\begin{table}\n\n\\caption{\\label{tab:tab-model-summary}(ref:tab-model-summary)}\n\\centering\n\\begin{tabular}[t]{lllll}\n\\toprule\n\\multicolumn{1}{c}{} & \\multicolumn{2}{c}{Link Function} & \\multicolumn{2}{c}{Inverse Link Function} \\\\\n\\cmidrule(l{3pt}r{3pt}){2-3} \\cmidrule(l{3pt}r{3pt}){4-5}\nModel & Equation & R Code & Equation & R Code\\\\\n\\midrule\nCumulative Logit & $\\text{logit}(p) = \\text{log}(p / (1-p))$ & `qlogis()` & $e^{\\text{logit}(p)} / (1 + e^{\\text{logit}(p)})$ & `plogis()`\\\\\nCumulative Probit & $z = \\Phi^{-1}(p)$ & `qnorm()` & $\\Phi^(z)$ & `pnorm()`\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n:::\n:::\n\n \\normalsize\n\n\n# Model fitting\n\nFor fitting the cumulative models we used the `ordinal` package [@Christensen2019-cz]. Despite the presence of other possibilities the `ordinal` package provide the most complete and intuitive way to implement the ordinal models. The syntax is very similar to standard linear models in R and default functions to calculate predictions, perform model comparison, extract relevant model information are implemented similarly to standard regression modelling.[^2].\n\n[^2]: For a very complete overview of the ordinal package see @Christensen2019-cz and the Package documentation <https://cran.r-project.org/web/packages/ordinal/ordinal.pdf>\n\nThe function to fit the model is `clm()` and the model equation is specified using the R formula syntax `y ~ x` where `y` is the ordinal dependent variable and `x` is one or more predictors eventually including also the interactions. The package also implements mixed-effects models (see the `clmm()` function) including random intercepts and slopes but the topic is beyond the scope of the current tutorial.\n\nWhen fitting the model the crucial arguments are the `formula`, the `link` function and the `data`. More advanced arguments are the `nominal`, `scale` and `threshold`.\n\n-   `formula`: the formula `y ~ x` with the dependent variable and predictors.\n-   `link`: is the link function. In this tutorial we consider only the *logit* and *probit* link but other link functions are available.\n-   `data`: is the dataset where with the variables included in the `formula`\n-   `nominal`: formula with predictors where the proportional odds assumption (See Section ) is relaxed (i.e., partial or non proportional odds)\n-   `scale`: formula with predictors for the scale (standard deviation) parameter. This argument allow to fit a scale-location model (see Section ). The main `formula` argument refers to predictors on the location parameter (i.e., the mean $\\mu$).\n-   `threshold`: different structures for estimating the thresholds. The default is `threshold = \"flexible\"` where $k - 1$ threshold (where $k$ is the number of ordinal levels for $Y$) are estimated.\n\nWe can start by fitting a simple model, highlighting the crucial parameters where the detailed explanation will be expanded in the next sections. Table \\@ref(tab:tab-dataset-example) contains simulated data from $n = 100$ participants rating the agreement about a certain item with $k = 4$ ordered options. The participants are divided into two groups ($x$). We can fit a cumulative link model with `clm()` function and check the model summary.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\\begin{table}\n\n\\caption{\\label{tab:tab-dataset-example}(ref:tab-dataset-example)}\n\\centering\n\\begin{tabular}[t]{llllllll}\n\\toprule\ngroup & mean & median & sd & Y1 & Y2 & Y3 & Y4\\\\\n\\midrule\na & 2.84 & 3 & 1.11 & 8 (p = 0.16) & 11 (p = 0.22) & 12 (p = 0.24) & 19 (p = 0.38)\\\\\nb & 3.02 & 3 & 1.08 & 5 (p = 0.10) & 13 (p = 0.26) & 8 (p = 0.16) & 24 (p = 0.48)\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\n:::\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- clm(y ~ x, data = dat, link = \"logit\")\nsummary(fit)\n#> formula: y ~ x\n#> data:    dat\n#> \n#>  link  threshold nobs logLik  AIC    niter max.grad cond.H \n#>  logit flexible  100  -128.88 265.77 5(0)  9.09e-13 2.2e+01\n#> \n#> Coefficients:\n#>    Estimate Std. Error z value Pr(>|z|)\n#> xb   0.3153     0.3670   0.859     0.39\n#> \n#> Threshold coefficients:\n#>     Estimate Std. Error z value\n#> 1|2  -1.7503     0.3432  -5.100\n#> 2|3  -0.3791     0.2722  -1.393\n#> 3|4   0.4381     0.2717   1.612\n```\n:::\n\n \\normalsize\n\n\nThe two main sections of the model summary are the *Coefficients* section reporting the regression coefficients $\\beta$ and the *Threshold* section reporting the $\\alpha$ estimation. Given that $k = 4$ we have $k - 1 = 3$ thresholds and a single $\\beta$ associated with the $x$ effect. As in standard regression models, when $x$ is a categorical predictor with $j$ level, we will estimate $j - 1$ regression coefficients where the interpretation depends on the contrast coding [see @Schad2020-ht]. In R the default is the dummy coding where a factor of $j$ levels is converted into $j - 1$ dummy variables. By default, the first level of the factor is taken as the reference level and the $j - 1$ coefficients represent the comparison between the other levels and the reference.\n\n# Interpreting parameters\n\n## *Logit Model*. Odds and odds ratio\n\nTo understand the logit model we need to introduce odds and odds ratio. The odds of a probability $p$ is defined as $\\frac{p}{1 - p}$ thus the probability of success divided by the probability of failure. The odds takes value ranging from 0 to $\\infty$. For example with a probability of $p = 0.8$ we have an odds of $4$, thus we have a 4 successes for each failure. The same as having $p = 0.2$ and an odds of $0.25$ means that for each $0.25$ successes we have a failure or that we have $4$ failures for each success. When comparing two groups or conditions we can compare the two odds calculating an odds ratio. The odds ratio is the mostly used statistics to compare groups or conditions on a binary outcome. An odds ratio of $4$ means that the odds of success at the numerator is 4 times higher than the odds of success at the denominator. The Figure \\@ref(fig:fig-odds-example) shows the relationship between probabilities and odds. The logit transformation is about taking the logarithm of the odds creating a symmetric function ranging from $-\\infty$ to $\\infty$ with $p = 0.5$ as the midpoint because $\\text{log}(\\frac{0.5}{1 - 0.5}) = 0$. The standard logistic regression with two outcome model the logit transformed probability.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-odds-example)](paper_files/figure-html/fig-odds-example-1.pdf){#fig-odds-example fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 3 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\nOdds and odds ratios are clearly defined with $k = 2$ outcomes. With an ordinal variable and a cumulative model we can use the cumulative odds ratio. With e.g. $k = 4$ outcomes we have $k - 1$ models determined by the cumulative probability in terms of $P(Y \\leq 1), \\dots, P(Y \\leq k - 1)$. We can manually calculate the odds ratio starting from the previous dataset.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# function to calculate odds\nodds <- function(p) p / (1 - p)\n\n# probability of Y = 1 for the group a\ndat$yn <- as.integer(dat$y) # better as number here\n(p1a <- mean(dat$yn[dat$x == \"a\"] == 1))\n#> [1] 0.16\n\n# thus P(Y > 1) = 1 - p1a\nodds(p1a)\n#> [1] 0.1904762\n\n# odds ratio Y = 1 a vs b\n(p1b <- mean(dat$yn[dat$x == \"b\"] == 1))\n#> [1] 0.1\nodds(p1a) / odds(p1b)\n#> [1] 1.714286\n```\n:::\n\n \\normalsize\n\n\nFor example the probability of responding $Y = 1$ in the group \"a\" is 0.16 corresponding to an odds of 0.1904762. When calculating the odds ratio comparing \"a\" vs \"b\" for $Y = 1$ we obtain that the group \"a\" has 1.7142857 times the odds of responding $Y = 1$ compared to the group \"b\".\n\nFor an ordinal variable we have $k$ levels and we can calculate $k - 1$ cumulative probabilities (excluding the last associated with $p = 1$). Thus we have $k - 1$ odds ratio or what is called a cumulative odds ratio. Basically we repeat the previous steps but on the cumulative probability $P(Y \\leq k), k = 1, \\dots, k - 1$. The three odds ratios are very similar correspond to the model estimate $e^\\beta_1$, in R `exp(fit$beta)` 1.3706633. In fact, the logit model essentially estimate the cumulative odds ratio.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# the dummy_ord() is a custom function creating k - 1 dummy variable for the cumulative probabilities\n\n# data.frame with the group (x) and the k - 1 dummy variables\ncum_p <- cbind(x = dat$x, dummy_ord(dat$y))\n\n# calculating the cumulative probability for k - 1 variables. \n# Taking the average of a series of 0-1 is the same as computing the proportion of 1s.\n(group_a <- apply(cum_p[cum_p$x == \"a\", -1], 2, mean))\n#> y1vs234 y12vs34 y123vs4 \n#>    0.16    0.38    0.62\n(group_b <- apply(cum_p[cum_p$x == \"b\", -1], 2, mean))\n#> y1vs234 y12vs34 y123vs4 \n#>    0.10    0.36    0.52\n\n# calculating k - 1 odds ratios on the cumulative probabilities as a/b\nodds(group_a) / odds(group_b)\n#>  y1vs234  y12vs34  y123vs4 \n#> 1.714286 1.089606 1.506073\n```\n:::\n\n \\normalsize\n\n\n## Proportional odds assumption\n\nFrom the previous example, we noticed that the $k - 1$ odds ratios are different. However, increasing $n$ will result in odds ratios that are very similar. This does not happen by chance but from an implicit (until now) assumption made by the CM called *proportional odds* (PO). Following again the taxonomy by @Tutz2022-dg, each of the presented ordinal regression model has a basic version making the PO assumption. There are more advanced versions of the model relaxing this assumption completely (*non proportional odds*) and partially (*partial proportional odds*).\n\nBasically if we use the *logit* link function $g(\\cdot)$, regression coefficients are interpreted as odds ratios. Given that we have $k > 2$ alternatives we need $k - 1$ equations. The PO assumption is formalized in Equation \\@ref(eq:prop-odds). The cumulative log odds ratio comparing $P(Y_k|x_0)$ with $P(Y_k|x_1)$ is the same regardless the specific threshold ($\\beta_1 = \\beta_2 \\dots = \\beta_{k - 1}$). The Figure (\\@ref(fig:fig-prop-odds)) depicts the proportional odds assumption for the $k - 1$ logistic curves both for probabilities and linear predictors $\\eta$.\n\n<!-- - https://hbiostat.org/ordinal/impactpo.pdf blog su proportional odds, mi pare di capire che non sia cosÃ¬ problematica come assunzione -->\n\n\\begin{equation}\n\\text{logit} (\\frac{P(Y \\leq 1 |x_1)}{P(Y \\leq 1 |x_0)}) = \\dots = \\text{logit} (\\frac{P(Y \\leq k - 1 |x_1)}{P(Y \\leq k -1 |x_0)})\n(\\#eq:prop-odds)\n\\end{equation}\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-prop-odds)](paper_files/figure-html/fig-prop-odds-1.pdf){#fig-prop-odds fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 4 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\nThe PO assumption is convenient because regardless of $k$, the $\\beta_j$ ($j$ being the number of regression coefficients) effect is assumed to be the same. The model is more parsimonious compared to estimating $k - 1$ coefficients for each $\\beta_j$ as in the multinomial regression or the non-proportional odds model. \n\nAt the same time, the PO assumption could be considered too strict. There are several methods for testing if data are supporting the PO assumption [see @Liu2023-bp for an overview]. @Tutz2020-xq suggested a trade-off between assuming/relaxing the PO assumption by fitting *location-shift* or *location-scale* models. Basically these methods should guarantee more flexibility in modelling the observed probabilities reducing the number of parameters. However, these methods and models are outside the scope of the tutorial. In the supplementary materials, we provide more details only about the *location-scale* model in terms of parameters interpretation and data simulation.\n\nThis can be shown by fitting the model with `clm()` that by default assume the PO and using the predicted probabilities to compute the odds ratios. In the following code we computed the odds ratio comparing $x_a$ and $x_b$ when $k \\leq 1$ and $k \\leq 2$. The odds ratios are exactly the same because we are using the predicted probabilities from the model (assuming PO).\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fitting the model\nfit <- clm(y ~ x, data = dat, link = \"logit\")\n\n# extracting the predicted probabilities for the two groups\npr <- predict(fit, data.frame(x = unique(dat$x)))$fit\n\n# y <= 1\ny1a <- pr[1, 1]\ny1b <- pr[2, 1]\n\n# y <= 2\ny12a <- sum(pr[1, 1:2])\ny12b <- sum(pr[2, 1:2])\n\n# odds ratio y <= 1 (a vs b) VS y <= 2 (a vs b)\nodds(y1a) / odds(y1b)\n#> [1] 1.370663\nodds(y12a) / odds(y12b)\n#> [1] 1.370663\n```\n:::\n\n \\normalsize\n\n\n## *Probit Model* $z$ scores\n\nWhen assuming a standard Gaussian distribution we are fitting *probit* model. The main difference regards the parameters interpretation. In the logit model the $\\boldsymbol{\\beta}$ are the log odds ratio. For categorical variables they represents the increase in the log odds of moving from one level to the other while for numerical variables is the increase in the log odds for a unit increase in $x$. \n\nFor the *probit* model, the $\\beta$ is the increase in terms of $z$ scores for a unit increase in $x$. This is very convenient especially for categorical variables because parameters can be interpreted as a Cohen's $d$ like measure. Thinking about the latent distributions, the $\\beta$ is the shift in the latent mean comparing two or more groups or the slope of latent scores as a function of a numeric $x$. More formally the shift in the latent distribution is $\\frac{\\beta}{\\sigma}$ (for the *probit* model $\\sigma = 1$). The interpretation in terms of shifting the latent mean holds also for the logistic model. However, the standard deviation of the standard logistic regression is $\\frac{\\pi}{\\sqrt{3}} \\approx 1.81$[^4]. The $\\beta$ for the logistic distribution can be interpreted as the location shift of the latent logistic distribution by $\\beta/(\\frac{\\pi}{\\sqrt{3}})$ standard deviations.\n\n[^4]: Actually the variance of the logistic distribution is $\\frac{s^2\\pi^2}{3}$ and the standard deviation $\\frac{s\\pi}{\\sqrt{3}}$ where $s$ is the scale of the distribution. For the standard logistic distribution $s = 1$ (as for the standard normal distribution).\n\n### Proportional odds and *probit* model\n\nThe PO assumption is relevant only for the *logit* model. When using a different link function (e.g., *probit*) the assumption of a common $\\beta$ is called *parallel slopes* (see Equation \\@ref(eq:probit-prop-odds)). Basically the difference in $z$ scores for a unit increase in $x$ (i.e., the slope) is the same regardless the threshold. As for the previous example, we can fit the *probit* model and manually calculate the difference in $z$ scores.\n\n\\begin{equation}\nz_{x_1 - x_0} = \\Phi(P \\leq 1|x_0) - \\Phi(P \\leq 1|x_1) = \\dots = \\Phi(P \\leq k - 1|x_0) - \\Phi(P \\leq k - 1|x_1)\n(\\#eq:probit-prop-odds)\n\\end{equation}\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fitting the model\nfit <- clm(y ~ x, data = dat, link = \"probit\")\n\n# extracting the predicted probabilities for the two groups\npr <- predict(fit, data.frame(x = unique(dat$x)))$fit\n\n# y <= 1\ny1a <- pr[1, 1]\ny1b <- pr[2, 1]\n\n# y <= 2\ny12a <- sum(pr[1, 1:2])\ny12b <- sum(pr[2, 1:2])\n\n# z score difference y <= 1 (a vs b) VS y <= 2 (a vs b)\nqnorm(y1a) - qnorm(y1b)\n#> [1] 0.204808\nqnorm(y12a) - qnorm(y12b)\n#> [1] 0.204808\n```\n:::\n\n \\normalsize\n\n\n## Simulating data\n\nIn this tutorial we present two method for simulating ordinal data. The first method calculate the probabilities of each $Y$ level as a function of predictors and generate data from a multinomial distribution. The second method simulate data using the the latent formulation.\n\n### Simulating from a multinomial distribution\n\nFor the first method we need to calculate $g^{-1}(\\eta)$ as a function of predictors and then sample from a *multinomial* (more specifically *categorical*) distribution using the `sample()` function in R. This method is similar to the general way of simulating data for a generalized linear model (see the supplementary materials for a general overview) \n\nAs a simple example, let's simulate two groups with $n = 100$ participants responding to an item with $k = 4$ ordered options. We simulate the second group having higher probability of responding higher categories of $Y$. \n\nAs explained in the previous sections, we can summarise the effect size of a CM (assuming PO) using a single $\\beta = \\log(OR)$. We can assume that the odds ratio is $OR = 2$. For simplicity, the probabilities of the first group $x = 0$ are uniform thus $P(Y = 1|x_0) = ... P(Y = k|x_0) = 1/k$. The following code summarise the first steps of the simulation. Basically we define the simulation parameters, we calculate the $k - 1$ thresholds $\\alpha$ and we apply Equations \\@ref(eq:prob-cum-model1) and \\@ref(eq:prob-cum-model2). In this way we calculated the true probability of each $Y$ level for the two groups.\n\nAn important step is converting from $\\alpha$ to probability and the opposite. This steps are implemented in the `alpha_to_prob()` and `prob_to_alpha()` functions. Basically given the input and the link function, these functions return the thresholds associated with $k$ probabilities or the $k - 1$ thresholds.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- 5\n(p <- rep(1/k, k)) # uniform\n#> [1] 0.2 0.2 0.2 0.2 0.2\nnames(p) <- paste0(\"y\", 1:k)\n\n(alpha <- prob_to_alpha(p, link = \"logit\")) # or prob_to_alpha(p, \"probit\")\n#>         y1         y2         y3         y4 \n#> -1.3862944 -0.4054651  0.4054651  1.3862944\nalpha_to_prob(alpha, link = \"logit\")\n#> [1] 0.2 0.2 0.2 0.2 0.2\n```\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## SIMULATION PARAMETERS\n\nN <- 100 # sample size\nor <- 2 # odds ratio\nk <- 4 # number of ordinal alternatives\nprob0 <- rep(1/k, k) # probabilities for the first group\nalpha <- prob_to_alpha(prob0, link = \"logit\")\ndat <- data.frame(x = rep(c(0, 1), each = N/2))\n\n## LINEAR PREDICTOR\n\n# calculate linear predictor using equation TODO put equation\n# obtaining k - 1 equations\n\nlp <- lapply(alpha, function(a) a - log(or) * dat$x)\nnames(lp) <- sprintf(\"lp_leq%s\", 1:(k - 1)) # giving appropriate names\nlp <- data.frame(lp)\nhead(lp)\n#>     lp_leq1 lp_leq2  lp_leq3\n#> 1 -1.098612       0 1.098612\n#> 2 -1.098612       0 1.098612\n#> 3 -1.098612       0 1.098612\n#> 4 -1.098612       0 1.098612\n#> 5 -1.098612       0 1.098612\n#> 6 -1.098612       0 1.098612\n\n## CUMULATIVE PROBABILITIES\n\n# apply the inverse of the link function (invlogit) to calculate cumulative probabilities\ncump <- lapply(lp, plogis)\ncump <- data.frame(cump)\nnames(cump) <- sprintf(\"cump_leq%s\", 1:(k - 1)) # giving appropriate names\nhead(cump)\n#>   cump_leq1 cump_leq2 cump_leq3\n#> 1      0.25       0.5      0.75\n#> 2      0.25       0.5      0.75\n#> 3      0.25       0.5      0.75\n#> 4      0.25       0.5      0.75\n#> 5      0.25       0.5      0.75\n#> 6      0.25       0.5      0.75\n\n## PROBABILITIES OF Y\n\n# for each row, we can calculate P(Y = k) using equation TODO put equation\n# P(Y = 1) = P(Y <= 1)\n# P(Y = 2) = P(Y <= 2) - P(Y <= 1)\n# P(Y = 3) = P(Y <= 3) - P(Y <= 2)\n# P(Y = 4) = 1 - P(Y <= 3)\n\n# adding a columns of 0 and 1, then diff() for adjacent differences\ncump <- cbind(0, cump, 1)\np <- apply(cump, 1, diff, simplify = FALSE)\n\np <- data.frame(do.call(rbind, p)) # collapse list of rows into a dataframe\nnames(p) <- sprintf(\"p%s\", 1:k) # giving appropriate names\nhead(p)\n#>     p1   p2   p3   p4\n#> 1 0.25 0.25 0.25 0.25\n#> 2 0.25 0.25 0.25 0.25\n#> 3 0.25 0.25 0.25 0.25\n#> 4 0.25 0.25 0.25 0.25\n#> 5 0.25 0.25 0.25 0.25\n#> 6 0.25 0.25 0.25 0.25\n\n# probabilities for id = 1 (x = 0) and id = 51 (x = 1)\np[c(1, 51), ]\n#>           p1        p2        p3   p4\n#> 1  0.2500000 0.2500000 0.2500000 0.25\n#> 51 0.1428571 0.1904762 0.2666667 0.40\n\n## CUMULATIVE ODDS RATIO\n\n# calculate the (cumulative) odds ratio\n\nx0 <- cump[1, 2:k]\nx1 <- cump[51, 2:k]\n\n# this is the same as the or (the b1) and we are assuming POA\nodds(x0) / odds(x1)\n#>   cump_leq1 cump_leq2 cump_leq3\n#> 1         2         2         2\n```\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\n## SAMPLING FROM THE MULTINOMIAL DISTRIBUTION\n\n# example of a random Y outcome based on the probabilities\nsample(x = 1:k, size = 1, prob = p[1, ])\n#> [1] 1\n\n# we can apply it to the full dataset, this step lead to different results\n# each time we run the code because we are sampling from a distribution\n\ndat$y <- apply(p, 1, function(ps) sample(1:k, size = 1, prob = ps))\nhead(dat)\n#>   x y\n#> 1 0 2\n#> 2 0 2\n#> 3 0 4\n#> 4 0 1\n#> 5 0 2\n#> 6 0 4\n\n# let's compute the observed probabilities, to be compared to the true\n# probabilities\n\n# observed\n(op <- prop.table(table(dat$x, dat$y), margin = 1))\n#>    \n#>        1    2    3    4\n#>   0 0.18 0.28 0.26 0.28\n#>   1 0.14 0.10 0.34 0.42\n\n# true (just selecting a row from x = 0 and x = 1)\np[c(1, 51), ]\n#>           p1        p2        p3   p4\n#> 1  0.2500000 0.2500000 0.2500000 0.25\n#> 51 0.1428571 0.1904762 0.2666667 0.40\n\n# similarly we can compute the observed cumulative odds ratios\n\n(cum_op <- apply(op, 1, cumsum))\n#>    \n#>        0    1\n#>   1 0.18 0.14\n#>   2 0.46 0.24\n#>   3 0.72 0.58\n#>   4 1.00 1.00\nodds(cum_op[, 1]) / odds(cum_op[, 2])\n#>        1        2        3        4 \n#> 1.348432 2.697531 1.862069      NaN\n\n# the odds ratios are not the same as the parameter. as we increase N\n# the parameter will converge to the true value\n```\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$y <- ordered(dat$y) # make an ordered factor in R where 1 < 2 < 3 < 4\nfit <- clm(y ~ x, data = dat, link = \"probit\")\nsummary(fit)\n#> formula: y ~ x\n#> data:    dat\n#> \n#>  link   threshold nobs logLik  AIC    niter max.grad cond.H \n#>  probit flexible  100  -132.11 272.23 5(0)  3.68e-08 1.6e+01\n#> \n#> Coefficients:\n#>   Estimate Std. Error z value Pr(>|z|)  \n#> x   0.3951     0.2193   1.801   0.0717 .\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Threshold coefficients:\n#>     Estimate Std. Error z value\n#> 1|2  -0.8204     0.1796  -4.568\n#> 2|3  -0.1991     0.1656  -1.202\n#> 3|4   0.5926     0.1731   3.423\n```\n:::\n\n \\normalsize\n\n\nWe can generalize the previous workflow as:\n\n1.  Define simulation parameters, baseline probabilities (`prob0`), sample size, etc.\n4.  Define regression coefficients $\\boldsymbol{\\beta}$ and calculate the $k - 1$ linear predictors ($\\eta$) using the equations\n5.  Apply the inverse of the link function $g^{-1}(\\eta)$ on the linear predictor and calculate the cumulative probabilities $p(y \\leq 1|x), p(y \\leq 2|x), ... p(y \\leq k - 1|x)$.\n6.  Calculate the probabilities of $k$ outcomes\n7.  Sample $n$ outcomes from a *multinomial* distribution, choosing between $k$ alternatives using the calculated probabilities\n8.  Fit the appropriate model using `ordinal::clm()`\n\n### Simulating from the latent distribution\n\nA more efficient way to simulate an ordinal outcome is using the latent formulation of the model. This require simulating a standard linear regression using the appropriate data generation function (*logistic* or *normal*) and the cutting the latent values according to the thresholds $\\alpha$. The workflow is slightly different compared to the previous approach.\n\n1.  Define simulation parameters as in the previous simulation. `prob0` are the probabilities when all predictors $\\mathbf{X}$ are zero.\n2.  Define regression coefficients $\\boldsymbol{\\beta}$ and calculate the linear predictor $\\eta$ using the Equation \\@ref(eq:latent-model)\n3.  Add the random errors $\\epsilon_i$ sampling from the chosen distribution (logistic or normal)\n4.  Cut the latent variable into $k$ areas using the thresholds $\\alpha$ and assign the corresponding ordinal value. This can be done using the `cut()` or the `findInterval()` functions.\n\nThe Figure \\@ref(fig:fig-sim-from-latent) depict the simulated $Y^{*}$ and the corresponding ordinal value. As for the previous simulation we can fit the model using `clm()` and check the estimated parameters.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## SIMULATION PARAMETERS\n\nN <- 1e3 # sample size\nor <- 4 # odds ratio, higher here just for a more clear plot\nk <- 4 # number of ordinal alternatives\nprobs0 <- rep(1/k, k) # probabilities for the first group\nalpha <- prob_to_alpha(probs0, link = \"logit\")\ndat <- data.frame(x = rep(c(0, 1), each = N/2))\n\n## LINEAR PREDICTOR\n\n# calculate the linear predictor using the model equation\ndat$lp <- log(or) * dat$x\n\n# add the random part by sampling errors from a standard logistic (or normal) distribution\ndat$ystar <- dat$lp + rlogis(N, location = 0, scale = 1)\n\n# cut the latent distribution. The + 1 because the first category is 0 by default.\ndat$y <- findInterval(dat$ystar, alpha) + 1\n\nhead_tail(dat, n = 3)\n#>      x       lp      ystar y\n#> 1    0 0.000000  1.0997758 4\n#> 2    0 0.000000  0.9751870 3\n#> 3    0 0.000000 -0.4056947 2\n#> 998  1 1.386294  2.3269231 4\n#> 999  1 1.386294  3.4289514 4\n#> 1000 1 1.386294  1.0372680 3\n```\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-sim-from-latent)](paper_files/figure-html/fig-sim-from-latent-1.pdf){#fig-sim-from-latent fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 5 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat$y <- ordered(dat$y) # make an ordered factor in R where 1 < 2 < 3 < 4\nfit <- clm(y ~ x, data = dat, link = \"logit\")\nsummary(fit)\n#> formula: y ~ x\n#> data:    dat\n#> \n#>  link  threshold nobs logLik   AIC     niter max.grad cond.H \n#>  logit flexible  1000 -1254.67 2517.35 4(0)  4.03e-08 1.7e+01\n#> \n#> Coefficients:\n#>   Estimate Std. Error z value Pr(>|z|)    \n#> x   1.1622     0.1205   9.643   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Threshold coefficients:\n#>     Estimate Std. Error z value\n#> 1|2 -1.30894    0.10081 -12.985\n#> 2|3 -0.16007    0.08658  -1.849\n#> 3|4  0.93461    0.09216  10.141\n```\n:::\n\n \\normalsize\n\n\nThe simulation using the latent formulation of the model is implemented in the `sim_ord_latent()` function. Details about the function can be found on the Github repository LINK. Basically, we define the dataset `dat` with predictors. Then the model formula is specified within the function as `location = ~` along with the vector of regression coefficients (`beta`), baseline probabilities (`prob0`) and the link function. The function return a dataset with the simulated $Y$ and the latent variable (this is possible only because we simulate the data, otherwise $Y^{\\star}$ is not observed by definition).\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nN <- 100 # sample size\nor <- 4 # odds ratio, higher here just for a more clear plot\nk <- 4 # number of ordinal alternatives\nprobs0 <- rep(1/k, k) # probabilities for the first group\nalpha <- prob_to_alpha(probs0, link = \"logit\")\ndat <- data.frame(x = rep(c(0, 1), each = N/2))\n\n# same as the previous simulation\ndat <- sim_ord_latent(location = ~x, beta = log(or), prob0 = probs0, data = dat, link = \"logit\")\n\nhead_tail(dat, n = 3)\n#>     x y         ys\n#> 1   0 3  0.8168914\n#> 2   0 1 -1.7325814\n#> 3   0 1 -1.4718580\n#> 98  1 3  0.5660396\n#> 99  1 4  1.1721564\n#> 100 1 3  0.6893142\n```\n:::\n\n \\normalsize\n\n\n### Choosing parameters values\n\n#### Thresholds $\\alpha$\n\nThe previous simulation can be easily extended by adding more predictors and their interactions. The crucial part is setting appropriate and empirically meaningful parameters. Thresholds $\\alpha$ are usually not of main interest [but see the location-shift models @Tutz2022-dg] and can be considered as intercepts in standard linear regression. The thresholds are quantiles of the latent distribution that produced certain $k$ probabilities. To set meaningful $\\alpha$ values we can convert probabilities into thresholds (`alpha_to_prob()`). The function `show_alpha()` produce a meaningful visual representation of using a specific set of thresholds (see Figure \\@ref(fig:fig-show-th-example)). In regression terms, the thresholds determine the probabilities when $x = 0$ (as the intercept). Thus with two groups for example, the thresholds are the $k$ probabilities of the ordinal variable $Y$ for the reference group.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-show-th-example)](paper_files/figure-html/fig-show-th-example-1.pdf){#fig-show-th-example fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 6 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n#### Regression coefficients\n\nFor *probit* models we can set the $\\beta_j$ to be in standardized (i.e., Cohen's $d$-like) units. For a categorical variable as the previous example with the group, $\\beta_j$ is the degree of separation in standard deviation unit between the two latent distributions. For *logit* models we can set the odds ratio. Meaningful odds ratios can be derived from previous literature, meta-analyses or converting to other effect sizes. For example, @Sanchez-Meca2003-ji proposed some equations to convert between odds ratios and Cohen's $d$. Using their approach, a Cohen's $d = 0.5$ usually considered a plausible medium effect size corresponds to and odds ratio of $\\approx 2.47$. \n\nWe can also calculate and plot the predicted probabilities (i.e., $g^{-1}(\\eta)$) given the predictors and the chosen regression coefficients. In this way we can try different values and see if predicted probabilities are plausible or not. The `cat_latent_plot()` and `num_latent_plot()` functions can be for respectively a categorical (Figures \\@ref(fig:fig-example-cat-latent)) and numerical predictor (Figures \\@ref(fig:fig-example-num-latent)).\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-example-cat-latent)](paper_files/figure-html/fig-example-cat-latent-1.pdf){#fig-example-cat-latent fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 7 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-example-num-latent)](paper_files/figure-html/fig-example-num-latent-1.pdf){#fig-example-num-latent fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 8 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n## 2x2 interaction\n\nA common research design could be a 2x2 factorial design. In this example we have 2 main effects and the interaction. By default R use dummy coding but setting sum-to-zero contrasts (e.g., $0.5$ and $-0.5$) for a factorial design is convenient. In this way $\\beta_1$ will be the main effect of $X_1$, $\\beta_2$ the main effect of $X_2$ and $\\beta_3$ the interaction (thus the difference of differences). Equation \\@ref(eq:interaction-equation) depict the model formula.\n\n\\begin{equation}\nP(Y \\leq k) = g^{-1}(\\alpha_k - \\beta_1X_{1_i} + \\beta_2X_{2_i} + \\beta_3X_{1_i}X_{2_i})\n(\\#eq:interaction-equation)\n\\end{equation}\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 100\nk <- 3\nbetas <- c(b1 = 0, b2 = 1, b3 = 0.5) # b1 = main effect X1, b2 = main effect X2, b3 = interaction\n\ndat <- expand.grid(x1 = c(\"a\", \"b\"), x2 = c(\"c\", \"d\"), n = 1:n)\ndat$x1 <- factor(dat$x1)\ndat$x2 <- factor(dat$x2)\n\n# sum to 0 coding\ncontrasts(dat$x1) <- c(0.5, -0.5)\ncontrasts(dat$x2) <- c(0.5, -0.5)\nprobs0 <- rep(1/k, k)\n\ndat <- sim_ord_latent(~ x1 * x2, beta = betas, prob0 = probs0, link = \"probit\", data = dat)\nfit <- clm(y ~ x1 * x2, data = dat, link = \"probit\")\nsummary(fit)\n#> formula: y ~ x1 * x2\n#> data:    dat\n#> \n#>  link   threshold nobs logLik  AIC    niter max.grad cond.H \n#>  probit flexible  400  -384.28 778.55 5(0)  1.65e-07 2.6e+01\n#> \n#> Coefficients:\n#>         Estimate Std. Error z value Pr(>|z|)    \n#> x11     -0.09934    0.11844  -0.839    0.402    \n#> x21      1.15208    0.12055   9.557   <2e-16 ***\n#> x11:x21  0.36443    0.23700   1.538    0.124    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Threshold coefficients:\n#>     Estimate Std. Error z value\n#> 1|2 -0.34669    0.06797  -5.101\n#> 2|3  0.40012    0.06814   5.872\n```\n:::\n\n \\normalsize\n\n\nThe parameters interpretation is the same as introduced in the general case. The thresholds $\\alpha$ are fixed representing the probabilities $P(Y = k)$ when all predictors are zero. In this case by doing `alpha_to_prob(fit$alpha, link = \"probit\")` we should recover the `probs0` vector. `x1` is the main effect thus the difference in $z$ scores between *a* and *b* averaging over `x2`. The same holds for `x2`. `x1:x2` is the interaction thus difference of differences in $z$ scores (see Figure \\@ref(fig:fig-effects-2-by-2-interaction)).\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![](paper_files/figure-html/fig-effects-2-by-2-interaction-1.pdf){#fig-effects-2-by-2-interaction fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 9 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n## Numerical by categorical interaction\n\nAnother common scenario is the interaction between a numerical variable $x$ and a categorical variable $g$. For simplicity we simulate the factor with two levels and the numerical variable sampled from an uniform distribution between 0 and 1.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 100\nk <- 3\ndat <- data.frame(x = runif(n),\n                  g = rep(c(\"a\", \"b\"), each = n/2))\ndat$g <- factor(dat$g)\ncontrasts(dat$g) <- c(0.5, -0.5)\nprobs0 <- rep(1/k, k)\n\ndat <- sim_ord_latent(~ x * g, beta = c(0.3, 0.5, 0.5), prob0 = probs0, link = \"probit\", data = dat)\nfit <- clm(y ~ x * g, data = dat, link = \"probit\")\nsummary(fit)\n#> formula: y ~ x * g\n#> data:    dat\n#> \n#>  link   threshold nobs logLik  AIC    niter max.grad cond.H \n#>  probit flexible  100  -105.05 220.11 5(0)  5.46e-14 8.8e+01\n#> \n#> Coefficients:\n#>      Estimate Std. Error z value Pr(>|z|)\n#> x      0.1518     0.3968   0.383    0.702\n#> g1     0.6446     0.4454   1.447    0.148\n#> x:g1  -0.4901     0.7936  -0.618    0.537\n#> \n#> Threshold coefficients:\n#>     Estimate Std. Error z value\n#> 1|2  -0.6631     0.2352  -2.820\n#> 2|3   0.2442     0.2305   1.059\n```\n:::\n\n \\normalsize\n\n\nAgain, `x` is the slope of the numerical predictor averaging over `g` (given that `g` has been coded with sum-to-zero contrasts) thus the increase in $z$ scores for a unit increase in `x`. `g1` is the main effect of the factor evaluated when $x = 0$ (centering $x$ will change the parameter interpretation). The `x:g1` is the slopes difference the two groups. Figure \\@ref(fig:fig-effects-num-by-cat-interaction) depicts the model results.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![](paper_files/figure-html/fig-effects-num-by-cat-interaction-1.pdf){#fig-effects-num-by-cat-interaction fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 10 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n# Power Analysis\n\nIn this section we introduce how to estimate the statistical power. Formally the power is defined as the probability of correctly rejecting the null hypothesis $H_0$. For simple cases (e.g., a t-test) the power can be calculated analytically while in complex cases simulating data is the best approach. We present very general example that, as for the previous simulations, can be extended easily extended. For example, by simulating a model fixing a specific parameter to zero, we can estimate the type-1 error rate (wrongly rejecting $H_0$) or assessing an eventual bias in model estimates. \n\nThe general workflow for a power simulation can be summarized as:\n\n1.  Specify the research design, e.g., 2x2 factorial design\n2.  Specify the effect/parameter of interest, e.g., the interaction effect\n3.  Define the simulation conditions, e.g., a range of sample size, effect size, etc.\n4.  Implement one of the simulation workflow described in the previous sections\n5.  Repeat the simulation a large number of times (e.g, 10000) and store the relevant values from each simulation\n6.  Summarise the simulation results\n\nWe can estimate the power of detecting a group difference of $d = 0.4$ (assuming a *probit* model). Participants respond to an ordinal variable with $k = 5$. The simulation is performed using a `for` loop that repeat 1000 times the data simulation, model fitting and extract the p-value for the $\\beta_1$. The power is then estimated as the number of p-values lower than the critical level over the number of simulations. \n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 40 # sample size\nk <- 5  # number of ordinal variables\nd <- 0.4 # effect size (i.e., our regression coefficients)\nnsim <- 1e3 # higher is better, here using 1000 for an example\nprobs0 <- rep(1/k, k)\nalpha <- 0.05 # critical alpha\n\np <- rep(NA, nsim) # preallocation to improve loop computational efficiency\ndat <- data.frame(group = rep(c(\"a\", \"b\"), each = n)) # data frame\n\nhead_tail(dat, n = 3)\n#>    group\n#> 1      a\n#> 2      a\n#> 3      a\n#> 78     b\n#> 79     b\n#> 80     b\n\nfor(i in 1:nsim){\n  sim <- sim_ord_latent(~group, beta = d, prob0 = probs0, link = \"probit\", data = dat)\n  fit <- clm(y ~ group, data = sim, link = \"probit\")\n  p[i] <- summary(fit)$coefficients[\"groupb\", \"Pr(>|z|)\"] # extract the pvalue\n}\n\n# estimate the power\nmean(p <= alpha, na.rm = TRUE)\n#> [1] 0.406\n```\n:::\n\n \\normalsize\n\n\nDespite useful, calculating power curves instead a single value is more informative. For this reason we repeat the previous simulation but for different sample sizes. Figure \\@ref(fig:fig-power-curve) depict the power curve resulting from the simulation.\n\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- c(20, 40, 60, 100, 200)\npower <- rep(NA, length(n))\n\nfor(i in 1:length(n)){\n  p <- rep(NA, nsim) # preallocation for speed\n  dat <- data.frame(group = rep(c(\"a\", \"b\"), each = n[i]))\n  for(j in 1:nsim){\n    sim <- sim_ord_latent(~group, beta = d, prob0 = probs0, link = \"probit\", data = dat)\n    fit <- clm(y ~ group, data = sim, link = \"probit\")\n    p[j] <- summary(fit)$coefficients[\"groupb\", \"Pr(>|z|)\"]\n  }\n  power[i] <- mean(p <= alpha)\n}\n\npower\n#> [1] 0.218 0.422 0.560 0.765 0.956\n```\n:::\n\n \\normalsize\n\n\n \\scriptsize\n\n::: {.cell layout-align=\"center\" fig.place='true'}\n::: {.cell-output-display}\n![(ref:fig-power-curve)](paper_files/figure-html/fig-power-curve-1.pdf){#fig-power-curve fig-align='center' width=672}\n:::\n\n\n \\begin{center}\\textbf{[Figure 11 about here]} \\end{center}\n:::\n\n \\normalsize\n\n\n\\newpage\n\n# References\n\n::: {#refs custom-style=\"Bibliography\"}\n:::\n\n<!-- FIGURES -->\n\n(ref:fig-ordinal-models) Cumulative model. Adapted from @Burkner2019-aw.\n\n(ref:fig-prop-odds) caption here\n\n(ref:fig-logit-vs-probit) caption here\n\n(ref:fig-example-cat-latent) `cat_latent_plot(m = c(0, 0.5), s = 1, probs = rep(1/4, 4), link = \"logit\", plot = \"both\")`\n\n(ref:fig-example-num-latent) `num_latent_plot(x = runif(100), b1 = 2, probs = c(0.6, 0.2, 0.1, 0.1), link = \"probit\")`\n\n(ref:fig-explain-cumulative) caption here\n\n(ref:fig-odds-example) caption here\n\n(ref:fig-effects-2-by-2-interaction) `plot(ggpredict(fit, terms = c(\"x1\", \"x2\")))`\n\n(ref:fig-effects-num-by-cat-interaction) `plot(ggpredict(fit, terms = c(\"x[all]\", \"g\")))`\n\n(ref:fig-power-curve) caption\n\n(ref:fig-sim-from-latent) caption\n\n(ref:fig-show-th-example) `show_alpha(prob = rep(1/4, 4), link = \"probit\", plot = c(\"latent\", \"probs\"))`\n\n<!-- TABLES -->\n\n(ref:tab-model-summary) caption here\n\n(ref:tab-dataset-example) caption here",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}